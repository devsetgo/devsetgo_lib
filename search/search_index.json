{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Support Python Versions</p> <p></p> <p>CI/CD Pipeline:</p> <p> </p> <p>SonarCloud:</p> <p> </p> <p> </p> <p></p>"},{"location":"#devsetgo-common-library","title":"DevSetGo Common Library","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The DevSetGo Common Library is a comprehensive package of common functions designed to eliminate repetitive coding and enhance code reusability. It aims to save developers time and effort across various projects.</p>"},{"location":"#compatibility-and-testing","title":"Compatibility and Testing","text":"<ul> <li>Tested on: Windows, Linux.</li> <li>Compatibility: Potentially compatible with MacOS (feedback on issues is appreciated).</li> </ul>"},{"location":"#library-functions","title":"Library Functions","text":""},{"location":"#common-functions","title":"Common Functions","text":"<ul> <li>File Functions:</li> <li>CSV File Functions</li> <li>JSON File Functions</li> <li>Text File Functions</li> <li>Folder Functions:</li> <li>Make Directory</li> <li>Remove Directory</li> <li>Last File Changed</li> <li>Directory List</li> <li>Calendar Functions:</li> <li>Get Month</li> <li>Get Month Number</li> <li>Patterns:</li> <li>Pattern Between Two Characters</li> <li>Logging:</li> <li>Logging configuration and interceptor</li> </ul>"},{"location":"#fastapi-endpoints","title":"FastAPI Endpoints","text":"<ul> <li>Systems Health Endpoints:</li> <li>Status/Health, Heapdump, Uptime</li> <li>HTTP Codes:</li> <li>Method to generate HTTP response codes</li> </ul>"},{"location":"#async-database","title":"Async Database","text":"<ul> <li>Database Config</li> <li>Async Session</li> <li>CRUD Operations</li> </ul>"},{"location":"#examples-and-usage","title":"Examples and Usage","text":"<p>Refer to the Recipes Pages</p>"},{"location":"#installation-guide","title":"Installation Guide","text":"<p>Quick Start</p> <pre><code>pip install devsetgo-lib\n\n# Aysync database setup\npip install devsetgo-lib[sqlite]\npip install devsetgo-lib[postgres]\n\n# Consider these experimental and untested\npip install devsetgo-lib[oracle]\npip install devsetgo-lib[mssql]\npip install devsetgo-lib[mysql]\n\n# For adding FastAPI endpoints\npip install devsetgo-lib[fastapi]\n\n# Install everything\npip install devsetgo-lib[all]\n</code></pre>"},{"location":"#contribution-and-feedback","title":"Contribution and Feedback","text":"<p>Contributions and feedback are highly appreciated. Please refer to our Contribution Guidelines.</p>"},{"location":"#license","title":"License","text":"<p>MIT Licensed</p>"},{"location":"#author-information","title":"Author Information","text":"<p>Mike Ryan</p>"},{"location":"#further-documentation","title":"Further Documentation","text":"<p>For more detailed information, visit LINK_TO_DETAILED_DOCUMENTATION.</p>"},{"location":"contribute/","title":"Contributing","text":"<p>Please feel to contribute to this project. Adding common functions is the intent and if you have one to add or improve an existing it is greatly appreciated.</p>"},{"location":"contribute/#ways-to-contribute","title":"Ways to Contribute!","text":"<ul> <li>Add or improve a function</li> <li>Add or improve documentation</li> <li>Add or improve Tests</li> <li>Report or fix a bug</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#install","title":"Install","text":"<pre><code>pip install devsetgo-lib\n\n# Aysync database setup\npip install devsetgo-lib[sqlite]\npip install devsetgo-lib[postgres]\n\n# Consider these experimental and untested\npip install devsetgo-lib[oracle]\npip install devsetgo-lib[mssql]\npip install devsetgo-lib[mysql]\n\n# For adding FastAPI endpoints\npip install devsetgo-lib[fastapi]\n\n# Install everything\npip install devsetgo-lib[all]\n</code></pre> <p>See documentation for more examples of library use</p>"},{"location":"release-notes/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog</p>"},{"location":"release-notes/#latest-changes","title":"Latest Changes","text":""},{"location":"release-notes/#adding-delete-many-and-minor-fixes-v0112","title":"Adding Delete Many and minor fixes (v0.11.2)","text":""},{"location":"release-notes/#whats-changed","title":"What's Changed","text":"<ul> <li>Adding Delete Many and Other Updates  (#381) @devsetgo</li> <li>pip(deps): bump mkdocs-material from 9.5.2 to 9.5.3 (#377) @dependabot</li> <li>pip(deps): bump fastapi[all] from 0.105.0 to 0.108.0 (#375) @dependabot</li> <li>pip(deps): bump sqlalchemy from 2.0.23 to 2.0.24 (#374) @dependabot</li> <li>pip(deps): bump pytest from 7.4.3 to 7.4.4 (#373) @dependabot</li> <li>pip(deps): bump black from 23.12.0 to 23.12.1 (#376) @dependabot</li> <li>github actionts(deps): bump actions/setup-python from 4 to 5 (#378) @dependabot</li> </ul> <p>Published Date: 2024 January 20, 00:07</p>"},{"location":"release-notes/#breaking-change-v0111","title":"Breaking Change (v0.11.1)","text":""},{"location":"release-notes/#whats-changed_1","title":"What's Changed","text":"<ul> <li>Bump of Version to 0.11.1 (#371) @devsetgo</li> <li>Query Improvement (#370) @devsetgo</li> <li>368 get one record should return an empty value when called (#369) @devsetgo</li> <li>updating docs from v0.11.0 release (#367) @devsetgo</li> </ul> <p>Published Date: 2023 December 23, 10:49</p>"},{"location":"release-notes/#full-release-of-new-features-v0110","title":"Full Release of New Features (v0.11.0)","text":""},{"location":"release-notes/#whats-changed_2","title":"What's Changed","text":"<ul> <li>Prep for Release (#366) @devsetgo</li> <li>Fixing sonar settings (#365) @devsetgo</li> <li>Fixes and improvements (#364) @devsetgo</li> <li>Dev (#362) @devsetgo</li> <li>Fix of issues from Beta release (#361) @devsetgo</li> <li>359 tables are created before create tables is called (#360) @devsetgo</li> <li>Change Log (#358) @devsetgo</li> <li>fixing latest-changes (#357) @devsetgo</li> <li>removing jinja template from Latest Changes Action (#356) @devsetgo</li> <li>Action fixing adding main (#355) @devsetgo</li> <li>Fixing actions (#354) @devsetgo</li> <li>Fixing Beta Publishing issues and Documentation Improvements (#353) @devsetgo</li> <li>Update setup.py for sub packages (#352) @devsetgo</li> <li>Import Bug Fix (#351) @devsetgo</li> <li>Latest Changes Action Fix (#350) @devsetgo</li> <li>Next Release (#349) @devsetgo</li> <li>Dev (#348) @devsetgo</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 17, 22:00</p>"},{"location":"release-notes/#beta-release-with-fixes-for-multiple-issues-v0110-beta3-fix1","title":"Beta Release with fixes for multiple issues (v0.11.0-beta3-fix1)","text":""},{"location":"release-notes/#whats-changed_3","title":"What's Changed","text":"<ul> <li>Dev (#362) @devsetgo</li> <li>Fix of issues from Beta release (#361) @devsetgo</li> <li>359 tables are created before create tables is called (#360) @devsetgo</li> <li>Change Log (#358) @devsetgo</li> <li>fixing latest-changes (#357) @devsetgo</li> <li>removing jinja template from Latest Changes Action (#356) @devsetgo</li> <li>Action fixing adding main (#355) @devsetgo</li> <li>Fixing actions (#354) @devsetgo</li> <li>Fixing Beta Publishing issues and Documentation Improvements (#353) @devsetgo</li> <li>Update setup.py for sub packages (#352) @devsetgo</li> <li>Import Bug Fix (#351) @devsetgo</li> <li>Latest Changes Action Fix (#350) @devsetgo</li> <li>Next Release (#349) @devsetgo</li> <li>Dev (#348) @devsetgo</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 17, 16:23</p>"},{"location":"release-notes/#fixing-asyncdatabase-create-tables-v0110-beta3","title":"Fixing AsyncDatabase create tables (v0.11.0-beta3)","text":""},{"location":"release-notes/#whats-changed_4","title":"What's Changed","text":"<ul> <li>Fix of issues from Beta release (#361) @devsetgo</li> <li>359 tables are created before create tables is called (#360) @devsetgo</li> <li>Change Log (#358) @devsetgo</li> <li>fixing latest-changes (#357) @devsetgo</li> <li>removing jinja template from Latest Changes Action (#356) @devsetgo</li> <li>Action fixing adding main (#355) @devsetgo</li> <li>Fixing actions (#354) @devsetgo</li> <li>Fixing Beta Publishing issues and Documentation Improvements (#353) @devsetgo</li> <li>Update setup.py for sub packages (#352) @devsetgo</li> <li>Import Bug Fix (#351) @devsetgo</li> <li>Latest Changes Action Fix (#350) @devsetgo</li> <li>Next Release (#349) @devsetgo</li> <li>Dev (#348) @devsetgo</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 17, 16:18</p>"},{"location":"release-notes/#build-updates-v0110-beta2","title":"Build Updates  (v0.11.0-beta2)","text":""},{"location":"release-notes/#whats-changed_5","title":"What's Changed","text":"<ul> <li>Change Log (#358) @devsetgo</li> <li>fixing latest-changes (#357) @devsetgo</li> <li>removing jinja template from Latest Changes Action (#356) @devsetgo</li> <li>Action fixing adding main (#355) @devsetgo</li> <li>Fixing actions (#354) @devsetgo</li> <li>Fixing Beta Publishing issues and Documentation Improvements (#353) @devsetgo</li> <li>Update setup.py for sub packages (#352) @devsetgo</li> <li>Import Bug Fix (#351) @devsetgo</li> <li>Latest Changes Action Fix (#350) @devsetgo</li> <li>Next Release (#349) @devsetgo</li> <li>Dev (#348) @devsetgo</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 16, 20:34</p>"},{"location":"release-notes/#beta-release-with-fixes-for-multiple-issues-v0110-beta1-fix5","title":"Beta Release with fixes for multiple issues (v0.11.0-beta1-fix5)","text":""},{"location":"release-notes/#whats-changed_6","title":"What's Changed","text":"<ul> <li>fixing latest-changes (#357) @devsetgo</li> <li>removing jinja template from Latest Changes Action (#356) @devsetgo</li> <li>Action fixing adding main (#355) @devsetgo</li> <li>Fixing actions (#354) @devsetgo</li> <li>Fixing Beta Publishing issues and Documentation Improvements (#353) @devsetgo</li> <li>Update setup.py for sub packages (#352) @devsetgo</li> <li>Import Bug Fix (#351) @devsetgo</li> <li>Latest Changes Action Fix (#350) @devsetgo</li> <li>Next Release (#349) @devsetgo</li> <li>Dev (#348) @devsetgo</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 16, 16:33</p>"},{"location":"release-notes/#build-fixes-v0110-beta1-fix4","title":"Build Fixes  (v0.11.0-beta1-fix4)","text":""},{"location":"release-notes/#whats-changed_7","title":"What's Changed","text":"<ul> <li>Update setup.py for sub packages (#352) @devsetgo</li> <li>Import Bug Fix (#351) @devsetgo</li> <li>Latest Changes Action Fix (#350) @devsetgo</li> <li>Next Release (#349) @devsetgo</li> <li>Dev (#348) @devsetgo</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 12, 11:45</p>"},{"location":"release-notes/#async-database-and-fastapi-functions-v0110-beta0","title":"Async Database and FastAPI functions (v0.11.0-beta0)","text":""},{"location":"release-notes/#whats-changed_8","title":"What's Changed","text":"<ul> <li>Dev (#348) @devsetgo - New functionality and documentation for FastAPI Endpoints and Async Database Functionality</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 10, 20:17</p>"},{"location":"release-notes/#pattern-analysis-update-and-bug-fix-v0101","title":"Pattern Analysis Update and Bug Fix (v0.10.1)","text":""},{"location":"release-notes/#whats-changed_9","title":"What's Changed","text":"<ul> <li>Improvement to the patterns analysis (#313) @devsetgo</li> <li>pip(deps): bump mkdocs-material from 9.1.3 to 9.1.5 (#308) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.0 to 3.2.1 (#310) @dependabot</li> <li>pip(deps): bump watchdog from 2.3.1 to 3.0.0 (#309) @dependabot</li> <li>pip(deps): bump pylint from 2.17.0 to 2.17.1 (#311) @dependabot</li> <li>pip(deps): bump tox from 4.4.7 to 4.4.8 (#312) @dependabot</li> </ul> <p>Published Date: 2023 April 08, 21:45</p>"},{"location":"release-notes/#chatgpt-driven-improvements-v0100","title":"ChatGPT Driven Improvements (v0.10.0)","text":""},{"location":"release-notes/#chatgpt","title":"ChatGPT","text":"<p>Using ChatGPT to improve tests, find bugs, and improve performance. Code coverage is at 100% and the code base appears to be performing better than before.</p> <p>Major changes are in PR #304</p>"},{"location":"release-notes/#whats-changed_10","title":"What's Changed","text":"<ul> <li>latest change fix for regex pattern. (#307) @devsetgo</li> <li>Dev (#306) @devsetgo</li> <li>Workflow changes (#305) @devsetgo</li> <li>ChatGPT Driven Improvements (#304) @devsetgo</li> <li>pip(deps): bump pre-commit from 3.0.2 to 3.1.1 (#300) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.1.0 to 3.2.0 (#302) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.0 to 2.0.1 (#299) @dependabot</li> <li>pip(deps): bump watchdog from 2.1.9 to 2.3.1 (#301) @dependabot</li> <li>pip(deps): bump pytest from 7.2.0 to 7.2.1 (#303) @dependabot</li> <li>pip(deps): bump pylint from 2.15.7 to 2.16.1 (#298) @dependabot</li> <li>pip(deps): bump autopep8 from 2.0.0 to 2.0.1 (#289) @dependabot</li> <li>pip(deps): bump pylint from 2.15.7 to 2.15.10 (#295) @dependabot</li> <li>pip(deps): bump black from 22.10.0 to 23.1.0 (#294) @dependabot</li> <li>pip(deps): bump tox from 3.27.1 to 4.4.4 (#296) @dependabot</li> <li>pip(deps): bump pre-commit from 2.20.0 to 3.0.2 (#297) @dependabot</li> </ul> <p>Published Date: 2023 April 01, 00:27</p>"},{"location":"release-notes/#open-csv-enhancements-and-library-updates-v090","title":"Open CSV enhancements and library updates (v0.9.0)","text":""},{"location":"release-notes/#whats-changed_11","title":"What's Changed","text":"<ul> <li>fix of latest changes (#288) @devsetgo</li> <li>Open_CSV Enhancements (#287) @devsetgo</li> <li>pip(deps): bump pytest-cov from 3.0.0 to 4.0.0 (#274) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.4.2 to 8.5.5 (#276) @dependabot</li> <li>pip(deps): bump autoflake from 1.5.3 to 1.6.1 (#275) @dependabot</li> <li>pip(deps): bump tqdm from 4.64.0 to 4.64.1 (#273) @dependabot</li> <li>pip(deps): bump pytest from 7.1.2 to 7.1.3 (#272) @dependabot</li> <li>pip(deps): bump mkdocs from 1.3.1 to 1.4.0 (#271) @dependabot</li> <li>pip(deps): bump tox from 3.25.1 to 3.26.0 (#269) @dependabot</li> <li>pip(deps): bump pylint from 2.15.0 to 2.15.3 (#270) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.3.9 to 8.4.2 (#268) @dependabot</li> <li>pip(deps): bump autopep8 from 1.6.0 to 1.7.0 (#264) @dependabot</li> <li>pip(deps): bump pylint from 2.14.5 to 2.15.0 (#265) @dependabot</li> <li>pip(deps): bump autoflake from 1.4 to 1.5.3 (#263) @dependabot</li> <li>pip(deps): bump black from 22.6.0 to 22.8.0 (#267) @dependabot</li> <li>pip(deps): bump flake8 from 5.0.1 to 5.0.4 (#266) @dependabot</li> <li>pip(deps): bump pre-commit from 2.19.0 to 2.20.0 (#260) @dependabot</li> <li>pip(deps): bump mkdocs from 1.3.0 to 1.3.1 (#261) @dependabot</li> <li>pip(deps): bump flake8 from 4.0.1 to 5.0.1 (#259) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.3.8 to 8.3.9 (#258) @dependabot</li> <li>pip(deps): bump pylint from 2.14.4 to 2.14.5 (#262) @dependabot</li> <li>pip(deps): bump twine from 4.0.0 to 4.0.1 (#252) @dependabot</li> <li>pip(deps): bump pylint from 2.14.0 to 2.14.4 (#251) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.2.16 to 8.3.8 (#253) @dependabot</li> <li>pip(deps): bump black from 22.3.0 to 22.6.0 (#254) @dependabot</li> <li>pip(deps): bump tox from 3.25.0 to 3.25.1 (#255) @dependabot</li> <li>pip(deps): bump watchdog from 2.1.8 to 2.1.9 (#256) @dependabot</li> <li>github actionts(deps): bump actions/setup-python from 3 to 4 (#257) @dependabot</li> <li>pip(deps): bump pylint from 2.13.7 to 2.14.0 (#250) @dependabot</li> <li>pip(deps): bump watchdog from 2.1.7 to 2.1.8 (#246) @dependabot</li> <li>pip(deps): bump pre-commit from 2.18.1 to 2.19.0 (#248) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.2.12 to 8.2.16 (#249) @dependabot</li> <li>pip(deps): bump tox from 3.24.5 to 3.25.0 (#242) @dependabot</li> <li>pip(deps): bump pre-commit from 2.17.0 to 2.18.1 (#243) @dependabot</li> <li>pip(deps): bump click from 8.1.2 to 8.1.3 (#245) @dependabot</li> <li>pip(deps): bump pylint from 2.13.4 to 2.13.7 (#240) @dependabot</li> <li>pip(deps): bump tqdm from 4.63.1 to 4.64.0 (#244) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.2.8 to 8.2.12 (#241) @dependabot</li> <li>pip(deps): bump pytest from 7.1.1 to 7.1.2 (#239) @dependabot</li> <li>pip(deps): bump watchdog from 2.1.6 to 2.1.7 (#238) @dependabot</li> <li>pip(deps): bump pylint from 2.12.2 to 2.13.4 (#237) @dependabot</li> <li>pip(deps): bump mkdocs from 1.2.3 to 1.3.0 (#234) @dependabot</li> <li>pip(deps): bump tqdm from 4.63.0 to 4.63.1 (#233) @dependabot</li> <li>pip(deps): bump black from 22.1.0 to 22.3.0 (#236) @dependabot</li> <li>pip(deps): bump pytest from 7.0.1 to 7.1.1 (#231) @dependabot</li> <li>pip(deps): bump click from 8.0.4 to 8.1.2 (#235) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.2.5 to 8.2.8 (#232) @dependabot</li> <li>pip(deps): bump twine from 3.8.0 to 4.0.0 (#230) @dependabot</li> <li>document updates (#229) @devsetgo</li> </ul> <p>Published Date: 2022 December 04, 16:55</p>"},{"location":"release-notes/#additional-logging-configuration-v080","title":"Additional Logging Configuration (v0.8.0)","text":""},{"location":"release-notes/#whats-changed_12","title":"What's Changed","text":"<ul> <li>New Logging Configuration items (#228) @devsetgo</li> <li>pip(deps): bump tqdm from 4.62.3 to 4.63.0 (#224) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.2.3 to 8.2.4 (#227) @dependabot</li> <li>github actionts(deps): bump actions/setup-python from 2.3.1 to 3 (#226) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.1.9 to 8.2.3 (#225) @dependabot</li> <li>pip(deps): bump twine from 3.7.1 to 3.8.0 (#223) @dependabot</li> <li>pip(deps): bump pytest from 6.2.5 to 7.0.1 (#222) @dependabot</li> <li>pip(deps): bump pytest-runner from 5.3.1 to 6.0.0 (#221) @dependabot</li> <li>pip(deps): bump loguru from 0.5.3 to 0.6.0 (#218) @dependabot</li> <li>pip(deps): bump black from 21.12b0 to 22.1.0 (#219) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.1.8 to 8.1.9 (#220) @dependabot</li> </ul> <p>Published Date: 2022 March 12, 21:07</p>"},{"location":"release-notes/#v071","title":"(v0.7.1)","text":""},{"location":"release-notes/#whats-changed_13","title":"What's Changed","text":"<ul> <li>Bump version: 0.7.0 \u2192 0.7.1 (#217) @devsetgo</li> <li>Hotfix for setup file (#216) @devsetgo</li> </ul> <p>Published Date: 2022 January 29, 01:51</p>"},{"location":"release-notes/#logging-to-beta-testing-v070","title":"Logging to Beta Testing (v0.7.0)","text":"<p>Logging is now has basic unit tests and is more ready to use with live application.</p>"},{"location":"release-notes/#whats-changed_14","title":"What's Changed","text":"<ul> <li>Adding Logging Config (#215) @devsetgo</li> <li>pip(deps): bump pre-commit from 2.15.0 to 2.16.0 (#210) @dependabot</li> <li>pip(deps): bump pylint from 2.12.1 to 2.12.2 (#211) @dependabot</li> <li>pip(deps): bump tox from 3.24.4 to 3.24.5 (#212) @dependabot</li> <li>pip(deps): bump black from 21.11b1 to 21.12b0 (#213) @dependabot</li> <li>pip(deps): bump twine from 3.6.0 to 3.7.1 (#214) @dependabot</li> <li>pip(deps): bump twine from 3.5.0 to 3.6.0 (#204) @dependabot</li> <li>pip(deps): bump coverage-badge from 1.0.2 to 1.1.0 (#205) @dependabot</li> <li>pip(deps): bump mkdocs-material from 7.3.6 to 8.0.2 (#206) @dependabot</li> <li>pip(deps): bump pylint from 2.11.1 to 2.12.1 (#207) @dependabot</li> <li>pip(deps): bump black from 21.10b0 to 21.11b1 (#208) @dependabot</li> <li>github actionts(deps): bump actions/setup-python from 2.2.2 to 2.3.1 (#209) @dependabot</li> <li>Dev (#203) @devsetgo</li> <li>pip(deps): bump tox from 3.24.3 to 3.24.4 (#193) @dependabot</li> <li>pip(deps): bump tqdm from 4.62.2 to 4.62.3 (#194) @dependabot</li> <li>pip(deps): bump pylint from 2.10.2 to 2.11.1 (#195) @dependabot</li> <li>pip(deps): bump mkdocs-material from 7.2.6 to 7.3.0 (#196) @dependabot</li> <li>pip(deps): bump black from 21.8b0 to 21.9b0 (#197) @dependabot</li> <li>pip(deps): bump mkdocs-material from 7.2.4 to 7.2.6 (#189) @dependabot</li> <li>pip(deps): bump pytest from 6.2.4 to 6.2.5 (#191) @dependabot</li> <li>pip(deps): bump watchdog from 2.1.3 to 2.1.5 (#192) @dependabot</li> <li>pip(deps): bump tox from 3.24.1 to 3.24.3 (#190) @dependabot</li> <li>pip(deps): bump pre-commit from 2.14.0 to 2.15.0 (#188) @dependabot</li> <li>pip(deps): bump black from 21.7b0 to 21.8b0 (#187) @dependabot</li> <li>pip(deps): bump pylint from 2.9.6 to 2.10.2 (#184) @dependabot</li> <li>pip(deps): bump tqdm from 4.62.0 to 4.62.2 (#185) @dependabot</li> <li>github actionts(deps): bump actions/setup-python from 1 to 2.2.2 (#182) @dependabot</li> <li>Bump wheel from 0.36.2 to 0.37.0 (#180) @dependabot</li> <li>Bump mkdocs-material from 7.2.2 to 7.2.4 (#181) @dependabot</li> <li>Bump tox from 3.24.0 to 3.24.1 (#177) @dependabot</li> <li>Bump mkdocs-material from 7.2.1 to 7.2.2 (#178) @dependabot</li> <li>Bump pre-commit from 2.13.0 to 2.14.0 (#179) @dependabot</li> <li>Bump pylint from 2.9.5 to 2.9.6 (#176) @dependabot</li> <li>Bump tqdm from 4.61.2 to 4.62.0 (#175) @dependabot</li> <li>Bump mkdocs-material from 7.1.10 to 7.2.1 (#174) @dependabot</li> <li>Bump twine from 3.4.1 to 3.4.2 (#171) @dependabot</li> <li>Bump pylint from 2.9.3 to 2.9.5 (#170) @dependabot</li> <li>Bump mkdocs from 1.2.1 to 1.2.2 (#173) @dependabot</li> <li>documentation update (#169) @devsetgo</li> <li>README fix (#168) @devsetgo</li> </ul> <p>Published Date: 2022 January 29, 01:42</p>"},{"location":"release-notes/#logging-configuration-v060","title":"Logging Configuration (v0.6.0)","text":""},{"location":"release-notes/#whats-changed_15","title":"What's Changed","text":"<ul> <li>Adding Logging and Cleanup (#167) @devsetgo</li> <li>Bump tqdm from 4.61.1 to 4.61.2 (#166) @dependabot</li> <li>Bump pylint from 2.8.3 to 2.9.3 (#165) @dependabot</li> <li>Bump watchdog from 2.1.2 to 2.1.3 (#164) @dependabot</li> <li>Bump mkdocs-material from 7.1.8 to 7.1.9 (#163) @dependabot</li> <li>Bump tqdm from 4.61.0 to 4.61.1 (#162) @dependabot</li> <li>Bump mkdocs-material from 7.1.7 to 7.1.8 (#161) @dependabot</li> <li>Bump mkdocs from 1.1.2 to 1.2.1 (#159) @dependabot</li> <li>Bump black from 21.5b2 to 21.6b0 (#158) @dependabot</li> <li>Bump mkdocs-material from 7.1.6 to 7.1.7 (#160) @dependabot</li> <li>Bump pytest-cov from 2.12.0 to 2.12.1 (#154) @dependabot</li> <li>Bump pylint from 2.8.2 to 2.8.3 (#155) @dependabot</li> <li>Bump black from 21.5b1 to 21.5b2 (#156) @dependabot</li> <li>Bump mkdocs-material from 7.1.5 to 7.1.6 (#157) @dependabot</li> <li>Bump tqdm from 4.60.0 to 4.61.0 (#153) @dependabot</li> <li>Bump pre-commit from 2.12.1 to 2.13.0 (#151) @dependabot</li> <li>Bump pytest-runner from 5.3.0 to 5.3.1 (#152) @dependabot</li> <li>Bump mkdocs-material from 7.1.4 to 7.1.5 (#150) @dependabot</li> <li>Bump watchdog from 2.1.1 to 2.1.2 (#149) @dependabot</li> <li>Bump click from 7.1.2 to 8.0.1 (#148) @dependabot</li> <li>Bump black from 21.5b0 to 21.5b1 (#147) @dependabot</li> <li>Bump watchdog from 2.1.0 to 2.1.1 (#146) @dependabot</li> <li>Bump pytest-cov from 2.11.1 to 2.12.0 (#145) @dependabot</li> <li>Bump flake8 from 3.9.1 to 3.9.2 (#143) @dependabot</li> <li>Bump pytest from 6.2.3 to 6.2.4 (#139) @dependabot</li> <li>Bump watchdog from 2.0.3 to 2.1.0 (#138) @dependabot</li> <li>Bump black from 21.4b2 to 21.5b0 (#140) @dependabot</li> <li>Bump mkdocs-material from 7.1.3 to 7.1.4 (#141) @dependabot</li> <li>Dev (#142) @devsetgo</li> <li>Bump tox from 3.23.0 to 3.23.1 (#137) @dependabot</li> <li>Bump autopep8 from 1.5.6 to 1.5.7 (#136) @dependabot</li> <li>Bump pylint from 2.7.4 to 2.8.2 (#135) @dependabot</li> <li>Bump black from 20.8b1 to 21.4b2 (#134) @dependabot</li> <li>Bump mkdocs-material from 7.1.2 to 7.1.3 (#133) @dependabot</li> <li>Adding SonarCloud Code Coverage (#130) @devsetgo</li> <li>Bump mkdocs-material from 7.1.1 to 7.1.2 (#132) @dependabot</li> <li>Bump watchdog from 2.0.2 to 2.0.3 (#131) @dependabot</li> <li>Bump pre-commit from 2.12.0 to 2.12.1 (#129) @dependabot</li> <li>Bump flake8 from 3.9.0 to 3.9.1 (#128) @dependabot</li> <li>Bump mkdocs-material from 7.1.0 to 7.1.1 (#127) @dependabot</li> <li>Bump tqdm from 4.59.0 to 4.60.0 (#124) @dependabot</li> <li>Bump pytest from 6.2.2 to 6.2.3 (#125) @dependabot</li> <li>Bump pre-commit from 2.11.1 to 2.12.0 (#126) @dependabot</li> <li>Bump pylint from 2.7.2 to 2.7.4 (#122) @dependabot</li> <li>Bump mkdocs-material from 7.0.6 to 7.1.0 (#123) @dependabot</li> <li>Bump mkdocs-material from 7.0.5 to 7.0.6 (#121) @dependabot</li> <li>Bump flake8 from 3.8.4 to 3.9.0 (#120) @dependabot</li> <li>Bump twine from 3.3.0 to 3.4.1 (#118) @dependabot</li> <li>Bump autopep8 from 1.5.5 to 1.5.6 (#119) @dependabot</li> </ul> <p>Published Date: 2021 July 16, 23:44</p>"},{"location":"release-notes/#fixing-publish-v050-2","title":"Fixing Publish (v0.5.0-2)","text":""},{"location":"release-notes/#whats-changed_16","title":"What's Changed","text":"<ul> <li>adding update for publish (#117) @devsetgo</li> </ul> <p>Published Date: 2021 March 18, 17:19</p>"},{"location":"release-notes/#calendar-and-regex-function-documentation-v050","title":"Calendar and RegEx Function + Documentation (v0.5.0)","text":""},{"location":"release-notes/#whats-changed_17","title":"What's Changed","text":"<ul> <li>Adding Calendar Functions (#116) @devsetgo</li> <li>Bump pre-commit from 2.10.1 to 2.11.1 (#113) @dependabot</li> <li>update to Saturday (#115) @devsetgo</li> <li>Bump tqdm from 4.58.0 to 4.59.0 (#112) @dependabot</li> <li>Bump mkdocs-material from 7.0.4 to 7.0.5 (#114) @dependabot</li> <li>fixes for mkdoc material update (#111) @devsetgo</li> <li>Bump tox from 3.22.0 to 3.23.0 (#109) @dependabot</li> <li>Bump mkdocs-material from 7.0.2 to 7.0.4 (#108) @dependabot</li> <li>Bump pylint from 2.7.1 to 2.7.2 (#107) @dependabot</li> <li>Bump coverage from 5.4 to 5.5 (#110) @dependabot</li> <li>Bump pylint from 2.6.2 to 2.7.1 (#103) @dependabot</li> <li>Bump mkdocs-material from 6.2.8 to 7.0.2 (#104) @dependabot</li> <li>Bump watchdog from 2.0.1 to 2.0.2 (#105) @dependabot</li> <li>Bump tqdm from 4.57.0 to 4.58.0 (#106) @dependabot</li> <li>Bump tox from 3.21.4 to 3.22.0 (#101) @dependabot</li> <li>Bump watchdog from 2.0.0 to 2.0.1 (#99) @dependabot</li> <li>Bump pylint from 2.6.0 to 2.6.2 (#102) @dependabot</li> <li>Bump tqdm from 4.56.2 to 4.57.0 (#100) @dependabot</li> <li>Bump pytest-runner from 5.2 to 5.3.0 (#98) @dependabot</li> <li>Bump tqdm from 4.56.0 to 4.56.2 (#97) @dependabot</li> <li>Bump watchdog from 1.0.2 to 2.0.0 (#96) @dependabot</li> <li>Bump pre-commit from 2.10.0 to 2.10.1 (#95) @dependabot</li> <li>Bump mkdocs-material from 6.2.6 to 6.2.8 (#94) @dependabot</li> <li>Bump tox from 3.21.3 to 3.21.4 (#93) @dependabot</li> <li>Bump autopep8 from 1.5.4 to 1.5.5 (#92) @dependabot</li> <li>Bump tox from 3.21.2 to 3.21.3 (#87) @dependabot</li> <li>Bump mkdocs-material from 6.2.5 to 6.2.6 (#88) @dependabot</li> <li>Bump pytest from 6.2.1 to 6.2.2 (#89) @dependabot</li> <li>Bump coverage from 5.3.1 to 5.4 (#91) @dependabot</li> <li>Bump pre-commit from 2.9.3 to 2.10.0 (#90) @dependabot</li> <li>Bump tox from 3.21.1 to 3.21.2 (#84) @dependabot</li> <li>Bump mkdocs-material from 6.2.4 to 6.2.5 (#85) @dependabot</li> <li>Bump pytest-cov from 2.10.1 to 2.11.1 (#86) @dependabot</li> <li>Bump tox from 3.20.1 to 3.21.1 (#81) @dependabot</li> <li>Bump mkdocs-material from 6.2.3 to 6.2.4 (#82) @dependabot</li> <li>Bump tqdm from 4.55.1 to 4.56.0 (#83) @dependabot</li> <li>Bump tqdm from 4.55.0 to 4.55.1 (#80) @dependabot</li> <li>Bump mkdocs-material from 6.2.2 to 6.2.3 (#79) @dependabot</li> </ul> <p>Published Date: 2021 March 18, 17:06</p>"},{"location":"release-notes/#minor-updates-and-library-updates-v041","title":"Minor updates and library updates. (v0.4.1)","text":""},{"location":"release-notes/#whats-changed_18","title":"What's Changed","text":"<ul> <li>Updates and Minor updates (#78) @devsetgo</li> <li>Bump tqdm from 4.54.1 to 4.55.0 (#77) @dependabot</li> <li>Bump twine from 3.2.0 to 3.3.0 (#76) @dependabot</li> <li>Bump coverage from 5.3 to 5.3.1 (#74) @dependabot</li> <li>Bump mkdocs-material from 6.1.7 to 6.2.2 (#75) @dependabot</li> <li>Bump watchdog from 0.10.4 to 1.0.2 (#73) @dependabot</li> <li>Bump pytest from 6.1.2 to 6.2.1 (#71) @dependabot</li> <li>Bump wheel from 0.36.1 to 0.36.2 (#70) @dependabot</li> <li>Bump tqdm from 4.54.0 to 4.54.1 (#67) @dependabot</li> <li>Bump mkdocs-material from 6.1.6 to 6.1.7 (#68) @dependabot</li> <li>Bump pre-commit from 2.9.2 to 2.9.3 (#69) @dependabot</li> <li>Bump wheel from 0.36.0 to 0.36.1 (#66) @dependabot</li> <li>Bump wheel from 0.35.1 to 0.36.0 (#64) @dependabot</li> <li>Bump tqdm from 4.53.0 to 4.54.0 (#65) @dependabot</li> <li>Bump pre-commit from 2.8.2 to 2.9.2 (#61) @dependabot</li> <li>Bump mkdocs-material from 6.1.5 to 6.1.6 (#60) @dependabot</li> <li>Bump tqdm from 4.52.0 to 4.53.0 (#62) @dependabot</li> <li>Bump watchdog from 0.10.3 to 0.10.4 (#63) @dependabot</li> <li>Bump tqdm from 4.51.0 to 4.52.0 (#59) @dependabot</li> <li>Bump mkdocs-material from 6.1.4 to 6.1.5 (#58) @dependabot</li> <li>Bump mkdocs-material from 6.1.2 to 6.1.4 (#57) @dependabot</li> <li>Bump pre-commit from 2.8.0 to 2.8.2 (#55) @dependabot</li> <li>Bump mkdocs-material from 6.1.0 to 6.1.2 (#56) @dependabot</li> <li>Bump pytest from 6.1.1 to 6.1.2 (#52) @dependabot</li> <li>Bump pre-commit from 2.7.1 to 2.8.0 (#53) @dependabot</li> <li>Bump tqdm from 4.50.2 to 4.51.0 (#54) @dependabot</li> <li>Bump mkdocs-material from 6.0.2 to 6.1.0 (#51) @dependabot</li> <li>Bump tqdm from 4.50.1 to 4.50.2 (#49) @dependabot</li> <li>Bump tox from 3.20.0 to 3.20.1 (#50) @dependabot</li> <li>Bump pytest from 6.1.0 to 6.1.1 (#48) @dependabot</li> <li>Bump mkdocs-material from 6.0.1 to 6.0.2 (#47) @dependabot</li> <li>Bump flake8 from 3.8.3 to 3.8.4 (#45) @dependabot</li> <li>Bump tqdm from 4.50.0 to 4.50.1 (#44) @dependabot</li> <li>Bump bump2version from 1.0.0 to 1.0.1 (#46) @dependabot</li> <li>Bump tqdm from 4.49.0 to 4.50.0 (#42) @dependabot</li> <li>Bump black from 19.10b0 to 20.8b1 (#43) @dependabot</li> <li>Bump tqdm from 4.46.0 to 4.49.0 (#40) @dependabot</li> <li>Bump pytest from 5.4.2 to 6.1.0 (#39) @dependabot</li> <li>Bump coverage from 5.1 to 5.3 (#38) @dependabot</li> <li>Bump autoflake from 1.3.1 to 1.4 (#41) @dependabot</li> <li>Bump twine from 3.1.1 to 3.2.0 (#37) @dependabot</li> <li>Bump wheel from 0.34.2 to 0.35.1 (#34) @dependabot</li> <li>Bump pytest-cov from 2.9.0 to 2.10.1 (#36) @dependabot</li> <li>Bump watchdog from 0.10.2 to 0.10.3 (#35) @dependabot</li> <li>Bump mkdocs-material from 5.2.2 to 6.0.1 (#33) @dependabot</li> <li>Bump pylint from 2.5.2 to 2.6.0 (#32) @dependabot-preview</li> <li>Bump pre-commit from 2.4.0 to 2.7.1 (#31) @dependabot-preview</li> <li>Bump tox from 3.15.1 to 3.20.0 (#30) @dependabot-preview</li> <li>Bump flake8 from 3.8.2 to 3.8.3 (#29) @dependabot-preview</li> <li>Bump autopep8 from 1.5.2 to 1.5.4 (#28) @dependabot-preview</li> </ul> <p>Published Date: 2020 December 26, 23:51</p>"},{"location":"release-notes/#040-save_csv-options-v040","title":"0.4.0 - save_csv options (v0.4.0)","text":""},{"location":"release-notes/#040-examples-and-data","title":"[0.4.0] - Examples and Data","text":""},{"location":"release-notes/#added","title":"Added","text":"<ul> <li>skipping version 0.3.0 and adding to 0.4.0</li> <li>Adding delimiter option to save_csv<ul> <li>Tests to check if delimiter &gt; 1 character</li> <li>set ',' if none</li> </ul> </li> <li>Adding quotechar option to save_csv</li> <li>Tests to check if quotechar &gt; 1 character<ul> <li>set '\"' if none</li> </ul> </li> <li>Add test of non-list to save_csv</li> </ul>"},{"location":"release-notes/#030-examples-and-data","title":"[0.3.0] - Examples and Data","text":""},{"location":"release-notes/#added_1","title":"Added","text":"<ul> <li>Adding examples (see examples folder)</li> <li>Adding file_function documentation</li> <li>Adding documents site - https://devsetgo.github.io/devsetgo_lib/</li> </ul> <p>Published Date: 2020 April 16, 21:54</p>"},{"location":"release-notes/#improvements-v020","title":"Improvements (v0.2.0)","text":"<ul> <li>Improved Tests</li> <li>Improved Errors</li> <li>Adding more logging</li> </ul> <p>Published Date: 2020 January 26, 21:08</p>"},{"location":"release-notes/#v011-v011","title":"v0.1.1 (v0.1.1)","text":"<ul> <li>New documentation</li> <li>fixes to pypi deployment</li> </ul> <p>Published Date: 2020 January 26, 17:26</p>"},{"location":"release-notes/#beta-release-v010b2","title":"Beta Release (v0.1.0b2)","text":"<p>Basic Function (file and folder) Publish to PyPi (fixing PyPi publishing issues) Needs documentation.</p> <p>Published Date: 2020 January 26, 13:03</p>"},{"location":"release-notes/#pypi-beta-release-v010b","title":"Pypi Beta Release (v0.1.0b)","text":"<p>Change to semantic versioning - Publish to Pypi - Base Functions</p> <p>Published Date: 2020 January 26, 12:53</p>"},{"location":"common_functions/calendar_functions/","title":"Reference","text":""},{"location":"common_functions/calendar_functions/#dsg_lib.common_functions.calendar_functions","title":"<code>dsg_lib.common_functions.calendar_functions</code>","text":"<p>This module provides two main functions to convert between month numbers and their corresponding names.</p> <p>Functions:</p> Name Description <code>get_month</code> <p>int) -&gt; str: Converts an integer month number to its corresponding month name.</p> <p>Args:     month (int): An integer between 1 and 12 representing the month     number.</p> <p>Returns:     str: The full name of the month corresponding to the input month     number.          If the input is not within the range of 1-12, returns \"Invalid          month number\". If the input is not an integer, returns \"Invalid          input, integer is required\".</p> <code>get_month_number</code> <p>str) -&gt; int: Converts a month name to its corresponding month number.</p> <p>Args:     month_name (str): A string containing the full name of a month.</p> <p>Returns:     int: The month number corresponding to the input month name.          If the input is not a valid month name, returns -1. If the          input is not a string, returns \"Invalid input, string is          required\".</p> <p>Example: <pre><code>from dsg_lib.common_functions.calendar_functions import get_month,\n\nget_month_number print(get_month(1))\n\n# Outputs: 'January'\n\nprint(get_month_number('January'))\n\n# Outputs: 1\n</code></pre></p> <p>This module is part of the dsg_lib package and is used for handling and converting between month numbers and names.</p>"},{"location":"common_functions/calendar_functions/#dsg_lib.common_functions.calendar_functions.get_month","title":"<code>get_month(month)</code>","text":"<p>Converts an integer month number to its corresponding month name.</p> <p>Parameters:</p> Name Type Description Default <code>month</code> <code>int</code> <p>An integer or integer-like float between 1 and 12</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The full name of the month corresponding to the input month number.  If the input is not within the range of 1-12, returns \"Invalid  month number\". If the input is not an integer or integer-like  float, returns \"Invalid input, integer is required\".</p> Source code in <code>dsg_lib/common_functions/calendar_functions.py</code> <pre><code>def get_month(month: int) -&gt; str:\n    \"\"\"\n    Converts an integer month number to its corresponding month name.\n\n    Args:\n        month (int): An integer or integer-like float between 1 and 12\n        representing the month number.\n\n    Returns:\n        str: The full name of the month corresponding to the input month number.\n             If the input is not within the range of 1-12, returns \"Invalid\n             month number\". If the input is not an integer or integer-like\n             float, returns \"Invalid input, integer is required\".\n    \"\"\"\n\n    # Define a tuple containing the names of all months\n    months = (\n        'January',\n        'February',\n        'March',\n        'April',\n        'May',\n        'June',\n        'July',\n        'August',\n        'September',\n        'October',\n        'November',\n        'December',\n    )\n\n    # Convert integer-like floats to integers\n    if isinstance(month, float) and month.is_integer():\n        month = int(month)\n\n    # Check if the input month is an integer\n    if not isinstance(month, int):\n        logger.error('Invalid input: %s, integer is required', month)\n        return 'Invalid input, integer is required'\n\n    # Check if the input month is within the range of 1-12\n    if 1 &lt;= month &lt;= 12:\n        logger.info('Returning month name for month number: %s', month)\n        return months[month - 1]\n    else:\n        logger.error('Invalid input: %s, month number should be between 1 and 12', month)\n        return 'Invalid month number'\n</code></pre>"},{"location":"common_functions/calendar_functions/#dsg_lib.common_functions.calendar_functions.get_month_number","title":"<code>get_month_number(month_name)</code>","text":"<p>Converts a month name to its corresponding month number.</p> <p>Parameters:</p> Name Type Description Default <code>month_name</code> <code>str</code> <p>A string containing the full name of a month.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The month number corresponding to the input month name.  If the input is not a valid month name or not a string, returns -1.</p> Source code in <code>dsg_lib/common_functions/calendar_functions.py</code> <pre><code>def get_month_number(month_name: str) -&gt; int:\n    \"\"\"\n    Converts a month name to its corresponding month number.\n\n    Args:\n        month_name (str): A string containing the full name of a month.\n\n    Returns:\n        int: The month number corresponding to the input month name.\n             If the input is not a valid month name or not a string, returns -1.\n    \"\"\"\n\n    # Define a dictionary mapping month names to month numbers\n    month_dict = {\n        'January': 1,\n        'February': 2,\n        'March': 3,\n        'April': 4,\n        'May': 5,\n        'June': 6,\n        'July': 7,\n        'August': 8,\n        'September': 9,\n        'October': 10,\n        'November': 11,\n        'December': 12,\n    }\n\n    # Check if the input month name is a string\n    if not isinstance(month_name, str):\n        logger.error('Invalid input, string is required')\n        return -1\n\n    # Convert the input string to title case and remove leading/trailing spaces\n    month_name = month_name.strip().title()\n\n    # Check if the input month name is a valid key in the dictionary\n    if month_name in month_dict:\n        return month_dict[month_name]\n    else:\n        logger.error('Invalid month name: %s', month_name)\n        return -1\n</code></pre>"},{"location":"common_functions/file_functions/","title":"Reference","text":""},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions","title":"<code>dsg_lib.common_functions.file_functions</code>","text":"<p>file_functions.py</p> <p>This module provides a function to delete a file with a specified name from a specified directory.</p> <p>Functions:</p> Name Description <code>delete_file</code> <p>str) -&gt; str: Deletes a file with the specified file name from the directory specified by the <code>directory_to_files</code> variable. The file type is determined by the file extension, and the file is deleted from the subdirectory corresponding to the file type.</p> <p>Args:     file_name (str): The name of the file to be deleted.</p> <p>Returns:     str: A string indicating that the file has been deleted.</p> <p>Raises:     TypeError: If the file name is not a string. ValueError: If the file     name contains a forward slash or backslash, or if the file type is     not supported. FileNotFoundError: If the file does not exist.</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\nfile_functions.delete_file(\"test.csv\")\n\n# Outputs: 'complete'\n</code></pre></p>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.create_sample_files","title":"<code>create_sample_files(file_name, sample_size)</code>","text":"<p>Create sample CSV and JSON files with random data.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The base name for the sample files (without extension).</p> required <code>sample_size</code> <code>int</code> <p>The number of rows to generate for the sample files.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while creating the sample files.</p> <p>Example: ```python from dsg_lib.common_functions import file_functions</p> <p>file_functions.create_sample_files(\"test\", 100)</p>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.create_sample_files--creates-testcsv-and-testjson-each-with-100-rows-of-random-data","title":"Creates 'test.csv' and 'test.json' each with 100 rows of random data ```","text":"Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def create_sample_files(file_name: str, sample_size: int) -&gt; None:\n    \"\"\"\n    Create sample CSV and JSON files with random data.\n\n    Args:\n        file_name (str): The base name for the sample files (without extension).\n        sample_size (int): The number of rows to generate for the sample files.\n\n    Returns:\n        None\n\n    Raises:\n        Exception: If an error occurs while creating the sample files.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    file_functions.create_sample_files(\"test\", 100)\n\n    # Creates 'test.csv' and 'test.json' each with 100 rows of random data ```\n    \"\"\"\n    logger.debug(f'Creating sample files for {file_name} with {sample_size} rows.')\n\n    try:\n        # Generate the CSV data\n        csv_header = ['name', 'birth_date', 'number']\n        csv_data: List[List[str]] = [csv_header]\n\n        # Generate rows for CSV data\n        for i in range(1, sample_size + 1):\n            r_int: int = random.randint(0, len(first_name) - 1)\n            name = first_name[r_int]\n            row: List[str] = [name, generate_random_date(), str(i)]\n            csv_data.append(row)\n\n        # Save the CSV file\n        csv_file = f'{file_name}.csv'\n        save_csv(csv_file, csv_data)\n\n        # Generate the JSON data\n        json_data: List[dict] = []\n\n        # Generate rows for JSON data\n        for _ in range(1, sample_size + 1):\n            r_int: int = random.randint(0, len(first_name) - 1)\n            name = first_name[r_int]\n            sample_dict: dict = {\n                'name': name,\n                'birthday_date': generate_random_date(),\n            }\n            json_data.append(sample_dict)\n\n        # Save the JSON file\n        json_file: str = f'{file_name}.json'\n        save_json(json_file, json_data)\n\n        # Log the data\n        logger.debug(f'CSV Data: {csv_data}')\n        logger.debug(f'JSON Data: {json_data}')\n\n    except Exception as e:  # pragma: no cover\n        logger.exception(f'Error occurred while creating sample files: {e}')  # pragma: no cover\n        raise  # pragma: no cover\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.delete_file","title":"<code>delete_file(file_name)</code>","text":"<p>Deletes a file with the specified file name from the specified directory. The file type is determined by the file extension.</p> <p>Parameters:</p> Name Type Description Default <code>directory_to_files</code> <code>str</code> <p>The directory where the file is located.</p> required <code>file_name</code> <code>str</code> <p>The name of the file to be deleted.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A message indicating whether the file has been deleted successfully</p> <code>str</code> <p>or an error occurred.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the directory or file name is not a string. ValueError: If</p> <code>is not supported. FileNotFoundError</code> <p>If the file does not exist.</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\nfile_functions.delete_file(\"test.csv\")\n\n# Outputs: 'File deleted successfully'\n</code></pre></p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def delete_file(file_name: str) -&gt; str:\n    \"\"\"\n    Deletes a file with the specified file name from the specified directory.\n    The file type is determined by the file extension.\n\n    Args:\n        directory_to_files (str): The directory where the file is located.\n        file_name (str): The name of the file to be deleted.\n\n    Returns:\n        str: A message indicating whether the file has been deleted successfully\n        or an error occurred.\n\n    Raises:\n        TypeError: If the directory or file name is not a string. ValueError: If\n        the file name contains a forward slash or backslash, or if the file type\n        is not supported. FileNotFoundError: If the file does not exist.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    file_functions.delete_file(\"test.csv\")\n\n    # Outputs: 'File deleted successfully'\n    ```\n    \"\"\"\n    logger.info(f'Deleting file: {file_name}')\n\n    # Check that the file name is a string\n    if not isinstance(file_name, str):\n        raise TypeError(f'{file_name} is not a valid string')\n\n    # Split the file name into its name and extension components\n    file_name, file_ext = os.path.splitext(file_name)\n\n    # Check that the file name does not contain a forward slash or backslash\n    if os.path.sep in file_name:\n        raise ValueError(f'{file_name} cannot contain {os.path.sep}')\n\n    # Check that the file type is supported\n    if file_ext not in directory_map:\n        raise ValueError(\n            f\"unsupported file type: {file_ext}. Supported file types are: {', '.join(directory_map.keys())}\"\n        )\n\n    # Construct the full file path\n    file_directory = Path.cwd() / directory_to_files / directory_map[file_ext]\n    file_path = file_directory / f'{file_name}{file_ext}'\n\n    # Check that the file exists\n    if not file_path.is_file():\n        raise FileNotFoundError(f'file not found: {file_name}{file_ext}')\n\n    # Delete the file\n    os.remove(file_path)\n    logger.info(f'File {file_name}{file_ext} deleted from file path: {file_path}')\n\n    # Return a string indicating that the file has been deleted\n    return 'complete'\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.generate_random_date","title":"<code>generate_random_date()</code>","text":"<p>Generate a random datetime string in the format yyyy-mm-dd hh:mm:ss.ffffff.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A randomly generated datetime string.</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\nrandom_date = file_functions.generate_random_date()\n# Returns: '1992-03-15 10:30:45.123456'\n</code></pre></p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def generate_random_date() -&gt; str:\n    \"\"\"\n    Generate a random datetime string in the format yyyy-mm-dd hh:mm:ss.ffffff.\n\n    Returns:\n        str: A randomly generated datetime string.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n    random_date = file_functions.generate_random_date()\n    # Returns: '1992-03-15 10:30:45.123456'\n    ```\n    \"\"\"\n    # Define the minimum and maximum years for the date range\n    min_year: int = 1905\n    max_year: int = datetime.now().year\n\n    # Generate random values for the year, month, day, hour, minute, and second\n    year: int = random.randrange(min_year, max_year + 1)\n    month: int = random.randint(1, 12)\n    day: int = random.randint(1, 28)\n    hour: int = random.randint(0, 12)\n    minute: int = random.randint(0, 59)\n    second: int = random.randint(0, 59)\n\n    # Create a datetime object with the random values\n    date_value: datetime = datetime(year, month, day, hour, minute, second)\n\n    # Format the datetime string and return it\n    return f'{date_value:%Y-%m-%d %H:%M:%S.%f}'\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.open_csv","title":"<code>open_csv(file_name, delimiter=',', quote_level='minimal', skip_initial_space=True)</code>","text":"<p>Opens a CSV file with the specified file name and returns its contents as a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to open. Should include the '.csv'</p> required <code>extension.</code> <code>delimiter (str</code> <p>The character used to separate</p> required <code>fields</code> <code>in the CSV file. Defaults to ','. quote_level (str</code> required <code>optional)</code> <p>Whether to skip initial whitespace in the CSV file. Defaults</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>The contents of the CSV file as a list of dictionaries. Each</p> <code>list</code> <p>dictionary represents a row in the CSV file, where the keys are column</p> <code>list</code> <p>names and the values are the data for those columns.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>file_name</code> is not a string. ValueError: If <code>quote_level</code></p> <code>is not a valid level. FileNotFoundError</code> <p>If the file does not exist.</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions data =\nfile_functions.open_csv(\"test.csv\", delimiter=\";\", quote_level=\"all\",\nskip_initial_space=False)  # Returns: [{'column1': 'value1', 'column2':\n'value2'}]\n</code></pre></p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def open_csv(\n    file_name: str,\n    delimiter: str = ',',\n    quote_level: str = 'minimal',\n    skip_initial_space: bool = True,\n) -&gt; list:\n    \"\"\"\n    Opens a CSV file with the specified file name and returns its contents as a\n    list of dictionaries.\n\n    Args:\n        file_name (str): The name of the file to open. Should include the '.csv'\n        extension. delimiter (str, optional): The character used to separate\n        fields in the CSV file. Defaults to ','. quote_level (str, optional):\n        The quoting level used in the CSV file. Valid levels are \"none\",\n        \"minimal\", and \"all\". Defaults to \"minimal\". skip_initial_space (bool,\n        optional): Whether to skip initial whitespace in the CSV file. Defaults\n        to True.\n\n    Returns:\n        list: The contents of the CSV file as a list of dictionaries. Each\n        dictionary represents a row in the CSV file, where the keys are column\n        names and the values are the data for those columns.\n\n    Raises:\n        TypeError: If `file_name` is not a string. ValueError: If `quote_level`\n        is not a valid level. FileNotFoundError: If the file does not exist.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions data =\n    file_functions.open_csv(\"test.csv\", delimiter=\";\", quote_level=\"all\",\n    skip_initial_space=False)  # Returns: [{'column1': 'value1', 'column2':\n    'value2'}]\n    ```\n    \"\"\"\n    # A dictionary that maps quote levels to csv quoting constants\n    quote_levels = {\n        'none': csv.QUOTE_NONE,\n        'minimal': csv.QUOTE_MINIMAL,\n        'all': csv.QUOTE_ALL,\n    }\n\n    # Check that file name is a string\n    if not isinstance(file_name, str):\n        error = f'{file_name} is not a valid string'\n        logger.error(error)\n        raise TypeError(error)\n\n    # Check that quote level is valid\n    quote_level = quote_level.lower()\n    if quote_level not in quote_levels:\n        error = f\"Invalid quote level: {quote_level}. Valid levels are: {', '.join(quote_levels)}\"\n        logger.error(error)\n        raise ValueError(error)\n    quoting = quote_levels[quote_level]\n\n    # Add extension to file name and create file path\n    file_name = f'{file_name}.csv'\n    file_directory = Path.cwd().joinpath(directory_to_files).joinpath('csv')\n    file_path = file_directory.joinpath(file_name)\n\n    # Check that file exists\n    if not file_path.is_file():\n        error = f'File not found: {file_path}'\n        logger.error(error)\n        raise FileNotFoundError(error)\n\n    # Read CSV file\n    data = []\n    with file_path.open(encoding='utf-8') as f:\n        reader = csv.DictReader(\n            f,\n            delimiter=delimiter,\n            quoting=quoting,\n            skipinitialspace=skip_initial_space,\n        )\n        for row in reader:\n            data.append(dict(row))\n\n    logger.info(f'File opened: {file_name}')\n    return data\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.open_json","title":"<code>open_json(file_name)</code>","text":"<p>Open a JSON file and load its contents into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the JSON file to open.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The contents of the JSON file as a dictionary.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the file name is not a string. FileNotFoundError: If the</p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def open_json(file_name: str) -&gt; dict:\n    \"\"\"\n    Open a JSON file and load its contents into a dictionary.\n\n    Args:\n        file_name (str): The name of the JSON file to open.\n\n    Returns:\n        dict: The contents of the JSON file as a dictionary.\n\n    Raises:\n        TypeError: If the file name is not a string. FileNotFoundError: If the\n        file does not exist.\n    \"\"\"\n    # Check if file name is a string\n    if not isinstance(file_name, str):\n        error = f'{file_name} is not a valid string'\n        logger.error(error)\n        raise TypeError(error)\n\n    file_directory = Path(directory_to_files) / directory_map['.json']\n    file_save = file_directory / file_name\n\n    # Check if path correct\n    if not file_save.is_file():\n        error = f'file not found error: {file_save}'\n        logger.exception(error)\n        raise FileNotFoundError(error)\n\n    # open file\n    with open(file_save) as read_file:\n        # load file into data variable\n        result = json.load(read_file)\n\n    logger.info(f'File Opened: {file_name}')\n    return result\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.open_text","title":"<code>open_text(file_name)</code>","text":"<p>Opens a text file with the specified file name and returns its contents as a string.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to open. Should include the '.txt'</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The contents of the text file as a string.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the <code>file_name</code> parameter is not a string or contains a</p> <code>forward slash. FileNotFoundError</code> <p>If the file does not exist.</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\ndata = file_functions.open_text(\"test.txt\")\n# Returns: 'This is a test text file.'\n</code></pre></p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def open_text(file_name: str) -&gt; str:\n    \"\"\"\n    Opens a text file with the specified file name and returns its contents as a\n    string.\n\n    Args:\n        file_name (str): The name of the file to open. Should include the '.txt'\n        extension.\n\n    Returns:\n        str: The contents of the text file as a string.\n\n    Raises:\n        TypeError: If the `file_name` parameter is not a string or contains a\n        forward slash. FileNotFoundError: If the file does not exist.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n    data = file_functions.open_text(\"test.txt\")\n    # Returns: 'This is a test text file.'\n    ```\n    \"\"\"\n    # Replace backslashes with forward slashes in the file name\n    if '\\\\' in file_name:  # pragma: no cover\n        file_name = file_name.replace('\\\\', '/')  # pragma: no cover\n\n    # Check that file_name does not contain invalid characters\n    if '/' in file_name:\n        logger.error(f'{file_name} cannot contain /')\n        raise TypeError(f'{file_name} cannot contain /')\n\n    # Get the path to the text directory and the file path\n    file_directory = os.path.join(directory_to_files, 'text')\n    file_path = Path.cwd().joinpath(file_directory, file_name)\n\n    # Check if the file exists\n    if not file_path.is_file():\n        raise FileNotFoundError(f'file not found error: {file_path}')\n\n    # Open the file and read the data\n    with open(file_path, 'r', encoding='utf-8') as file:\n        data = file.read()\n\n    logger.info(f'File opened: {file_path}')\n    return data\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.save_csv","title":"<code>save_csv(file_name, data, root_folder=None, delimiter=',', quotechar='\"')</code>","text":"<p>Saves a list of dictionaries as a CSV file with the specified file name in the specified directory. Each dictionary in the list should represent a row in the CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to save the data in. Should</p> required <code>include</code> <code>the '.csv' extension. data (list</code> <p>The data to be saved. Each</p> required <code>optional)</code> <p>The root directory where the file will be saved. If None, the</p> required <code>(str,</code> <code>optional</code> <p>The character used to separate fields in the CSV file.</p> required <code>Defaults</code> <code>to ','. quotechar (str</code> <p>The character used to quote</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A message indicating whether the file has been saved successfully</p> <code>str</code> <p>or an error occurred.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the data is not a list, or the file name, delimiter, or</p> <code>quotechar is not a string. ValueError</code> <p>If the file name does not end</p> <p>Example: ```python from dsg_lib.common_functions import file_functions</p> <p>data = [{\"column1\": \"value1\", \"column2\": \"value2\"}]</p> <p>file_functions.save_csv(\"test.csv\", data, \"/path/to/directory\", delimiter=\";\", quotechar=\"'\")</p>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.save_csv--saves-data-to-pathtodirectorytestcsv","title":"Saves data to '/path/to/directory/test.csv'","text":"<p>```</p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def save_csv(\n    file_name: str,\n    data: list,\n    root_folder: str = None,\n    delimiter: str = ',',\n    quotechar: str = '\"',\n) -&gt; str:\n    \"\"\"\n    Saves a list of dictionaries as a CSV file with the specified file name in\n    the specified directory. Each dictionary in the list should represent a row\n    in the CSV file.\n\n    Args:\n        file_name (str): The name of the file to save the data in. Should\n        include the '.csv' extension. data (list): The data to be saved. Each\n        element of the list should be a dictionary where the keys are column\n        names and the values are the data for those columns. root_folder (str,\n        optional): The root directory where the file will be saved. If None, the\n        file will be saved in the current directory. Defaults to None. delimiter\n        (str, optional): The character used to separate fields in the CSV file.\n        Defaults to ','. quotechar (str, optional): The character used to quote\n        fields in the CSV file. Defaults to '\"'.\n\n    Returns:\n        str: A message indicating whether the file has been saved successfully\n        or an error occurred.\n\n    Raises:\n        TypeError: If the data is not a list, or the file name, delimiter, or\n        quotechar is not a string. ValueError: If the file name does not end\n        with '.csv'.\n\n    Example: ```python\n    from dsg_lib.common_functions import file_functions\n\n    data = [{\"column1\": \"value1\", \"column2\": \"value2\"}]\n\n    file_functions.save_csv(\"test.csv\", data, \"/path/to/directory\", delimiter=\";\", quotechar=\"'\")\n\n    # Saves data to '/path/to/directory/test.csv'\n    ```\n    \"\"\"\n    # Set the root folder to directory_to_files if None\n    if root_folder is None:\n        root_folder = directory_to_files\n\n    # Create the csv directory if it does not exist\n    csv_directory = Path(root_folder) / 'csv'\n    csv_directory.mkdir(parents=True, exist_ok=True)\n\n    # Check that delimiter and quotechar are single characters\n    if len(delimiter) != 1:\n        raise TypeError(f'{delimiter} can only be a single character')\n    if len(quotechar) != 1:\n        raise TypeError(f'{quotechar} can only be a single character')\n\n    # Check that data is a list\n    if not isinstance(data, list):\n        raise TypeError(f'{data} is not a valid list')\n\n    # Check that file_name is a string and does not contain invalid characters\n    if not isinstance(file_name, str) or '/' in file_name or '\\\\' in file_name:\n        raise TypeError(f'{file_name} is not a valid file name')\n\n    # Add extension to file_name if needed\n    if not file_name.endswith('.csv'):\n        file_name += '.csv'\n\n    # Create the file path\n    file_path = csv_directory / file_name\n\n    # Write data to file\n    with open(file_path, 'w', encoding='utf-8', newline='') as csv_file:\n        csv_writer = csv.writer(csv_file, delimiter=delimiter, quotechar=quotechar)\n        csv_writer.writerows(data)\n\n    logger.info(f'File Create: {file_name}')\n    return 'complete'\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.save_json","title":"<code>save_json(file_name, data, root_folder=None)</code>","text":"<p>Saves a dictionary or a list as a JSON file with the specified file name in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to save the data in. Should</p> required <code>include</code> <code>the '.json' extension. data (list or dict</code> <p>The data to be</p> required <code>saved.</code> <code>root_folder (str</code> <p>The root directory where the file</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A message indicating whether the file has been saved successfully</p> <code>str</code> <p>or an error occurred.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the data is not a list or a dictionary, or the file name</p> <code>or directory is not a string. ValueError</code> <p>If the file name contains a</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\ndata = {\"key\": \"value\"}\n\nfile_functions.save_json(\"test.json\", data, \"/path/to/directory\")\n\n# Saves data to '/path/to/directory/test.json'\n</code></pre></p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def save_json(file_name: str, data, root_folder: str = None) -&gt; str:\n    \"\"\"\n    Saves a dictionary or a list as a JSON file with the specified file name in\n    the specified directory.\n\n    Args:\n        file_name (str): The name of the file to save the data in. Should\n        include the '.json' extension. data (list or dict): The data to be\n        saved. root_folder (str, optional): The root directory where the file\n        will be saved. Defaults to None, which means the file will be saved in\n        the 'data' directory.\n\n    Returns:\n        str: A message indicating whether the file has been saved successfully\n        or an error occurred.\n\n    Raises:\n        TypeError: If the data is not a list or a dictionary, or the file name\n        or directory is not a string. ValueError: If the file name contains a\n        forward slash or backslash, or if the file name does not end with\n        '.json'.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    data = {\"key\": \"value\"}\n\n    file_functions.save_json(\"test.json\", data, \"/path/to/directory\")\n\n    # Saves data to '/path/to/directory/test.json'\n    ```\n    \"\"\"\n    try:\n        # Validate inputs\n        if not isinstance(data, (list, dict)):\n            raise TypeError(f'data must be a list or a dictionary instead of type {type(data)}')\n        if '/' in file_name or '\\\\' in file_name:\n            raise ValueError(f'{file_name} cannot contain / or \\\\')\n\n        # Add extension if not present in file_name\n        if not file_name.endswith('.json'):  # pragma: no cover\n            file_name += '.json'  # pragma: no cover\n\n        if root_folder is None:\n            root_folder = directory_to_files\n\n        # Determine directory\n        json_directory = Path(root_folder) / 'json'\n\n        # Construct file path\n        file_path = json_directory / file_name\n\n        # Create the json directory if it does not exist\n        json_directory.mkdir(parents=True, exist_ok=True)\n\n        # Write data to file\n        with open(file_path, 'w') as write_file:\n            json.dump(data, write_file)\n\n        # Log success message\n        logger.info(f'File created: {file_path}')\n\n        return 'File saved successfully'\n\n    except (TypeError, ValueError) as e:\n        logger.error(f'Error creating file {file_name}: {e}')\n        raise\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.save_text","title":"<code>save_text(file_name, data, root_folder=None)</code>","text":"<p>Saves a string of text to a file with the specified file name in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to save the data in. Should not</p> required <code>include</code> <code>the '.txt' extension. data (str</code> <p>The text data to be saved.</p> required <code>root_folder</code> <code>str</code> <p>The root directory where the file will be</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A message indicating whether the file has been saved successfully</p> <code>str</code> <p>or an error occurred.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the <code>data</code> parameter is not a string, or the <code>file_name</code></p> <code>contains a forward slash or backslash. FileNotFoundError</code> <p>If the</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\nfile_functions.save_text(\"test\", \"This is a test text file.\", \"/path/to/directory\")\n\n# Saves data to '/path/to/directory/test.txt'\n</code></pre></p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def save_text(file_name: str, data: str, root_folder: str = None) -&gt; str:\n    \"\"\"\n    Saves a string of text to a file with the specified file name in the\n    specified directory.\n\n    Args:\n        file_name (str): The name of the file to save the data in. Should not\n        include the '.txt' extension. data (str): The text data to be saved.\n        root_folder (str, optional): The root directory where the file will be\n        saved. If None, the file will be saved in the current directory.\n        Defaults to None.\n\n    Returns:\n        str: A message indicating whether the file has been saved successfully\n        or an error occurred.\n\n    Raises:\n        TypeError: If the `data` parameter is not a string, or the `file_name`\n        contains a forward slash or backslash. FileNotFoundError: If the\n        directory does not exist.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    file_functions.save_text(\"test\", \"This is a test text file.\", \"/path/to/directory\")\n\n    # Saves data to '/path/to/directory/test.txt'\n    ```\n    \"\"\"\n    # If no root folder is provided, use the default directory\n    if root_folder is None:  # pragma: no cover\n        root_folder = directory_to_files  # pragma: no cover\n\n    # Determine the directory for text files\n    text_directory = Path(root_folder) / 'text'\n\n    # Construct the file path for text files\n    file_path = text_directory / f'{file_name}.txt'\n\n    # Create the text directory if it does not exist\n    text_directory.mkdir(parents=True, exist_ok=True)\n\n    # Check that data is a string and that file_name does not contain invalid\n    # characters\n    if not isinstance(data, str):\n        logger.error(f'{file_name} is not a valid string')\n        raise TypeError(f'{file_name} is not a valid string')\n    elif '/' in file_name or '\\\\' in file_name:\n        logger.error(f'{file_name} cannot contain \\\\ or /')\n        raise ValueError(f'{file_name} cannot contain \\\\ or /')\n\n    # Open or create the file and write the data\n    with open(file_path, 'w+', encoding='utf-8') as file:\n        file.write(data)\n\n    logger.info(f'File created: {file_path}')\n    return 'complete'\n</code></pre>"},{"location":"common_functions/folder_functions/","title":"Reference","text":""},{"location":"common_functions/folder_functions/#dsg_lib.common_functions.folder_functions","title":"<code>dsg_lib.common_functions.folder_functions</code>","text":"<p>This module contains functions for working with directories and files.</p> <p>Functions:</p> Name Description <code>last_data_files_changed</code> <p>Get the last modified file in a</p> <code>get_directory_list</code> <p>Get a list of directories in the</p> <code>specified directory. make_folder</code> <p>Make a folder in a</p> <code>specific directory. remove_folder</code> <p>Remove a folder from the</p> <p>Example: <pre><code>from dsg_lib.common_functions import folder_functions\n\n# Get the last modified file in a directory time_stamp, file_path =\nfolder_functions.last_data_files_changed(\"/path/to/directory\")  # Returns:\n(datetime.datetime(2022, 1, 1, 12, 0, 0), '/path/to/directory/test.txt')\n\n# Get a list of directories in the specified directory directories =\nfolder_functions.get_directory_list(\"/path/to/directory\")  # Returns:\n['/path/to/directory/dir1', '/path/to/directory/dir2']\n\n# Make a folder in a specific directory\nfolder_functions.make_folder(\"/path/to/directory/new_folder\")  # Creates a new\nfolder at '/path/to/directory/new_folder'\n\n# Remove a folder from the specified directory\nfolder_functions.remove_folder(\"/path/to/directory/old_folder\")  # Removes the\nfolder at '/path/to/directory/old_folder'\n</code></pre></p>"},{"location":"common_functions/folder_functions/#dsg_lib.common_functions.folder_functions.get_directory_list","title":"<code>get_directory_list(file_directory)</code>","text":"<p>Get a list of directories in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>file_directory</code> <code>str</code> <p>The path of the directory to check.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of directories in the specified directory.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the directory does not exist.</p> <p>Example: <pre><code>from dsg_lib import file_functions\n\ndirectories = file_functions.get_directory_list(\"/path/to/directory\")\n\n# Returns: ['/path/to/directory/dir1', '/path/to/directory/dir2']\n</code></pre></p> Source code in <code>dsg_lib/common_functions/folder_functions.py</code> <pre><code>def get_directory_list(file_directory: str) -&gt; List[str]:\n    \"\"\"\n    Get a list of directories in the specified directory.\n\n    Args:\n        file_directory (str): The path of the directory to check.\n\n    Returns:\n        List[str]: A list of directories in the specified directory.\n\n    Raises:\n        FileNotFoundError: If the directory does not exist.\n\n    Example:\n    ```python\n    from dsg_lib import file_functions\n\n    directories = file_functions.get_directory_list(\"/path/to/directory\")\n\n    # Returns: ['/path/to/directory/dir1', '/path/to/directory/dir2']\n    ```\n    \"\"\"\n    # Create a Path object for the specified directory\n    file_path = Path.cwd().joinpath(file_directory)\n\n    try:\n        # Use a list comprehension to create a list of directories in the\n        # specified directory\n        direct_list = [x for x in file_path.iterdir() if x.is_dir()]\n\n        # Log a message indicating that the list of directories was retrieved\n        logger.info(f'Retrieved list of directories: {file_directory}')\n\n        # Return the list of directories\n        return direct_list\n\n    except FileNotFoundError as err:\n        # Log an error message if the specified directory does not exist\n        logger.error(err)\n</code></pre>"},{"location":"common_functions/folder_functions/#dsg_lib.common_functions.folder_functions.last_data_files_changed","title":"<code>last_data_files_changed(directory_path)</code>","text":"<p>Get the last modified file in a directory and return its modification time and path.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>str</code> <p>The path of the directory to check.</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>Tuple[datetime, str]: A tuple containing the modification time and path</p> <code>str</code> <p>of the last modified file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the directory does not exist.</p> <p>Example: <pre><code>from dsg_lib import file_functions\n\ntime_stamp, file_path = file_functions.last_data_files_changed(\"/path/to/directory\")\n\n# Returns: (datetime.datetime(2022, 1, 1, 12, 0, 0), '/path/to/directory/test.txt')\n</code></pre></p> Source code in <code>dsg_lib/common_functions/folder_functions.py</code> <pre><code>def last_data_files_changed(directory_path: str) -&gt; Tuple[datetime, str]:\n    \"\"\"\n    Get the last modified file in a directory and return its modification time\n    and path.\n\n    Args:\n        directory_path (str): The path of the directory to check.\n\n    Returns:\n        Tuple[datetime, str]: A tuple containing the modification time and path\n        of the last modified file.\n\n    Raises:\n        FileNotFoundError: If the directory does not exist.\n\n    Example:\n    ```python\n    from dsg_lib import file_functions\n\n    time_stamp, file_path = file_functions.last_data_files_changed(\"/path/to/directory\")\n\n    # Returns: (datetime.datetime(2022, 1, 1, 12, 0, 0), '/path/to/directory/test.txt')\n    ```\n    \"\"\"\n    try:\n        # Use a generator expression to find the last modified file in the\n        # directory\n        time, file_path = max((f.stat().st_mtime, f) for f in directory_path.iterdir())\n\n        # Convert the modification time to a datetime object\n        time_stamp = datetime.fromtimestamp(time)\n\n        # Log a message to indicate that the directory was checked for the last\n        # modified file\n        logger.info(f'Directory checked for last change: {directory_path}')\n\n        # Return the modification time and path of the last modified file\n        return time_stamp, file_path\n\n    except Exception as err:\n        # Log an error message if an exception occurs, and return a default\n        # value to indicate an error\n        logger.error(err)\n        return None, None\n</code></pre>"},{"location":"common_functions/folder_functions/#dsg_lib.common_functions.folder_functions.remove_folder","title":"<code>remove_folder(file_directory)</code>","text":"<p>Remove a folder from the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>file_directory</code> <code>str</code> <p>The directory containing the folder to be removed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified directory does not exist. OSError:</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\nfile_functions.remove_folder(\"/path/to/directory/old_folder\")\n\n# Removes the folder at '/path/to/directory/old_folder'\n</code></pre></p> Source code in <code>dsg_lib/common_functions/folder_functions.py</code> <pre><code>def remove_folder(file_directory: str) -&gt; None:\n    \"\"\"\n    Remove a folder from the specified directory.\n\n    Args:\n        file_directory (str): The directory containing the folder to be removed.\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: If the specified directory does not exist. OSError:\n        If the specified folder could not be removed.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    file_functions.remove_folder(\"/path/to/directory/old_folder\")\n\n    # Removes the folder at '/path/to/directory/old_folder'\n    ```\n    \"\"\"\n    try:\n        # Create a Path object for the specified directory\n        path = Path(file_directory)\n\n        # Use the rmdir method of the Path object to remove the folder\n        path.rmdir()\n\n        # Log a message indicating that the folder was removed\n        logger.info(f'Folder removed: {file_directory}')\n\n    except FileNotFoundError as err:\n        # Log an error message if the specified directory does not exist\n        logger.error(err)\n\n        # Raise the FileNotFoundError exception to be handled by the calling\n        # code\n        raise\n\n    except OSError as err:\n        # Log an error message if the folder could not be removed\n        logger.error(err)\n\n        # Raise the OSError exception to be handled by the calling code\n        raise\n</code></pre>"},{"location":"common_functions/logging/","title":"Reference","text":""},{"location":"common_functions/logging/#dsg_lib.common_functions.logging_config","title":"<code>dsg_lib.common_functions.logging_config</code>","text":"<p>This module provides a function to configure and set up a logger using the loguru package.</p> <p>The <code>config_log</code> function takes several optional parameters to customize the logger's behavior, including the logging directory, log name, logging level, log rotation size, log retention period, and more. It also provides an option to append the application name to the log file name.</p> <p>Example: <pre><code>from dsg_lib.common_functions.logging_config import config_log\n\nconfig_log(\n    logging_directory='logs',  # Directory where logs will be stored\n    log_name='log',  # Name of the log file (extension will be added automatically set v0.12.2)\n    logging_level='DEBUG',  # Logging level\n    log_rotation='100 MB',  # Log rotation size\n    log_retention='30 days',  # Log retention period\n    log_backtrace=True,  # Enable backtrace\n    log_format=\"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSSSSS}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - &lt;level&gt;{message}&lt;/level&gt;\",  # Log format\n    log_serializer=False,  # Disable log serialization\n    log_diagnose=True,  # Enable diagnose\n    app_name='my_app',  # Application name\n    append_app_name=True  # Append application name to the log file name\n)\n\nlogger.debug(\"This is a debug message\")\nlogger.info(\"This is an info message\")\nlogger.error(\"This is an error message\")\nlogger.warning(\"This is a warning message\")\nlogger.critical(\"This is a critical message\")\n</code></pre></p>"},{"location":"common_functions/logging/#dsg_lib.common_functions.logging_config.config_log","title":"<code>config_log(logging_directory='log', log_name='log', logging_level='INFO', log_rotation='100 MB', log_retention='30 days', log_backtrace=False, log_format=None, log_serializer=False, log_diagnose=False, app_name=None, append_app_name=False)</code>","text":"<p>Configures and sets up a logger using the loguru package.</p> <p>Parameters: - logging_directory (str): The directory where logs will be stored. Default is \"log\". - log_name (str): The name of the log file. Default is \"log\" (extension automatically set in 0.12.2). - logging_level (str): The logging level. Default is \"INFO\". - log_rotation (str): The log rotation size. Default is \"100 MB\". - log_retention (str): The log retention period. Default is \"30 days\". - log_backtrace (bool): Whether to enable backtrace. Default is False. - log_format (str): The log format. Default is None. - log_serializer (bool): Whether to disable log serialization. Default is False. - log_diagnose (bool): Whether to enable diagnose. Default is False. - app_name (str): The application name. Default is None. - append_app_name (bool): Whether to append the application name to the log file name. Default is False.</p> <p>Raises: - ValueError: If the provided logging level is not valid.</p> <p>Usage Example: <pre><code>from logging_config import config_log\n\nconfig_log(\n    logging_directory='logs',\n    log_name='app.log',\n    logging_level='DEBUG',\n    log_rotation='500 MB',\n    log_retention='10 days',\n    log_backtrace=True,\n    log_format=\"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSSSSS}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - &lt;level&gt;{message}&lt;/level&gt;\",\n    log_serializer=False,\n    log_diagnose=True,\n    app_name='my_app',\n    append_app_name=True\n)\n</code></pre></p> Source code in <code>dsg_lib/common_functions/logging_config.py</code> <pre><code>def config_log(\n    logging_directory: str = 'log',\n    log_name: str = 'log',\n    logging_level: str = 'INFO',\n    log_rotation: str = '100 MB',\n    log_retention: str = '30 days',\n    log_backtrace: bool = False,\n    log_format: str = None,\n    log_serializer: bool = False,\n    log_diagnose: bool = False,\n    app_name: str = None,\n    append_app_name: bool = False,\n):\n    \"\"\"\n    Configures and sets up a logger using the loguru package.\n\n    Parameters:\n    - logging_directory (str): The directory where logs will be stored. Default is \"log\".\n    - log_name (str): The name of the log file. Default is \"log\" (extension automatically set in 0.12.2).\n    - logging_level (str): The logging level. Default is \"INFO\".\n    - log_rotation (str): The log rotation size. Default is \"100 MB\".\n    - log_retention (str): The log retention period. Default is \"30 days\".\n    - log_backtrace (bool): Whether to enable backtrace. Default is False.\n    - log_format (str): The log format. Default is None.\n    - log_serializer (bool): Whether to disable log serialization. Default is False.\n    - log_diagnose (bool): Whether to enable diagnose. Default is False.\n    - app_name (str): The application name. Default is None.\n    - append_app_name (bool): Whether to append the application name to the log file name. Default is False.\n\n    Raises:\n    - ValueError: If the provided logging level is not valid.\n\n    Usage Example:\n    ```python\n    from logging_config import config_log\n\n    config_log(\n        logging_directory='logs',\n        log_name='app.log',\n        logging_level='DEBUG',\n        log_rotation='500 MB',\n        log_retention='10 days',\n        log_backtrace=True,\n        log_format=\"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSSSSS}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - &lt;level&gt;{message}&lt;/level&gt;\",\n        log_serializer=False,\n        log_diagnose=True,\n        app_name='my_app',\n        append_app_name=True\n    )\n    ```\n    \"\"\"\n\n    # If the log_name ends with \".log\", remove the extension\n    if log_name.endswith('.log'):\n        log_name = log_name.replace('.log', '')  # pragma: no cover\n\n    # If the log_name ends with \".json\", remove the extension\n    if log_name.endswith('.json'):\n        log_name = log_name.replace('.json', '')  # pragma: no cover\n\n    # Set default log format if not provided\n    if log_format is None:  # pragma: no cover\n        log_format = '&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSSSSS}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt; {name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - &lt;level&gt;{message}&lt;/level&gt;'  # pragma: no cover\n\n    if log_serializer is True:\n        log_name = f'{log_name}.json'  # pragma: no cover\n    else:\n        log_name = f'{log_name}.log'  # pragma: no cover\n\n    # Validate logging level\n    log_levels: list = ['DEBUG', 'INFO', 'ERROR', 'WARNING', 'CRITICAL']\n    if logging_level.upper() not in log_levels:\n        raise ValueError(f'Invalid logging level: {logging_level}. Valid levels are: {log_levels}')\n\n    # Generate unique trace ID\n    trace_id: str = str(uuid4())\n    logger.configure(extra={'app_name': app_name, 'trace_id': trace_id})\n\n    # Append app name to log format if provided\n    if app_name is not None:\n        log_format += ' | app_name: {extra[app_name]}'\n\n    # Remove any previously added sinks\n    logger.remove()\n\n    # Append app name to log file name if required\n    if append_app_name is True and app_name is not None:\n        log_name = log_name.replace('.', f'_{app_name}.')\n\n    # Construct log file path\n    log_path = Path.cwd().joinpath(logging_directory).joinpath(log_name)\n\n    # Add loguru logger with specified configuration\n    logger.add(\n        log_path,\n        level=logging_level.upper(),\n        format=log_format,\n        enqueue=True,\n        backtrace=log_backtrace,\n        rotation=log_rotation,\n        retention=log_retention,\n        compression='zip',\n        serialize=log_serializer,\n        diagnose=log_diagnose,\n    )\n\n    class InterceptHandler(logging.Handler):\n        \"\"\"\n        Interceptor for standard logging.\n\n        This class intercepts standard Python logging messages and redirects\n        them to the Loguru logger. It is used as a handler for the standard\n        Python logger.\n\n        Attributes:\n            level (str): The minimum severity level of messages that this\n            handler should handle.\n\n        Usage Example: ```python from dsg_lib.logging_config import\n        InterceptHandler import logging\n\n        # Create a standard Python logger logger =\n        logging.getLogger('my_logger')\n\n        # Create an InterceptHandler handler = InterceptHandler()\n\n        # Add the InterceptHandler to the logger logger.addHandler(handler)\n\n        # Now, when you log a message using the standard Python logger, it will\n        be intercepted and redirected to the Loguru logger logger.info('This is\n        an info message') ```\n        \"\"\"\n\n        def emit(self, record):\n            # Get corresponding Loguru level if it exists\n            try:\n                level = logger.level(record.levelname).name\n            except ValueError:  # pragma: no cover\n                level = record.levelno  # pragma: no cover\n\n            # Find caller from where originated the logged message\n            frame, depth = logging.currentframe(), 2\n            while frame.f_code.co_filename == logging.__file__:  # pragma: no cover\n                frame = frame.f_back  # pragma: no cover\n                depth += 1  # pragma: no cover\n\n            # Log the message using loguru\n            logger.opt(depth=depth, exception=record.exc_info).log(\n                level, record.getMessage()\n            )  # pragma: no cover\n\n    # Configure standard logging to use interceptor handler\n    logging.basicConfig(handlers=[InterceptHandler()], level=logging_level.upper())\n\n    # Add interceptor handler to all existing loggers\n    for name in logging.root.manager.loggerDict:\n        logging.getLogger(name).addHandler(InterceptHandler())\n\n    # Set the root logger's level to the lowest level possible\n    logging.getLogger().setLevel(logging.NOTSET)\n</code></pre>"},{"location":"common_functions/regex/","title":"Reference","text":""},{"location":"common_functions/regex/#dsg_lib.common_functions.patterns","title":"<code>dsg_lib.common_functions.patterns</code>","text":"<p>This module contains functions for pattern searching in text using regular expressions.</p> <p>The main function in this module is <code>pattern_between_two_char</code>, which searches for all patterns between two characters in a given string. The function uses Python's built-in <code>re</code> module for regex searching and the <code>loguru</code> module for logging.</p> <p>Functions:</p> Name Description <code>pattern_between_two_char</code> <p>str, left_characters: str,</p> <code>right_characters</code> <p>str) -&gt; dict: Searches for all patterns between two characters (left and right) in a given string using regular expressions.</p> Example <pre><code>from dsg_lib.common_functions import patterns\n\ntext = \"Hello, my name is 'John Doe' and I live in 'New York'.\" left_char =\n\"'\" right_char = \"'\"\n\nresults = patterns.pattern_between_two_char(text, left_char, right_char)\n\nprint(results) ``` This will output: ```python {\n    'found': ['John Doe', 'New York'], 'matched_found': 2,\n    'pattern_parameters': {\n        'left_character': \"'\", 'right_character': \"'\", 'regex_pattern':\n        \"'(.+?)'\", 'text_string': \"Hello, my name is 'John Doe' and I live\n        in 'New York'.\"\n    }\n}\n</code></pre>"},{"location":"common_functions/regex/#dsg_lib.common_functions.patterns.pattern_between_two_char","title":"<code>pattern_between_two_char(text_string, left_characters, right_characters)</code>","text":"<p>Searches for all patterns between two characters (left and right) in a given string using regular expressions.</p> <p>This function takes a string and two characters as input, and returns a dictionary containing all patterns found between the two characters in the string. The dictionary also includes the number of matches found and the regex pattern used for searching.</p> <p>The function uses Python's built-in <code>re</code> module for regex searching and the <code>loguru</code> module for logging.</p> <p>Parameters:</p> Name Type Description Default <code>text_string</code> <code>str</code> <p>The string in which to search for patterns.</p> required <code>left_characters</code> <code>str</code> <p>The character(s) that appear(s) immediately to</p> required <code>the</code> <code>left of the desired pattern. right_characters (str</code> <p>The</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the following keys: - \"found\": a list of strings containing all patterns found. - \"matched_found\": the number of patterns found. - \"pattern_parameters\": a dictionary with the following keys:     - \"left_character\": the escaped left character string used to       build the regex pattern.     - \"right_character\": the escaped right character string used to       build the regex pattern.     - \"regex_pattern\": the final regex pattern used for searching.     - \"text_string\": the escaped input string used for searching.</p> Example <pre><code>from dsg_lib.common_functions import patterns\n\ntext = \"Hello, my name is 'John Doe' and I live in 'New York'.\"\nleft_char = \"'\" right_char = \"'\"\n\nresults = patterns.pattern_between_two_char(text, left_char, right_char)\n\nprint(results) ``` This will output: ```python {\n    'found': ['John Doe', 'New York'], 'matched_found': 2,\n    'pattern_parameters': {\n        'left_character': \"'\", 'right_character': \"'\", 'regex_pattern':\n        \"'(.+?)'\", 'text_string': \"Hello, my name is 'John Doe' and I\n        live in 'New York'.\"\n    }\n}\n</code></pre> Source code in <code>dsg_lib/common_functions/patterns.py</code> <pre><code>def pattern_between_two_char(text_string: str, left_characters: str, right_characters: str) -&gt; dict:\n    \"\"\"\n    Searches for all patterns between two characters (left and right) in a given\n    string using regular expressions.\n\n    This function takes a string and two characters as input, and returns a\n    dictionary containing all patterns found between the two characters in the\n    string. The dictionary also includes the number of matches found and the\n    regex pattern used for searching.\n\n    The function uses Python's built-in `re` module for regex searching and the\n    `loguru` module for logging.\n\n    Args:\n        text_string (str): The string in which to search for patterns.\n        left_characters (str): The character(s) that appear(s) immediately to\n        the left of the desired pattern. right_characters (str): The\n        character(s) that appear(s) immediately to the right of the desired\n        pattern.\n\n    Returns:\n        dict: A dictionary with the following keys:\n            - \"found\": a list of strings containing all patterns found.\n            - \"matched_found\": the number of patterns found.\n            - \"pattern_parameters\": a dictionary with the following keys:\n                - \"left_character\": the escaped left character string used to\n                  build the regex pattern.\n                - \"right_character\": the escaped right character string used to\n                  build the regex pattern.\n                - \"regex_pattern\": the final regex pattern used for searching.\n                - \"text_string\": the escaped input string used for searching.\n\n    Example:\n        ```python\n        from dsg_lib.common_functions import patterns\n\n        text = \"Hello, my name is 'John Doe' and I live in 'New York'.\"\n        left_char = \"'\" right_char = \"'\"\n\n        results = patterns.pattern_between_two_char(text, left_char, right_char)\n\n        print(results) ``` This will output: ```python {\n            'found': ['John Doe', 'New York'], 'matched_found': 2,\n            'pattern_parameters': {\n                'left_character': \"'\", 'right_character': \"'\", 'regex_pattern':\n                \"'(.+?)'\", 'text_string': \"Hello, my name is 'John Doe' and I\n                live in 'New York'.\"\n            }\n        }\n        ```\n    \"\"\"\n\n    if not left_characters or not right_characters:\n        raise ValueError(\n            f\"Left '{left_characters}' and/or Right '{right_characters}' characters must not be None or empty\"\n        )\n\n    try:\n        # Escape input strings to safely use them in regex pattern\n        esc_text = re.escape(text_string)\n        esc_left_char = re.escape(left_characters)\n        esc_right_char = re.escape(right_characters)\n\n        # Create a regex pattern that matches all substrings between target\n        # characters\n        pattern = f'{esc_left_char}(.+?){esc_right_char}'\n\n        # Replace \\w with . to match any printable UTF-8 character\n        pattern = pattern.replace(r'\\w', r'.')\n\n        # Search for all patterns and store them in pattern_list variable\n        pattern_list = re.findall(pattern, esc_text)\n\n        # Create a dictionary to store match details\n        results: dict = {\n            'found': pattern_list,\n            'matched_found': len(pattern_list),\n            'pattern_parameters': {\n                'left_character': esc_left_char,\n                'right_character': esc_right_char,\n                'regex_pattern': pattern,\n                'text_string': esc_text,\n            },\n        }\n\n        # Log matched pattern(s) found using 'debug' log level\n        if len(pattern_list) &gt; 0:\n            logger.debug(f'Matched pattern(s): {pattern_list}')\n\n        # Log successful function execution using 'info' log level\n        logger.info(\"Successfully executed 'pattern_between_two_char' function\")\n        return results\n\n    except ValueError as e:  # pragma: no cover\n        # capture exception and return error in case of invalid input parameters\n        results: dict = {\n            'error': str(e),\n            'matched_found': 0,\n            'pattern_parameters': {\n                'left_character': left_characters,\n                'right_character': right_characters,\n                'regex_pattern': None,\n                'text_string': text_string,\n            },\n        }\n        # logger of regex error using 'critical' log level\n        logger.critical(f'Failed to generate regex pattern with error: {e}')\n        return results\n</code></pre>"},{"location":"database/async_database_setup/","title":"Reference","text":""},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database","title":"<code>dsg_lib.async_database_functions.async_database</code>","text":"<p>async_database.py.</p> <p>This module provides classes for managing asynchronous database operations using SQLAlchemy and asyncio.</p> <p>Classes:</p> Name Description <code>- DBConfig</code> <p>Manages the database configuration.</p> <code>- AsyncDatabase</code> <p>Manages the asynchronous database operations.</p> <p>The DBConfig class initializes the database configuration and creates a SQLAlchemy engine and a MetaData instance.</p> <p>The AsyncDatabase class uses an instance of DBConfig to perform asynchronous database operations. It provides methods to get a database session and to create tables in the database.</p> <p>This module uses the logger from the dsg_lib.common_functions for logging.</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n)\n\n# Create a DBConfig instance\nconfig = {\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    \"pool_recycle\": 3600,\n}\n\n# create database configuration\ndb_config = database_config.DBConfig(config)\n\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n</code></pre></p>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase","title":"<code>AsyncDatabase</code>","text":"<p>A class used to manage the asynchronous database operations.</p>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase--attributes","title":"Attributes","text":"<p>db_config : DBConfig     an instance of DBConfig class containing the database configuration Base : Base     the declarative base model for SQLAlchemy</p>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase--methods","title":"Methods","text":"<p>get_db_session():     Returns a context manager that provides a new database session. create_tables():     Asynchronously creates all tables in the database.</p> Source code in <code>dsg_lib/async_database_functions/async_database.py</code> <pre><code>class AsyncDatabase:\n    \"\"\"\n    A class used to manage the asynchronous database operations.\n\n    Attributes\n    ----------\n    db_config : DBConfig\n        an instance of DBConfig class containing the database configuration\n    Base : Base\n        the declarative base model for SQLAlchemy\n\n    Methods\n    -------\n    get_db_session():\n        Returns a context manager that provides a new database session.\n    create_tables():\n        Asynchronously creates all tables in the database.\n    \"\"\"\n\n    def __init__(self, db_config: DBConfig):\n        \"\"\"Initialize the AsyncDatabase class with an instance of DBConfig.\n\n        Parameters:\n        db_config (DBConfig): An instance of DBConfig class containing the\n        database configuration.\n\n        Returns: None\n        \"\"\"\n        self.db_config = db_config\n        self.Base = BASE\n        logger.debug('AsyncDatabase initialized')\n\n    def get_db_session(self):\n        \"\"\"This method returns a context manager that provides a new database\n        session.\n\n        Parameters: None\n\n        Returns: contextlib._GeneratorContextManager: A context manager that\n        provides a new database session.\n        \"\"\"\n        logger.debug('Getting database session')\n        return self.db_config.get_db_session()\n\n    async def create_tables(self):\n        \"\"\"This method asynchronously creates all tables in the database.\n\n        Parameters: None\n\n        Returns: None\n        \"\"\"\n        logger.debug('Creating tables')\n        try:\n            # Bind the engine to the metadata of the base class\n            self.Base.metadata.bind = self.db_config.engine\n\n            # Begin a new transaction\n            async with self.db_config.engine.begin() as conn:\n                # Run a function in a synchronous manner\n                await conn.run_sync(self.Base.metadata.create_all)\n            logger.info('Tables created successfully')\n        except Exception as ex:  # pragma: no cover\n            # Log the error and raise it\n            logger.error(f'Error creating tables: {ex}')  # pragma: no cover\n            raise  # pragma: no cover\n</code></pre>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase.__init__","title":"<code>__init__(db_config)</code>","text":"<p>Initialize the AsyncDatabase class with an instance of DBConfig.</p> <p>Parameters: db_config (DBConfig): An instance of DBConfig class containing the database configuration.</p> <p>Returns: None</p> Source code in <code>dsg_lib/async_database_functions/async_database.py</code> <pre><code>def __init__(self, db_config: DBConfig):\n    \"\"\"Initialize the AsyncDatabase class with an instance of DBConfig.\n\n    Parameters:\n    db_config (DBConfig): An instance of DBConfig class containing the\n    database configuration.\n\n    Returns: None\n    \"\"\"\n    self.db_config = db_config\n    self.Base = BASE\n    logger.debug('AsyncDatabase initialized')\n</code></pre>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase.create_tables","title":"<code>create_tables()</code>  <code>async</code>","text":"<p>This method asynchronously creates all tables in the database.</p> <p>Parameters: None</p> <p>Returns: None</p> Source code in <code>dsg_lib/async_database_functions/async_database.py</code> <pre><code>async def create_tables(self):\n    \"\"\"This method asynchronously creates all tables in the database.\n\n    Parameters: None\n\n    Returns: None\n    \"\"\"\n    logger.debug('Creating tables')\n    try:\n        # Bind the engine to the metadata of the base class\n        self.Base.metadata.bind = self.db_config.engine\n\n        # Begin a new transaction\n        async with self.db_config.engine.begin() as conn:\n            # Run a function in a synchronous manner\n            await conn.run_sync(self.Base.metadata.create_all)\n        logger.info('Tables created successfully')\n    except Exception as ex:  # pragma: no cover\n        # Log the error and raise it\n        logger.error(f'Error creating tables: {ex}')  # pragma: no cover\n        raise  # pragma: no cover\n</code></pre>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase.get_db_session","title":"<code>get_db_session()</code>","text":"<p>This method returns a context manager that provides a new database session.</p> <p>Parameters: None</p> <p>Returns: contextlib._GeneratorContextManager: A context manager that provides a new database session.</p> Source code in <code>dsg_lib/async_database_functions/async_database.py</code> <pre><code>def get_db_session(self):\n    \"\"\"This method returns a context manager that provides a new database\n    session.\n\n    Parameters: None\n\n    Returns: contextlib._GeneratorContextManager: A context manager that\n    provides a new database session.\n    \"\"\"\n    logger.debug('Getting database session')\n    return self.db_config.get_db_session()\n</code></pre>"},{"location":"database/base_schema/","title":"Reference","text":""},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema","title":"<code>dsg_lib.async_database_functions.base_schema</code>","text":"<p>This module defines the base schema for database models in the application.</p> <p>The module uses SQLAlchemy as the ORM and provides a <code>SchemaBase</code> class that all other models should inherit from. The <code>SchemaBase</code> class includes common columns that are needed for most models like <code>pkid</code>, <code>date_created</code>, and <code>date_updated</code>.</p> <ul> <li><code>pkid</code>: A unique identifier for each record. It's a string representation of a   UUID.</li> <li><code>date_created</code>: The date and time when a particular row was inserted into the   table.     It defaults to the current UTC time when the instance is created.</li> <li><code>date_updated</code>: The date and time when a particular row was last updated.     It defaults to the current UTC time whenever the instance is updated.</li> </ul> <p>To create a new database model, import this module and extend the <code>SchemaBase</code> class.</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\n\nclass MyModel(base_schema.SchemaBaseSQLite):\n        # Define your model-specific columns here my_column =\n        base_schema.Column(base_schema.String(50))\n</code></pre></p>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseCockroachDB","title":"<code>SchemaBaseCockroachDB</code>","text":"<p>This class provides a base schema that includes common columns for most models when using a CockroachDB database. CockroachDB uses the same syntax as PostgreSQL. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBaseCockroachDB, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseCockroachDB:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using a CockroachDB database. CockroachDB uses the same syntax\n    as PostgreSQL. All other models should inherit from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBaseCockroachDB, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"(CURRENT_TIMESTAMP AT TIME ZONE 'UTC')\"),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"(CURRENT_TIMESTAMP AT TIME ZONE 'UTC')\"),\n        onupdate=text(\"(CURRENT_TIMESTAMP AT TIME ZONE 'UTC')\"),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseFirebird","title":"<code>SchemaBaseFirebird</code>","text":"<p>This class provides a base schema that includes common columns for most models when using a Firebird database. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBaseFirebird, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseFirebird:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using a Firebird database. All other models should inherit\n    from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBaseFirebird, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text('CURRENT_TIMESTAMP'),\n        comment='Date and time when a row was inserted, defaults to current time',\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text('CURRENT_TIMESTAMP'),\n        onupdate=text('CURRENT_TIMESTAMP'),\n        comment='Date and time when a row was last updated, defaults to current time on update',\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseMSSQL","title":"<code>SchemaBaseMSSQL</code>","text":"<p>This class provides a base schema that includes common columns for most models when using a Microsoft SQL Server database. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBaseMSSQL, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseMSSQL:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using a Microsoft SQL Server database. All other models should\n    inherit from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBaseMSSQL, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text('GETUTCDATE()'),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text('GETUTCDATE()'),\n        onupdate=text('GETUTCDATE()'),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseMySQL","title":"<code>SchemaBaseMySQL</code>","text":"<p>This class provides a base schema that includes common columns for most models when using a MySQL database. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBaseMySQL, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseMySQL:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using a MySQL database. All other models should inherit\n    from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBaseMySQL, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text('UTC_TIMESTAMP()'),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text('UTC_TIMESTAMP()'),\n        onupdate=text('UTC_TIMESTAMP()'),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseOracle","title":"<code>SchemaBaseOracle</code>","text":"<p>This class provides a base schema that includes common columns for most models when using an Oracle database. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBaseOracle, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseOracle:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using an Oracle database. All other models should inherit\n    from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBaseOracle, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text('SYS_EXTRACT_UTC(SYSTIMESTAMP)'),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text('SYS_EXTRACT_UTC(SYSTIMESTAMP)'),\n        onupdate=text('SYS_EXTRACT_UTC(SYSTIMESTAMP)'),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBasePostgres","title":"<code>SchemaBasePostgres</code>","text":"<p>This class provides a base schema that includes common columns for most models when using a PostgreSQL database. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBasePostgres, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBasePostgres:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using a PostgreSQL database. All other models should inherit\n    from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBasePostgres, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"(CURRENT_TIMESTAMP AT TIME ZONE 'UTC')\"),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"(CURRENT_TIMESTAMP AT TIME ZONE 'UTC')\"),\n        onupdate=text(\"(CURRENT_TIMESTAMP AT TIME ZONE 'UTC')\"),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseSQLite","title":"<code>SchemaBaseSQLite</code>","text":"<p>This class provides a base schema that includes common columns for most models. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBase, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseSQLite:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models. All other models should inherit from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBase, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        default=datetime.datetime.now(datetime.timezone.utc),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        default=datetime.datetime.now(datetime.timezone.utc),\n        onupdate=datetime.datetime.now(datetime.timezone.utc),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseSybase","title":"<code>SchemaBaseSybase</code>","text":"<p>This class provides a base schema that includes common columns for most models when using a Sybase database. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBaseSybase, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseSybase:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using a Sybase database. All other models should inherit\n    from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBaseSybase, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text('GETUTCDATE()'),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text('GETUTCDATE()'),\n        onupdate=text('GETUTCDATE()'),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/database_configuration/","title":"Reference","text":""},{"location":"database/database_configuration/#dsg_lib.async_database_functions.database_config","title":"<code>dsg_lib.async_database_functions.database_config</code>","text":"<p>This module provides classes and functions for managing asynchronous database operations using SQLAlchemy and asyncio.</p> <p>The main classes are DBConfig, which manages the database configuration and creates a SQLAlchemy engine and a MetaData instance, and AsyncDatabase, which uses an instance of DBConfig to perform asynchronous database operations.</p> <p>The module also provides a function, import_sqlalchemy, which tries to import SQLAlchemy and its components, and raises an ImportError if SQLAlchemy is not installed or if the installed version is not compatible.</p> <p>The module uses the logger from the <code>dsg_lib</code> for logging, and the <code>time</code> module for working with times. It also uses the <code>contextlib</code> module for creating context managers, and the <code>typing</code> module for type hinting.</p> <p>The <code>BASE</code> variable is a base class for declarative database models. It is created using the <code>declarative_base</code> function from <code>sqlalchemy.orm</code>.</p> <p>This module is part of the <code>dsg_lib</code> package, which provides utilities for working with databases in Python.</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import database_config\n\n# Define your database configuration config = {\n    \"database_uri\": \"postgresql+asyncpg://user:password@localhost/dbname\",\n    \"echo\": True, \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n    \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n}\n\n# Create a DBConfig instance db_config = database_config.DBConfig(config)\n\n# Use the DBConfig instance to get a database session async with\ndb_config.get_db_session() as session:\n    # Perform your database operations here pass\n</code></pre></p>"},{"location":"database/database_configuration/#dsg_lib.async_database_functions.database_config.DBConfig","title":"<code>DBConfig</code>","text":"<p>A class used to manage the database configuration and create a SQLAlchemy engine.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>dict</code> <p>A dictionary containing the database configuration</p> <code>parameters.</code> <code>engine (Engine</code> <p>The SQLAlchemy engine created with the</p> <code>database</code> <code>URI from the config. metadata (MetaData</code> <p>The SQLAlchemy</p> <p>Create Engine Support Functions by Database Type Confirmed by testing [SQLITE, PostrgeSQL] To Be Tested [MySQL, Oracle, MSSQL] and should be considered experimental ------- Option          SQLite  PostgreSQL  MySQL Oracle  MSSQL echo                Yes         Yes         Yes     Yes Yes future              Yes         Yes         Yes     Yes     Yes pool_pre_ping       Yes         Yes         Yes     Yes     Yes pool_size No          Yes         Yes     Yes     Yes max_overflow        No Yes         Yes     Yes     Yes pool_recycle        Yes         Yes Yes     Yes     Yes pool_timeout        No          Yes         Yes     Yes Yes</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import database_config\n\n# Define your database configuration config = {\n    \"database_uri\": \"postgresql+asyncpg://user:password@localhost/dbname\",\n    \"echo\": True, \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n    \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n}\n\n# Create a DBConfig instance db_config = database_config.DBConfig(config)\n\n# Use the DBConfig instance to get a database session async with\ndb_config.get_db_session() as session:\n    # Perform your database operations here pass\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_config.py</code> <pre><code>class DBConfig:\n    \"\"\"\n    A class used to manage the database configuration and create a SQLAlchemy\n    engine.\n\n    Attributes:\n        config (dict): A dictionary containing the database configuration\n        parameters. engine (Engine): The SQLAlchemy engine created with the\n        database URI from the config. metadata (MetaData): The SQLAlchemy\n        MetaData instance.\n\n\n    Create Engine Support Functions by Database Type Confirmed by testing\n    [SQLITE, PostrgeSQL] To Be Tested [MySQL, Oracle, MSSQL] and should be\n    considered experimental ------- Option          SQLite  PostgreSQL  MySQL\n    Oracle  MSSQL echo                Yes         Yes         Yes     Yes\n    Yes future              Yes         Yes         Yes     Yes     Yes\n    pool_pre_ping       Yes         Yes         Yes     Yes     Yes pool_size\n    No          Yes         Yes     Yes     Yes max_overflow        No\n    Yes         Yes     Yes     Yes pool_recycle        Yes         Yes\n    Yes     Yes     Yes pool_timeout        No          Yes         Yes     Yes\n    Yes\n\n    Example:\n    ```python\n\n    from dsg_lib.async_database_functions import database_config\n\n    # Define your database configuration config = {\n        \"database_uri\": \"postgresql+asyncpg://user:password@localhost/dbname\",\n        \"echo\": True, \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n        \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n    }\n\n    # Create a DBConfig instance db_config = database_config.DBConfig(config)\n\n    # Use the DBConfig instance to get a database session async with\n    db_config.get_db_session() as session:\n        # Perform your database operations here pass\n    ```\n\n    \"\"\"\n\n    SUPPORTED_PARAMETERS = {\n        'sqlite': {'echo', 'future', 'pool_recycle'},\n        'postgresql': {\n            'echo',\n            'future',\n            'pool_pre_ping',\n            'pool_size',\n            'max_overflow',\n            'pool_recycle',\n            'pool_timeout',\n        },\n        # Add other engines here...\n    }\n\n    def __init__(self, config: Dict):\n        \"\"\"\n        Initializes the DBConfig instance with the given database configuration.\n\n        The configuration should be a dictionary with the following keys: -\n        \"database_uri\": The URI for the database. - \"echo\": If True, the engine\n        will log all statements as well as a `repr()` of their parameter lists\n        to the engines logger, which defaults to sys.stdout. - \"future\": If\n        True, use the future version of SQLAlchemy, which supports asyncio. -\n        \"pool_pre_ping\": If True, the pool will test the connection for liveness\n        upon each checkout. - \"pool_size\": The size of the connection pool to be\n        maintained. - \"max_overflow\": The number of connections that can be\n        opened above the `pool_size` setting, when all other connections are in\n        use. - \"pool_recycle\": The number of seconds after which a connection is\n        automatically recycled. This is required for MySQL, which removes\n        connections after 8 hours idle by default. - \"pool_timeout\": The number\n        of seconds to wait before giving up on getting a connection from the\n        pool.\n\n        Args:\n            config (Dict): A dictionary containing the database configuration\n            parameters.\n\n        Raises:\n            Exception: If there are unsupported parameters for the database\n            engine type.\n\n        Example:\n        ```python\n\n        from dsg_lib.async_database_functions import database_config\n\n        # Define your database configuration config = {\n            \"database_uri\":\n            \"postgresql+asyncpg://user:password@localhost/dbname\", \"echo\": True,\n            \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n            \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n        }\n\n        # Create a DBConfig instance db_config =\n        database_config.DBConfig(config)\n        ```\n        \"\"\"\n        self.config = config\n        engine_type = self.config['database_uri'].split('+')[0]\n        supported_parameters = self.SUPPORTED_PARAMETERS.get(engine_type, set())\n        unsupported_parameters = set(config.keys()) - supported_parameters - {'database_uri'}\n        if unsupported_parameters:\n            error_message = f'Unsupported parameters for {engine_type}: {unsupported_parameters}'\n            logger.error(error_message)\n            raise Exception(error_message)\n\n        engine_parameters = {\n            param: self.config.get(param)\n            for param in supported_parameters\n            if self.config.get(param) is not None\n        }\n        self.engine = create_async_engine(self.config['database_uri'], **engine_parameters)\n        self.metadata = MetaData()\n\n    @asynccontextmanager\n    async def get_db_session(self):\n        \"\"\"\n        This method returns a context manager that provides a new database\n        session.\n\n        The session is created using the SQLAlchemy engine from the DBConfig\n        instance, and it does not expire on commit. The session is of type\n        AsyncSession.\n\n        This method should be used with the `async with` statement.\n\n        Yields:\n            AsyncSession: A new SQLAlchemy asynchronous session.\n\n        Raises:\n            SQLAlchemyError: If a database error occurs.\n\n        Example:\n        ```python\n\n        from dsg_lib.async_database_functions import database_config\n\n        # Define your database configuration config = {\n            \"database_uri\":\n            \"postgresql+asyncpg://user:password@localhost/dbname\", \"echo\": True,\n            \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n            \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n        }\n\n        # Create a DBConfig instance db_config =\n        database_config.DBConfig(config)\n\n        # Use the DBConfig instance to get a database session async with\n        db_config.get_db_session() as session:\n            # Perform your database operations here pass\n        ```\n        \"\"\"\n        logger.debug('Creating new database session')\n        try:\n            # Create a new database session\n            async with sessionmaker(\n                self.engine, expire_on_commit=False, class_=AsyncSession\n            )() as session:\n                # Yield the session to the context manager\n                yield session\n        except SQLAlchemyError as e:  # pragma: no cover\n            # Log the error and raise it\n            logger.error(f'Database error occurred: {str(e)}')  # pragma: no cover\n            raise  # pragma: no cover\n        finally:  # pragma: no cover\n            # Log the end of the database session\n            logger.debug('Database session ended')  # pragma: no cover\n</code></pre>"},{"location":"database/database_configuration/#dsg_lib.async_database_functions.database_config.DBConfig.__init__","title":"<code>__init__(config)</code>","text":"<p>Initializes the DBConfig instance with the given database configuration.</p> <p>The configuration should be a dictionary with the following keys: - \"database_uri\": The URI for the database. - \"echo\": If True, the engine will log all statements as well as a <code>repr()</code> of their parameter lists to the engines logger, which defaults to sys.stdout. - \"future\": If True, use the future version of SQLAlchemy, which supports asyncio. - \"pool_pre_ping\": If True, the pool will test the connection for liveness upon each checkout. - \"pool_size\": The size of the connection pool to be maintained. - \"max_overflow\": The number of connections that can be opened above the <code>pool_size</code> setting, when all other connections are in use. - \"pool_recycle\": The number of seconds after which a connection is automatically recycled. This is required for MySQL, which removes connections after 8 hours idle by default. - \"pool_timeout\": The number of seconds to wait before giving up on getting a connection from the pool.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict</code> <p>A dictionary containing the database configuration</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If there are unsupported parameters for the database</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import database_config\n\n# Define your database configuration config = {\n    \"database_uri\":\n    \"postgresql+asyncpg://user:password@localhost/dbname\", \"echo\": True,\n    \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n    \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n}\n\n# Create a DBConfig instance db_config =\ndatabase_config.DBConfig(config)\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_config.py</code> <pre><code>def __init__(self, config: Dict):\n    \"\"\"\n    Initializes the DBConfig instance with the given database configuration.\n\n    The configuration should be a dictionary with the following keys: -\n    \"database_uri\": The URI for the database. - \"echo\": If True, the engine\n    will log all statements as well as a `repr()` of their parameter lists\n    to the engines logger, which defaults to sys.stdout. - \"future\": If\n    True, use the future version of SQLAlchemy, which supports asyncio. -\n    \"pool_pre_ping\": If True, the pool will test the connection for liveness\n    upon each checkout. - \"pool_size\": The size of the connection pool to be\n    maintained. - \"max_overflow\": The number of connections that can be\n    opened above the `pool_size` setting, when all other connections are in\n    use. - \"pool_recycle\": The number of seconds after which a connection is\n    automatically recycled. This is required for MySQL, which removes\n    connections after 8 hours idle by default. - \"pool_timeout\": The number\n    of seconds to wait before giving up on getting a connection from the\n    pool.\n\n    Args:\n        config (Dict): A dictionary containing the database configuration\n        parameters.\n\n    Raises:\n        Exception: If there are unsupported parameters for the database\n        engine type.\n\n    Example:\n    ```python\n\n    from dsg_lib.async_database_functions import database_config\n\n    # Define your database configuration config = {\n        \"database_uri\":\n        \"postgresql+asyncpg://user:password@localhost/dbname\", \"echo\": True,\n        \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n        \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n    }\n\n    # Create a DBConfig instance db_config =\n    database_config.DBConfig(config)\n    ```\n    \"\"\"\n    self.config = config\n    engine_type = self.config['database_uri'].split('+')[0]\n    supported_parameters = self.SUPPORTED_PARAMETERS.get(engine_type, set())\n    unsupported_parameters = set(config.keys()) - supported_parameters - {'database_uri'}\n    if unsupported_parameters:\n        error_message = f'Unsupported parameters for {engine_type}: {unsupported_parameters}'\n        logger.error(error_message)\n        raise Exception(error_message)\n\n    engine_parameters = {\n        param: self.config.get(param)\n        for param in supported_parameters\n        if self.config.get(param) is not None\n    }\n    self.engine = create_async_engine(self.config['database_uri'], **engine_parameters)\n    self.metadata = MetaData()\n</code></pre>"},{"location":"database/database_configuration/#dsg_lib.async_database_functions.database_config.DBConfig.get_db_session","title":"<code>get_db_session()</code>  <code>async</code>","text":"<p>This method returns a context manager that provides a new database session.</p> <p>The session is created using the SQLAlchemy engine from the DBConfig instance, and it does not expire on commit. The session is of type AsyncSession.</p> <p>This method should be used with the <code>async with</code> statement.</p> <p>Yields:</p> Name Type Description <code>AsyncSession</code> <p>A new SQLAlchemy asynchronous session.</p> <p>Raises:</p> Type Description <code>SQLAlchemyError</code> <p>If a database error occurs.</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import database_config\n\n# Define your database configuration config = {\n    \"database_uri\":\n    \"postgresql+asyncpg://user:password@localhost/dbname\", \"echo\": True,\n    \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n    \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n}\n\n# Create a DBConfig instance db_config =\ndatabase_config.DBConfig(config)\n\n# Use the DBConfig instance to get a database session async with\ndb_config.get_db_session() as session:\n    # Perform your database operations here pass\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_config.py</code> <pre><code>@asynccontextmanager\nasync def get_db_session(self):\n    \"\"\"\n    This method returns a context manager that provides a new database\n    session.\n\n    The session is created using the SQLAlchemy engine from the DBConfig\n    instance, and it does not expire on commit. The session is of type\n    AsyncSession.\n\n    This method should be used with the `async with` statement.\n\n    Yields:\n        AsyncSession: A new SQLAlchemy asynchronous session.\n\n    Raises:\n        SQLAlchemyError: If a database error occurs.\n\n    Example:\n    ```python\n\n    from dsg_lib.async_database_functions import database_config\n\n    # Define your database configuration config = {\n        \"database_uri\":\n        \"postgresql+asyncpg://user:password@localhost/dbname\", \"echo\": True,\n        \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n        \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n    }\n\n    # Create a DBConfig instance db_config =\n    database_config.DBConfig(config)\n\n    # Use the DBConfig instance to get a database session async with\n    db_config.get_db_session() as session:\n        # Perform your database operations here pass\n    ```\n    \"\"\"\n    logger.debug('Creating new database session')\n    try:\n        # Create a new database session\n        async with sessionmaker(\n            self.engine, expire_on_commit=False, class_=AsyncSession\n        )() as session:\n            # Yield the session to the context manager\n            yield session\n    except SQLAlchemyError as e:  # pragma: no cover\n        # Log the error and raise it\n        logger.error(f'Database error occurred: {str(e)}')  # pragma: no cover\n        raise  # pragma: no cover\n    finally:  # pragma: no cover\n        # Log the end of the database session\n        logger.debug('Database session ended')  # pragma: no cover\n</code></pre>"},{"location":"database/database_operations/","title":"Reference","text":""},{"location":"database/database_operations/#configuration-matrix","title":"Configuration Matrix","text":"<p>Create Engine Support Functions by Database Type Confirmed by testing [SQLITE, PostgreSQL] To Be Tested [MySQL, Oracle, MSSQL] and should be considered experimental.</p> Option SQLite PostgreSQL MySQL Oracle MSSQL echo Yes Yes Yes Yes Yes future Yes Yes Yes Yes Yes pool_pre_ping Yes Yes Yes Yes Yes pool_size No Yes Yes Yes Yes max_overflow No Yes Yes Yes Yes pool_recycle Yes Yes Yes Yes Yes pool_timeout No Yes Yes Yes Yes"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations","title":"<code>dsg_lib.async_database_functions.database_operations</code>","text":"<p>This module contains tests for the DatabaseOperations class in the dsg_lib module.</p> <p>The DatabaseOperations class provides methods for performing CRUD operations on a database using SQLAlchemy's asynchronous session.</p> <p>The methods include:</p> <ul> <li><code>create_one</code>: Creates a single record in the database.</li> <li><code>create_many</code>: Creates multiple records in the database.</li> <li><code>read_one</code>: Reads a single record from the database.</li> <li><code>read_many</code>: Reads multiple records from the database.</li> <li><code>update_one</code>: Updates a single record in the database.</li> <li><code>update_many</code>: Updates multiple records in the database.</li> <li><code>delete_one</code>: Deletes a single record from the database.</li> <li><code>delete_many</code>: Deletes multiple records from the database.</li> <li><code>count_query</code>: Counts the number of records that match a given query.</li> </ul> <p>Each method is tested to ensure it performs the expected operation and handles errors correctly. The tests use the pytest-asyncio plugin to run the asynchronous methods in an event loop, and the unittest.mock library to mock the database session and simulate errors.</p> <p>The tests are organized into a single class, TestDatabaseOperations, which contains one test method for each method in the DatabaseOperations class. Each test method follows the Arrange-Act-Assert pattern: it sets up the necessary objects and state (Arrange), calls the method being tested (Act), and checks that the results are as expected (Assert).</p>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations","title":"<code>DatabaseOperations</code>","text":"<p>This class provides methods for performing CRUD operations on a database using SQLAlchemy's asynchronous session.</p> <p>The methods include:</p> <ul> <li><code>create_one</code>: Creates a single record in the database.</li> <li><code>create_many</code>: Creates multiple records in the database.</li> <li><code>read_one</code>: Reads a single record from the database.</li> <li><code>read_many</code>: Reads multiple records from the database.</li> <li><code>update_one</code>: Updates a single record in the database.</li> <li><code>update_many</code>: Updates multiple records in the database.</li> <li><code>delete_one</code>: Deletes a single record from the database.</li> <li><code>delete_many</code>: Deletes multiple records from the database.</li> <li><code>count_query</code>: Counts the number of records that match a given query.</li> <li><code>get_column_details</code>: Gets the details of the columns in a table.</li> <li><code>get_primary_keys</code>: Gets the primary keys of a table.</li> <li><code>get_table_names</code>: Gets the names of all tables in the database.</li> </ul> <p>Examples: <pre><code>data = await db_ops.create_one(User(name='John Doe'))\ndata = await db_ops.create_many([User(name='John Doe'), User(name='Jane Doe')])\ndata = await db_ops.read_one(User, 1)\ndata = await db_ops.read_many(User, [1, 2, 3])\ndata = await db_ops.update_one(User, 1, {'name': 'John Smith'})\ndata = await db_ops.update_many(User, [1, 2], [{'name': 'John Smith'}, {'name': 'Jane Smith'}])\ndata = await db_ops.delete_one(User, 1)\ndata = await db_ops.delete_many(User, [1, 2, 3])\ndata = await db_ops.count_query(select(User))\ndata = await db_ops.get_column_details(User)\ndata = await db_ops.get_primary_keys(User)\ndata = await db_ops.get_table_names()\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>class DatabaseOperations:\n    \"\"\"\n    This class provides methods for performing CRUD operations on a database using SQLAlchemy's asynchronous session.\n\n    The methods include:\n\n    - `create_one`: Creates a single record in the database.\n    - `create_many`: Creates multiple records in the database.\n    - `read_one`: Reads a single record from the database.\n    - `read_many`: Reads multiple records from the database.\n    - `update_one`: Updates a single record in the database.\n    - `update_many`: Updates multiple records in the database.\n    - `delete_one`: Deletes a single record from the database.\n    - `delete_many`: Deletes multiple records from the database.\n    - `count_query`: Counts the number of records that match a given query.\n    - `get_column_details`: Gets the details of the columns in a table.\n    - `get_primary_keys`: Gets the primary keys of a table.\n    - `get_table_names`: Gets the names of all tables in the database.\n\n    Examples:\n    ```python\n\n    data = await db_ops.create_one(User(name='John Doe'))\n    data = await db_ops.create_many([User(name='John Doe'), User(name='Jane Doe')])\n    data = await db_ops.read_one(User, 1)\n    data = await db_ops.read_many(User, [1, 2, 3])\n    data = await db_ops.update_one(User, 1, {'name': 'John Smith'})\n    data = await db_ops.update_many(User, [1, 2], [{'name': 'John Smith'}, {'name': 'Jane Smith'}])\n    data = await db_ops.delete_one(User, 1)\n    data = await db_ops.delete_many(User, [1, 2, 3])\n    data = await db_ops.count_query(select(User))\n    data = await db_ops.get_column_details(User)\n    data = await db_ops.get_primary_keys(User)\n    data = await db_ops.get_table_names()\n    ```\n    \"\"\"\n\n    def __init__(self, async_db: AsyncDatabase):\n        \"\"\"\n        Initializes a new instance of the DatabaseOperations class.\n\n        Args:\n            async_db (module_name.AsyncDatabase): An instance of the\n            AsyncDatabase class for performing asynchronous database operations.\n\n        Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n\n        db_config = database_config.DBConfig(config)\n\n        async_db = async_database.AsyncDatabase(db_config)\n\n        db_ops = database_operations.DatabaseOperations(async_db)\n\n        ```\n        \"\"\"\n        # Log the start of the initialization\n        logger.debug('Initializing DatabaseOperations instance')\n\n        # Store the AsyncDatabase instance in the async_db attribute This\n        # instance will be used for performing asynchronous database operations\n        self.async_db = async_db\n\n        # Log the successful initialization\n        logger.info('DatabaseOperations instance initialized successfully')\n\n    async def get_columns_details(self, table):\n        \"\"\"\n        Retrieves the details of the columns of a given table.\n\n        This asynchronous method accepts a table object and returns a\n        dictionary. Each key in the dictionary is a column name from the table,\n        and the corresponding value is another dictionary containing details\n        about that column, such as type, if it's nullable, if it's a primary\n        key, if it's unique, its autoincrement status, and its default value.\n\n        Args:\n            table (Table): An instance of the SQLAlchemy Table class\n            representing the database table for which column details are\n            required.\n\n        Returns:\n            dict: A dictionary where each key is a column name, and each value\n            is a dictionary with the column's details.\n\n        Raises:\n            Exception: If any error occurs during the database operation.\n\n        Example:\n        ```python\n        from sqlalchemy import Table, MetaData, Column,\n        Integer, String from dsg_lib.async_database_functions import module_name metadata = MetaData()\n        my_table = Table('my_table', metadata,\n                        Column('id', Integer, primary_key=True), Column('name',\n                        String))\n\n        from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # get columns details\n        columns = await db_ops.get_columns_details(my_table)\n        ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug(f'Starting get_columns_details operation for table: {table.__name__}')\n\n        try:\n            # Log the start of the column retrieval\n            logger.debug(f'Getting columns for table: {table.__name__}')\n\n            # Retrieve the details of the columns and store them in a dictionary\n            # The keys are the column names and the values are dictionaries\n            # containing the column details\n            columns = {\n                c.name: {\n                    'type': str(c.type),\n                    'nullable': c.nullable,\n                    'primary_key': c.primary_key,\n                    'unique': c.unique,\n                    'autoincrement': c.autoincrement,\n                    'default': (\n                        str(c.default.arg)\n                        if c.default is not None and not callable(c.default.arg)\n                        else None\n                    ),\n                }\n                for c in table.__table__.columns\n            }\n\n            # Log the successful column retrieval\n            logger.info(f'Successfully retrieved columns for table: {table.__name__}')\n\n            return columns\n        except Exception as ex:  # pragma: no cover\n            # Handle any exceptions that occur during the column retrieval\n            logger.error(\n                f'An error occurred while getting columns for table: {table.__name__}'\n            )  # pragma: no cover\n            return handle_exceptions(ex)  # pragma: no cover\n\n    async def get_primary_keys(self, table):\n        \"\"\"\n        Retrieves the primary keys of a given table.\n\n        This asynchronous method accepts a table object and returns a list\n        containing the names of its primary keys. It is useful for understanding\n        the structure of the table and for operations that require knowledge of\n        the primary keys.\n\n        Args:\n            table (Table): An instance of the SQLAlchemy Table class\n            representing the database table for which primary keys are required.\n\n        Returns:\n            list: A list containing the names of the primary keys of the table.\n\n        Raises:\n            Exception: If any error occurs during the database operation.\n\n        Example:\n            ```python\n            from sqlalchemy import Table, MetaData, Column, Integer,\n                String from dsg_lib.async_database_functions import module_name metadata = MetaData()\n                my_table = Table('my_table', metadata,\n                                Column('id', Integer, primary_key=True),\n                                Column('name', String, primary_key=True))\n            from dsg_lib.async_database_functions import (\n                async_database,\n                base_schema,\n                database_config,\n                database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n\n            # get primary keys\n            primary_keys = await db_ops.get_primary_keys(my_table)\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug(f'Starting get_primary_keys operation for table: {table.__name__}')\n\n        try:\n            # Log the start of the primary key retrieval\n            logger.debug(f'Getting primary keys for table: {table.__name__}')\n\n            # Retrieve the primary keys and store them in a list\n            primary_keys = table.__table__.primary_key.columns.keys()\n\n            # Log the successful primary key retrieval\n            logger.info(f'Primary keys retrieved successfully: {primary_keys}')\n\n            return primary_keys\n\n        except Exception as ex:  # pragma: no cover\n            # Handle any exceptions that occur during the primary key retrieval\n            logger.error(f'Exception occurred: {ex}')  # pragma: no cover\n            return handle_exceptions(ex)  # pragma: no cover\n\n    async def get_table_names(self):\n        \"\"\"\n        Retrieves the names of all tables in the database.\n\n        This asynchronous method returns a list containing the names of all\n        tables in the database. It is useful for database introspection,\n        allowing the user to know which tables are available in the current\n        database context.\n\n        Returns:\n            list: A list containing the names of all tables in the database.\n\n        Raises:\n            Exception: If any error occurs during the database operation.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # get table names\n            table_names = await db_ops.get_table_names()\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug('Starting get_table_names operation')\n\n        try:\n            # Log the start of the table name retrieval\n            logger.debug('Retrieving table names')\n\n            # Retrieve the table names and store them in a list The keys of the\n            # metadata.tables dictionary are the table names\n            table_names = list(self.async_db.Base.metadata.tables.keys())\n\n            # Log the successful table name retrieval\n            logger.info(f'Table names retrieved successfully: {table_names}')\n\n            return table_names\n\n        except Exception as ex:  # pragma: no cover\n            # Handle any exceptions that occur during the table name retrieval\n            logger.error(f'Exception occurred: {ex}')  # pragma: no cover\n            return handle_exceptions(ex)  # pragma: no cover\n\n    async def read_one_record(self, query):\n        \"\"\"\n        Retrieves a single record from the database based on the provided query.\n\n        This asynchronous method accepts a SQL query object and returns the\n        first record that matches the query. If no record matches the query, it\n        returns None. This method is useful for fetching specific data\n        when the expected result is a single record.\n\n        Parameters:\n            query (Select): An instance of the SQLAlchemy Select class,\n            representing the query to be executed.\n\n        Returns:\n            Result: The first record that matches the query or None if no record matches.\n\n        Raises:\n            Exception: If any error occurs during the database operation.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # read one record\n            record = await db_ops.read_one_record(select(User).where(User.name == 'John Doe'))\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug(f'Starting read_one_record operation for {query}')\n\n        try:\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the start of the record retrieval\n                logger.debug(f'Getting record with query: {query}')\n\n                # Execute the query and retrieve the first record\n                result = await session.execute(query)\n                record = result.scalar_one()\n\n                # Log the successful record retrieval\n                logger.info(f'Record retrieved successfully: {record}')\n\n                return record\n\n        except NoResultFound:\n            # No record was found\n            logger.info('No record found')\n            return None\n\n        except Exception as ex:  # pragma: no cover\n            # Handle any exceptions that occur during the record retrieval\n            logger.error(f'Exception occurred: {ex}')  # pragma: no cover\n            return handle_exceptions(ex)  # pragma: no cover\n\n    async def create_one(self, record):\n        \"\"\"\n        Adds a single record to the database.\n\n        This asynchronous method accepts a record object and adds it to the\n        database. If the operation is successful, it returns the added record.\n        The method is useful for inserting a new row into a database table.\n\n        Parameters:\n            record (Base): An instance of the SQLAlchemy declarative base class\n            representing the record to be added to the database.\n\n        Returns:\n            Base: The instance of the record that was added to the database.\n\n        Raises:\n            Exception: If any error occurs during the database operation.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # create one record\n            record = await db_ops.create_one(User(name='John Doe'))\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug('Starting create_one operation')\n\n        try:\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the record being added\n                logger.debug(f'Adding record to session: {record.__dict__}')\n\n                # Add the record to the session and commit the changes\n                session.add(record)\n                await session.commit()\n\n                # Log the successful record addition\n                logger.info(f'Record added successfully: {record}')\n\n                return record\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the record addition\n            logger.error(f'Exception occurred: {ex}')\n            return handle_exceptions(ex)\n\n    async def create_many(self, records):\n        \"\"\"\n        Adds multiple records to the database.\n\n        This asynchronous method accepts a list of record objects and adds them\n        to the database. If the operation is successful, it returns the added\n        records. This method is useful for bulk inserting multiple rows into a\n        database table efficiently.\n\n        Parameters:\n            records (list[Base]): A list of instances of the SQLAlchemy\n            declarative base class, each representing a record to be added to\n            the database.\n\n        Returns:\n            list[Base]: A list of instances of the records that were added to\n            the database.\n\n        Raises:\n            Exception: If any error occurs during the database operation.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # create many records\n            records = await db_ops.create_many([User(name='John Doe'), User(name='Jane Doe')])\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug('Starting create_many operation')\n\n        try:\n            # Start a timer to measure the operation time\n            t0 = time.time()\n\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the number of records being added\n                logger.debug(f'Adding {len(records)} records to session')\n\n                # Add the records to the session and commit the changes\n                session.add_all(records)\n                await session.commit()\n\n                # Log the added records\n                records_data = [record.__dict__ for record in records]\n                logger.debug(f'Records added to session: {records_data}')\n\n                # Calculate the operation time and log the successful record\n                # addition\n                num_records = len(records)\n                t1 = time.time() - t0\n                logger.info(\n                    f'Record operations were successful. {num_records} records were created in {t1:.4f} seconds.'\n                )\n\n                return records\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the record addition\n            logger.error(f'Exception occurred: {ex}')\n            return handle_exceptions(ex)\n\n    async def count_query(self, query):\n        \"\"\"\n        Executes a count query on the database and returns the number of records\n        that match the query.\n\n        This asynchronous method accepts a SQLAlchemy `Select` query object and\n        returns the count of records that match the query. This is particularly\n        useful for getting the total number of records that satisfy certain\n        conditions without actually fetching the records themselves.\n\n        Parameters:\n            query (Select): A SQLAlchemy `Select` query object specifying the\n            conditions to count records for.\n\n        Returns:\n            int: The number of records that match the query.\n\n        Raises:\n            Exception: If any error occurs during the execution of the query.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # count query\n            count = await db_ops.count_query(select(User).where(User.age &gt; 30))\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug('Starting count_query operation')\n\n        try:\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the query being executed\n                logger.debug(f'Executing count query: {query}')\n\n                # Execute the count query and retrieve the count\n                result = await session.execute(select(func.count()).select_from(query.subquery()))\n                count = result.scalar()\n\n                # Log the successful query execution\n                logger.info(f'Count query executed successfully. Result: {count}')\n\n                return count\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the query execution\n            logger.error(f'Exception occurred: {ex}')\n            return handle_exceptions(ex)\n\n    async def read_query(self, query, limit=500, offset=0):\n        \"\"\"\n        Executes a fetch query on the database and returns a list of records\n        that match the query.\n\n        This asynchronous method accepts a SQLAlchemy `Select` query object\n        along with optional limit and offset parameters. It returns a list of\n        records that match the query, with the number of records controlled by\n        the limit, and the starting point of the records determined by the\n        offset.\n\n        Parameters:\n            query (Select): A SQLAlchemy `Select` query object specifying the\n            conditions to fetch records for. limit (int, optional): The maximum\n            number of records to return. Defaults to 500. offset (int,\n            optional): The number of records to skip before starting to return\n            records. Defaults to 0.\n\n        Returns:\n            list: A list of records that match the query.\n\n        Raises:\n            Exception: If any error occurs during the execution of the query.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # read query\n            records = await db_ops.read_query(select(User).where(User.age &gt; 30), limit=10)\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug('Starting read_query operation')\n\n        try:\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the query being executed\n                logger.debug(\n                    f'Executing fetch query: {query} with limit: {limit} and offset: {offset}'\n                )\n\n                # Execute the fetch query and retrieve the records\n                result = await session.execute(query.limit(limit).offset(offset))\n                records = result.scalars().all()\n                logger.debug(f'read_query result: {records}')\n                # Log the successful query execution\n                if all(isinstance(record, tuple) for record in records):\n                    logger.debug(f'read_query result is a tuple {type(records)}')\n                    # If all records are tuples, convert them to dictionaries\n                    records_data = [\n                        dict(zip(('request_group_id', 'count'), record, strict=False))\n                        for record in records\n                    ]\n                else:\n                    logger.debug(f'read_query result is a dictionary {type(records)}')\n                    # Otherwise, try to convert the records to dictionaries using the __dict__ attribute\n                    records_data = [record.__dict__ for record in records]\n\n                logger.info(f'Fetch query executed successfully. Records: {records_data}')\n\n                return records\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the query execution\n            logger.error(f'Exception occurred: {ex}')\n            return handle_exceptions(ex)\n\n    async def read_multi_query(self, queries: Dict[str, str], limit=500, offset=0):\n        \"\"\"\n        Executes multiple fetch queries on the database and returns a dictionary\n        of results for each query.\n\n        This asynchronous method takes a dictionary where each key is a query\n        name and each value is a SQLAlchemy `Select` query object. It also\n        accepts optional limit and offset parameters. The method executes each\n        query and returns a dictionary where each key is the query name, and the\n        corresponding value is a list of records that match that query.\n\n        Parameters:\n            queries (Dict[str, Select]): A dictionary of SQLAlchemy `Select`\n            query objects. limit (int, optional): The maximum number of records\n            to return for each query. Defaults to 500. offset (int, optional):\n            The number of records to skip before returning records for each\n            query. Defaults to 0.\n\n        Returns:\n            dict: A dictionary where each key is a query name and each value is\n            a list of records that match the query.\n\n        Raises:\n            Exception: If any error occurs during the execution of the queries.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # read multi query\n            queries = {\n                \"query1\": select(User).where(User.age &gt; 30),\n                \"query2\": select(User).where(User.age &lt; 20),\n            }\n            results = await db_ops.read_multi_query(queries, limit=10)\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug('Starting read_multi_query operation')\n\n        try:\n            results = {}\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                for query_name, query in queries.items():\n                    # Log the query being executed\n                    logger.debug(f'Executing fetch query: {query}')\n\n                    # Execute the fetch query and retrieve the records\n                    result = await session.execute(query.limit(limit).offset(offset))\n                    data = result.scalars().all()\n\n                    # Convert the records to dictionaries for logging\n                    data_dicts = [record.__dict__ for record in data]\n                    logger.debug(f\"Fetch result for query '{query_name}': {data_dicts}\")\n\n                    # Log the successful query execution\n                    logger.info(\n                        f'Fetch query executed successfully: {query_name} with {len(data)} records'\n                    )\n\n                    # Store the records in the results dictionary\n                    results[query_name] = data\n            return results\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the query execution\n            logger.error(f'Exception occurred: {ex}')\n            return handle_exceptions(ex)\n\n    async def update_one(self, table, record_id: str, new_values: dict):\n        \"\"\"\n        Updates a single record in the database identified by its ID.\n\n        This asynchronous method takes a SQLAlchemy `Table` object, a record ID,\n        and a dictionary of new values to update the record. It updates the\n        specified record in the given table with the new values. The method does\n        not allow updating certain fields, such as 'id' or 'date_created'.\n\n        Parameters:\n            table (Table): The SQLAlchemy `Table` object representing the table\n            in the database. record_id (str): The ID of the record to be\n            updated. new_values (dict): A dictionary containing the fields to\n            update and their new values.\n\n        Returns:\n            Base: The updated record if successful; otherwise, an error\n            dictionary.\n\n        Raises:\n            Exception: If any error occurs during the update operation.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # update one record\n            record = await db_ops.update_one(User, 1, {'name': 'John Smith'})\n            ```\n        \"\"\"\n        non_updatable_fields = ['id', 'date_created']\n\n        # Log the start of the operation\n        logger.debug(\n            f'Starting update_one operation for record_id: {record_id} in table: {table.__name__}'\n        )\n\n        try:\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the record being fetched\n                logger.debug(f'Fetching record with id: {record_id}')\n\n                # Fetch the record\n                record = await session.get(table, record_id)\n                if not record:\n                    # Log the error if no record is found\n                    logger.error(f'No record found with pkid: {record_id}')\n                    return {\n                        'error': 'Record not found',\n                        'details': f'No record found with pkid {record_id}',\n                    }\n\n                # Log the record being updated\n                logger.debug(f'Updating record with new values: {new_values}')\n\n                # Update the record with the new values\n                for key, value in new_values.items():\n                    if key not in non_updatable_fields:\n                        setattr(record, key, value)\n                await session.commit()\n\n                # Log the successful record update\n                logger.info(f'Record updated successfully: {record.pkid}')\n                return record\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the record update\n            logger.error(f'Exception occurred: {ex}')\n            return handle_exceptions(ex)\n\n    async def delete_one(self, table, record_id: str):\n        \"\"\"\n        Deletes a single record from the database based on the provided table\n        and record ID.\n\n        This asynchronous method accepts a SQLAlchemy `Table` object and a\n        record ID. It attempts to delete the record with the given ID from the\n        specified table. If the record is successfully deleted, it returns a\n        success message. If no record with the given ID is found, it returns an\n        error message.\n\n        Args:\n            table (Table): An instance of the SQLAlchemy `Table` class\n            representing the database table from which the record will be\n            deleted. record_id (str): The ID of the record to be deleted.\n\n        Returns:\n            dict: A dictionary containing a success message if the record was\n            deleted successfully, or an error message if the record was not\n            found or an exception occurred.\n\n        Raises:\n            Exception: If any error occurs during the delete operation.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # delete one record\n            result = await db_ops.delete_one(User, 1)\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug(\n            f'Starting delete_one operation for record_id: {record_id} in table: {table.__name__}'\n        )\n\n        try:\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the record being fetched\n                logger.debug(f'Fetching record with id: {record_id}')\n\n                # Fetch the record\n                record = await session.get(table, record_id)\n\n                # If the record doesn't exist, return an error\n                if not record:\n                    logger.error(f'No record found with pkid: {record_id}')\n                    return {\n                        'error': 'Record not found',\n                        'details': f'No record found with pkid {record_id}',\n                    }\n\n                # Log the record being deleted\n                logger.debug(f'Deleting record with id: {record_id}')\n\n                # Delete the record\n                await session.delete(record)\n\n                # Log the successful record deletion from the session\n                logger.debug(f'Record deleted from session: {record}')\n\n                # Log the start of the commit\n                logger.debug(f'Committing changes to delete record with id: {record_id}')\n\n                # Commit the changes\n                await session.commit()\n\n                # Log the successful record deletion\n                logger.info(f'Record deleted successfully: {record_id}')\n\n                return {'success': 'Record deleted successfully'}\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the record deletion\n            logger.error(f'Exception occurred: {ex}')\n            return handle_exceptions(ex)\n\n    async def delete_many(\n        self,\n        table: Type[DeclarativeMeta],\n        id_column_name: str = 'pkid',\n        id_values: List[int] = None,\n    ) -&gt; int:\n        \"\"\"\n        Deletes multiple records from the specified table in the database.\n\n        This method takes a table, an optional id column name, and a list of id values. It deletes the records in the table where the id column matches any of the id values in the list.\n\n        Args:\n            table (Type[DeclarativeMeta]): The table from which to delete records.\n            id_column_name (str, optional): The name of the id column in the table. Defaults to \"pkid\".\n            id_values (List[int], optional): A list of id values for the records to delete. Defaults to [].\n\n        Returns:\n            int: The number of records deleted from the table.\n\n        Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            \"pool_recycle\": 3600,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # Delete multiple records\n        deleted_count = await db_ops.delete_many(User, 'id', [1, 2, 3])\n        print(f\"Deleted {deleted_count} records.\")\n        ```\n        \"\"\"\n        if id_values is None:\n            id_values = []\n        try:\n            # Start a timer to measure the operation time\n            t0 = time.time()\n\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the number of records being deleted\n                logger.debug(f'Deleting {len(id_values)} records from session')\n\n                # Create delete statement\n                stmt = delete(table).where(getattr(table, id_column_name).in_(id_values))\n\n                # Execute the delete statement and fetch result\n                result = await session.execute(stmt)\n\n                # Commit the changes\n                await session.commit()\n\n                # Get the count of deleted records\n                deleted_count = result.rowcount\n\n                # Log the deleted records\n                logger.debug(f'Records deleted from session: {deleted_count}')\n\n                # Calculate the operation time and log the successful record deletion\n                t1 = time.time() - t0\n                logger.info(\n                    f'Record operations were successful. {deleted_count} records were deleted in {t1:.4f} seconds.'\n                )\n\n                return deleted_count\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the record deletion\n            logger.error(f'Exception occurred: {ex}')\n            return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.__init__","title":"<code>__init__(async_db)</code>","text":"<p>Initializes a new instance of the DatabaseOperations class.</p> <p>Parameters:</p> Name Type Description Default <code>async_db</code> <code>AsyncDatabase</code> <p>An instance of the</p> required <p>Example: <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n\ndb_config = database_config.DBConfig(config)\n\nasync_db = async_database.AsyncDatabase(db_config)\n\ndb_ops = database_operations.DatabaseOperations(async_db)\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>def __init__(self, async_db: AsyncDatabase):\n    \"\"\"\n    Initializes a new instance of the DatabaseOperations class.\n\n    Args:\n        async_db (module_name.AsyncDatabase): An instance of the\n        AsyncDatabase class for performing asynchronous database operations.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n    )\n\n    config = {\n        # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n        \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n        \"echo\": False,\n        \"future\": True,\n        # \"pool_pre_ping\": True,\n        # \"pool_size\": 10,\n        # \"max_overflow\": 10,\n        \"pool_recycle\": 3600,\n        # \"pool_timeout\": 30,\n    }\n\n    db_config = database_config.DBConfig(config)\n\n    async_db = async_database.AsyncDatabase(db_config)\n\n    db_ops = database_operations.DatabaseOperations(async_db)\n\n    ```\n    \"\"\"\n    # Log the start of the initialization\n    logger.debug('Initializing DatabaseOperations instance')\n\n    # Store the AsyncDatabase instance in the async_db attribute This\n    # instance will be used for performing asynchronous database operations\n    self.async_db = async_db\n\n    # Log the successful initialization\n    logger.info('DatabaseOperations instance initialized successfully')\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.count_query","title":"<code>count_query(query)</code>  <code>async</code>","text":"<p>Executes a count query on the database and returns the number of records that match the query.</p> <p>This asynchronous method accepts a SQLAlchemy <code>Select</code> query object and returns the count of records that match the query. This is particularly useful for getting the total number of records that satisfy certain conditions without actually fetching the records themselves.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Select</code> <p>A SQLAlchemy <code>Select</code> query object specifying the</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of records that match the query.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the execution of the query.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# count query\ncount = await db_ops.count_query(select(User).where(User.age &gt; 30))\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def count_query(self, query):\n    \"\"\"\n    Executes a count query on the database and returns the number of records\n    that match the query.\n\n    This asynchronous method accepts a SQLAlchemy `Select` query object and\n    returns the count of records that match the query. This is particularly\n    useful for getting the total number of records that satisfy certain\n    conditions without actually fetching the records themselves.\n\n    Parameters:\n        query (Select): A SQLAlchemy `Select` query object specifying the\n        conditions to count records for.\n\n    Returns:\n        int: The number of records that match the query.\n\n    Raises:\n        Exception: If any error occurs during the execution of the query.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # count query\n        count = await db_ops.count_query(select(User).where(User.age &gt; 30))\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug('Starting count_query operation')\n\n    try:\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the query being executed\n            logger.debug(f'Executing count query: {query}')\n\n            # Execute the count query and retrieve the count\n            result = await session.execute(select(func.count()).select_from(query.subquery()))\n            count = result.scalar()\n\n            # Log the successful query execution\n            logger.info(f'Count query executed successfully. Result: {count}')\n\n            return count\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the query execution\n        logger.error(f'Exception occurred: {ex}')\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.create_many","title":"<code>create_many(records)</code>  <code>async</code>","text":"<p>Adds multiple records to the database.</p> <p>This asynchronous method accepts a list of record objects and adds them to the database. If the operation is successful, it returns the added records. This method is useful for bulk inserting multiple rows into a database table efficiently.</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>list[Base]</code> <p>A list of instances of the SQLAlchemy</p> required <p>Returns:</p> Type Description <p>list[Base]: A list of instances of the records that were added to</p> <p>the database.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the database operation.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# create many records\nrecords = await db_ops.create_many([User(name='John Doe'), User(name='Jane Doe')])\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def create_many(self, records):\n    \"\"\"\n    Adds multiple records to the database.\n\n    This asynchronous method accepts a list of record objects and adds them\n    to the database. If the operation is successful, it returns the added\n    records. This method is useful for bulk inserting multiple rows into a\n    database table efficiently.\n\n    Parameters:\n        records (list[Base]): A list of instances of the SQLAlchemy\n        declarative base class, each representing a record to be added to\n        the database.\n\n    Returns:\n        list[Base]: A list of instances of the records that were added to\n        the database.\n\n    Raises:\n        Exception: If any error occurs during the database operation.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # create many records\n        records = await db_ops.create_many([User(name='John Doe'), User(name='Jane Doe')])\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug('Starting create_many operation')\n\n    try:\n        # Start a timer to measure the operation time\n        t0 = time.time()\n\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the number of records being added\n            logger.debug(f'Adding {len(records)} records to session')\n\n            # Add the records to the session and commit the changes\n            session.add_all(records)\n            await session.commit()\n\n            # Log the added records\n            records_data = [record.__dict__ for record in records]\n            logger.debug(f'Records added to session: {records_data}')\n\n            # Calculate the operation time and log the successful record\n            # addition\n            num_records = len(records)\n            t1 = time.time() - t0\n            logger.info(\n                f'Record operations were successful. {num_records} records were created in {t1:.4f} seconds.'\n            )\n\n            return records\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the record addition\n        logger.error(f'Exception occurred: {ex}')\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.create_one","title":"<code>create_one(record)</code>  <code>async</code>","text":"<p>Adds a single record to the database.</p> <p>This asynchronous method accepts a record object and adds it to the database. If the operation is successful, it returns the added record. The method is useful for inserting a new row into a database table.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Base</code> <p>An instance of the SQLAlchemy declarative base class</p> required <p>Returns:</p> Name Type Description <code>Base</code> <p>The instance of the record that was added to the database.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the database operation.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# create one record\nrecord = await db_ops.create_one(User(name='John Doe'))\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def create_one(self, record):\n    \"\"\"\n    Adds a single record to the database.\n\n    This asynchronous method accepts a record object and adds it to the\n    database. If the operation is successful, it returns the added record.\n    The method is useful for inserting a new row into a database table.\n\n    Parameters:\n        record (Base): An instance of the SQLAlchemy declarative base class\n        representing the record to be added to the database.\n\n    Returns:\n        Base: The instance of the record that was added to the database.\n\n    Raises:\n        Exception: If any error occurs during the database operation.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # create one record\n        record = await db_ops.create_one(User(name='John Doe'))\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug('Starting create_one operation')\n\n    try:\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the record being added\n            logger.debug(f'Adding record to session: {record.__dict__}')\n\n            # Add the record to the session and commit the changes\n            session.add(record)\n            await session.commit()\n\n            # Log the successful record addition\n            logger.info(f'Record added successfully: {record}')\n\n            return record\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the record addition\n        logger.error(f'Exception occurred: {ex}')\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.delete_many","title":"<code>delete_many(table, id_column_name='pkid', id_values=None)</code>  <code>async</code>","text":"<p>Deletes multiple records from the specified table in the database.</p> <p>This method takes a table, an optional id column name, and a list of id values. It deletes the records in the table where the id column matches any of the id values in the list.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Type[DeclarativeMeta]</code> <p>The table from which to delete records.</p> required <code>id_column_name</code> <code>str</code> <p>The name of the id column in the table. Defaults to \"pkid\".</p> <code>'pkid'</code> <code>id_values</code> <code>List[int]</code> <p>A list of id values for the records to delete. Defaults to [].</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of records deleted from the table.</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    \"pool_recycle\": 3600,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# Delete multiple records\ndeleted_count = await db_ops.delete_many(User, 'id', [1, 2, 3])\nprint(f\"Deleted {deleted_count} records.\")\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def delete_many(\n    self,\n    table: Type[DeclarativeMeta],\n    id_column_name: str = 'pkid',\n    id_values: List[int] = None,\n) -&gt; int:\n    \"\"\"\n    Deletes multiple records from the specified table in the database.\n\n    This method takes a table, an optional id column name, and a list of id values. It deletes the records in the table where the id column matches any of the id values in the list.\n\n    Args:\n        table (Type[DeclarativeMeta]): The table from which to delete records.\n        id_column_name (str, optional): The name of the id column in the table. Defaults to \"pkid\".\n        id_values (List[int], optional): A list of id values for the records to delete. Defaults to [].\n\n    Returns:\n        int: The number of records deleted from the table.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n    )\n    # Create a DBConfig instance\n    config = {\n        \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n        \"echo\": False,\n        \"future\": True,\n        \"pool_recycle\": 3600,\n    }\n    # create database configuration\n    db_config = database_config.DBConfig(config)\n    # Create an AsyncDatabase instance\n    async_db = async_database.AsyncDatabase(db_config)\n    # Create a DatabaseOperations instance\n    db_ops = database_operations.DatabaseOperations(async_db)\n    # Delete multiple records\n    deleted_count = await db_ops.delete_many(User, 'id', [1, 2, 3])\n    print(f\"Deleted {deleted_count} records.\")\n    ```\n    \"\"\"\n    if id_values is None:\n        id_values = []\n    try:\n        # Start a timer to measure the operation time\n        t0 = time.time()\n\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the number of records being deleted\n            logger.debug(f'Deleting {len(id_values)} records from session')\n\n            # Create delete statement\n            stmt = delete(table).where(getattr(table, id_column_name).in_(id_values))\n\n            # Execute the delete statement and fetch result\n            result = await session.execute(stmt)\n\n            # Commit the changes\n            await session.commit()\n\n            # Get the count of deleted records\n            deleted_count = result.rowcount\n\n            # Log the deleted records\n            logger.debug(f'Records deleted from session: {deleted_count}')\n\n            # Calculate the operation time and log the successful record deletion\n            t1 = time.time() - t0\n            logger.info(\n                f'Record operations were successful. {deleted_count} records were deleted in {t1:.4f} seconds.'\n            )\n\n            return deleted_count\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the record deletion\n        logger.error(f'Exception occurred: {ex}')\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.delete_one","title":"<code>delete_one(table, record_id)</code>  <code>async</code>","text":"<p>Deletes a single record from the database based on the provided table and record ID.</p> <p>This asynchronous method accepts a SQLAlchemy <code>Table</code> object and a record ID. It attempts to delete the record with the given ID from the specified table. If the record is successfully deleted, it returns a success message. If no record with the given ID is found, it returns an error message.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>An instance of the SQLAlchemy <code>Table</code> class</p> required <code>deleted.</code> <code>record_id (str</code> <p>The ID of the record to be deleted.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing a success message if the record was</p> <p>deleted successfully, or an error message if the record was not</p> <p>found or an exception occurred.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the delete operation.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# delete one record\nresult = await db_ops.delete_one(User, 1)\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def delete_one(self, table, record_id: str):\n    \"\"\"\n    Deletes a single record from the database based on the provided table\n    and record ID.\n\n    This asynchronous method accepts a SQLAlchemy `Table` object and a\n    record ID. It attempts to delete the record with the given ID from the\n    specified table. If the record is successfully deleted, it returns a\n    success message. If no record with the given ID is found, it returns an\n    error message.\n\n    Args:\n        table (Table): An instance of the SQLAlchemy `Table` class\n        representing the database table from which the record will be\n        deleted. record_id (str): The ID of the record to be deleted.\n\n    Returns:\n        dict: A dictionary containing a success message if the record was\n        deleted successfully, or an error message if the record was not\n        found or an exception occurred.\n\n    Raises:\n        Exception: If any error occurs during the delete operation.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # delete one record\n        result = await db_ops.delete_one(User, 1)\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug(\n        f'Starting delete_one operation for record_id: {record_id} in table: {table.__name__}'\n    )\n\n    try:\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the record being fetched\n            logger.debug(f'Fetching record with id: {record_id}')\n\n            # Fetch the record\n            record = await session.get(table, record_id)\n\n            # If the record doesn't exist, return an error\n            if not record:\n                logger.error(f'No record found with pkid: {record_id}')\n                return {\n                    'error': 'Record not found',\n                    'details': f'No record found with pkid {record_id}',\n                }\n\n            # Log the record being deleted\n            logger.debug(f'Deleting record with id: {record_id}')\n\n            # Delete the record\n            await session.delete(record)\n\n            # Log the successful record deletion from the session\n            logger.debug(f'Record deleted from session: {record}')\n\n            # Log the start of the commit\n            logger.debug(f'Committing changes to delete record with id: {record_id}')\n\n            # Commit the changes\n            await session.commit()\n\n            # Log the successful record deletion\n            logger.info(f'Record deleted successfully: {record_id}')\n\n            return {'success': 'Record deleted successfully'}\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the record deletion\n        logger.error(f'Exception occurred: {ex}')\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.get_columns_details","title":"<code>get_columns_details(table)</code>  <code>async</code>","text":"<p>Retrieves the details of the columns of a given table.</p> <p>This asynchronous method accepts a table object and returns a dictionary. Each key in the dictionary is a column name from the table, and the corresponding value is another dictionary containing details about that column, such as type, if it's nullable, if it's a primary key, if it's unique, its autoincrement status, and its default value.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>An instance of the SQLAlchemy Table class</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where each key is a column name, and each value</p> <p>is a dictionary with the column's details.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the database operation.</p> <p>Example: <pre><code>from sqlalchemy import Table, MetaData, Column,\nInteger, String from dsg_lib.async_database_functions import module_name metadata = MetaData()\nmy_table = Table('my_table', metadata,\n                Column('id', Integer, primary_key=True), Column('name',\n                String))\n\nfrom dsg_lib.async_database_functions import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# get columns details\ncolumns = await db_ops.get_columns_details(my_table)\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def get_columns_details(self, table):\n    \"\"\"\n    Retrieves the details of the columns of a given table.\n\n    This asynchronous method accepts a table object and returns a\n    dictionary. Each key in the dictionary is a column name from the table,\n    and the corresponding value is another dictionary containing details\n    about that column, such as type, if it's nullable, if it's a primary\n    key, if it's unique, its autoincrement status, and its default value.\n\n    Args:\n        table (Table): An instance of the SQLAlchemy Table class\n        representing the database table for which column details are\n        required.\n\n    Returns:\n        dict: A dictionary where each key is a column name, and each value\n        is a dictionary with the column's details.\n\n    Raises:\n        Exception: If any error occurs during the database operation.\n\n    Example:\n    ```python\n    from sqlalchemy import Table, MetaData, Column,\n    Integer, String from dsg_lib.async_database_functions import module_name metadata = MetaData()\n    my_table = Table('my_table', metadata,\n                    Column('id', Integer, primary_key=True), Column('name',\n                    String))\n\n    from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n    )\n    # Create a DBConfig instance\n    config = {\n        # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n        \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n        \"echo\": False,\n        \"future\": True,\n        # \"pool_pre_ping\": True,\n        # \"pool_size\": 10,\n        # \"max_overflow\": 10,\n        \"pool_recycle\": 3600,\n        # \"pool_timeout\": 30,\n    }\n    # create database configuration\n    db_config = database_config.DBConfig(config)\n    # Create an AsyncDatabase instance\n    async_db = async_database.AsyncDatabase(db_config)\n    # Create a DatabaseOperations instance\n    db_ops = database_operations.DatabaseOperations(async_db)\n    # get columns details\n    columns = await db_ops.get_columns_details(my_table)\n    ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug(f'Starting get_columns_details operation for table: {table.__name__}')\n\n    try:\n        # Log the start of the column retrieval\n        logger.debug(f'Getting columns for table: {table.__name__}')\n\n        # Retrieve the details of the columns and store them in a dictionary\n        # The keys are the column names and the values are dictionaries\n        # containing the column details\n        columns = {\n            c.name: {\n                'type': str(c.type),\n                'nullable': c.nullable,\n                'primary_key': c.primary_key,\n                'unique': c.unique,\n                'autoincrement': c.autoincrement,\n                'default': (\n                    str(c.default.arg)\n                    if c.default is not None and not callable(c.default.arg)\n                    else None\n                ),\n            }\n            for c in table.__table__.columns\n        }\n\n        # Log the successful column retrieval\n        logger.info(f'Successfully retrieved columns for table: {table.__name__}')\n\n        return columns\n    except Exception as ex:  # pragma: no cover\n        # Handle any exceptions that occur during the column retrieval\n        logger.error(\n            f'An error occurred while getting columns for table: {table.__name__}'\n        )  # pragma: no cover\n        return handle_exceptions(ex)  # pragma: no cover\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.get_primary_keys","title":"<code>get_primary_keys(table)</code>  <code>async</code>","text":"<p>Retrieves the primary keys of a given table.</p> <p>This asynchronous method accepts a table object and returns a list containing the names of its primary keys. It is useful for understanding the structure of the table and for operations that require knowledge of the primary keys.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>An instance of the SQLAlchemy Table class</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list containing the names of the primary keys of the table.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the database operation.</p> Example <pre><code>from sqlalchemy import Table, MetaData, Column, Integer,\n    String from dsg_lib.async_database_functions import module_name metadata = MetaData()\n    my_table = Table('my_table', metadata,\n                    Column('id', Integer, primary_key=True),\n                    Column('name', String, primary_key=True))\nfrom dsg_lib.async_database_functions import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n\n# get primary keys\nprimary_keys = await db_ops.get_primary_keys(my_table)\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def get_primary_keys(self, table):\n    \"\"\"\n    Retrieves the primary keys of a given table.\n\n    This asynchronous method accepts a table object and returns a list\n    containing the names of its primary keys. It is useful for understanding\n    the structure of the table and for operations that require knowledge of\n    the primary keys.\n\n    Args:\n        table (Table): An instance of the SQLAlchemy Table class\n        representing the database table for which primary keys are required.\n\n    Returns:\n        list: A list containing the names of the primary keys of the table.\n\n    Raises:\n        Exception: If any error occurs during the database operation.\n\n    Example:\n        ```python\n        from sqlalchemy import Table, MetaData, Column, Integer,\n            String from dsg_lib.async_database_functions import module_name metadata = MetaData()\n            my_table = Table('my_table', metadata,\n                            Column('id', Integer, primary_key=True),\n                            Column('name', String, primary_key=True))\n        from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n\n        # get primary keys\n        primary_keys = await db_ops.get_primary_keys(my_table)\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug(f'Starting get_primary_keys operation for table: {table.__name__}')\n\n    try:\n        # Log the start of the primary key retrieval\n        logger.debug(f'Getting primary keys for table: {table.__name__}')\n\n        # Retrieve the primary keys and store them in a list\n        primary_keys = table.__table__.primary_key.columns.keys()\n\n        # Log the successful primary key retrieval\n        logger.info(f'Primary keys retrieved successfully: {primary_keys}')\n\n        return primary_keys\n\n    except Exception as ex:  # pragma: no cover\n        # Handle any exceptions that occur during the primary key retrieval\n        logger.error(f'Exception occurred: {ex}')  # pragma: no cover\n        return handle_exceptions(ex)  # pragma: no cover\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.get_table_names","title":"<code>get_table_names()</code>  <code>async</code>","text":"<p>Retrieves the names of all tables in the database.</p> <p>This asynchronous method returns a list containing the names of all tables in the database. It is useful for database introspection, allowing the user to know which tables are available in the current database context.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list containing the names of all tables in the database.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the database operation.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# get table names\ntable_names = await db_ops.get_table_names()\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def get_table_names(self):\n    \"\"\"\n    Retrieves the names of all tables in the database.\n\n    This asynchronous method returns a list containing the names of all\n    tables in the database. It is useful for database introspection,\n    allowing the user to know which tables are available in the current\n    database context.\n\n    Returns:\n        list: A list containing the names of all tables in the database.\n\n    Raises:\n        Exception: If any error occurs during the database operation.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # get table names\n        table_names = await db_ops.get_table_names()\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug('Starting get_table_names operation')\n\n    try:\n        # Log the start of the table name retrieval\n        logger.debug('Retrieving table names')\n\n        # Retrieve the table names and store them in a list The keys of the\n        # metadata.tables dictionary are the table names\n        table_names = list(self.async_db.Base.metadata.tables.keys())\n\n        # Log the successful table name retrieval\n        logger.info(f'Table names retrieved successfully: {table_names}')\n\n        return table_names\n\n    except Exception as ex:  # pragma: no cover\n        # Handle any exceptions that occur during the table name retrieval\n        logger.error(f'Exception occurred: {ex}')  # pragma: no cover\n        return handle_exceptions(ex)  # pragma: no cover\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.read_multi_query","title":"<code>read_multi_query(queries, limit=500, offset=0)</code>  <code>async</code>","text":"<p>Executes multiple fetch queries on the database and returns a dictionary of results for each query.</p> <p>This asynchronous method takes a dictionary where each key is a query name and each value is a SQLAlchemy <code>Select</code> query object. It also accepts optional limit and offset parameters. The method executes each query and returns a dictionary where each key is the query name, and the corresponding value is a list of records that match that query.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Dict[str, Select]</code> <p>A dictionary of SQLAlchemy <code>Select</code></p> required <code>query</code> <code>objects. limit (int</code> <p>The maximum number of records</p> required <code>to</code> <code>return for each query. Defaults to 500. offset (int</code> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where each key is a query name and each value is</p> <p>a list of records that match the query.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the execution of the queries.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# read multi query\nqueries = {\n    \"query1\": select(User).where(User.age &gt; 30),\n    \"query2\": select(User).where(User.age &lt; 20),\n}\nresults = await db_ops.read_multi_query(queries, limit=10)\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def read_multi_query(self, queries: Dict[str, str], limit=500, offset=0):\n    \"\"\"\n    Executes multiple fetch queries on the database and returns a dictionary\n    of results for each query.\n\n    This asynchronous method takes a dictionary where each key is a query\n    name and each value is a SQLAlchemy `Select` query object. It also\n    accepts optional limit and offset parameters. The method executes each\n    query and returns a dictionary where each key is the query name, and the\n    corresponding value is a list of records that match that query.\n\n    Parameters:\n        queries (Dict[str, Select]): A dictionary of SQLAlchemy `Select`\n        query objects. limit (int, optional): The maximum number of records\n        to return for each query. Defaults to 500. offset (int, optional):\n        The number of records to skip before returning records for each\n        query. Defaults to 0.\n\n    Returns:\n        dict: A dictionary where each key is a query name and each value is\n        a list of records that match the query.\n\n    Raises:\n        Exception: If any error occurs during the execution of the queries.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # read multi query\n        queries = {\n            \"query1\": select(User).where(User.age &gt; 30),\n            \"query2\": select(User).where(User.age &lt; 20),\n        }\n        results = await db_ops.read_multi_query(queries, limit=10)\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug('Starting read_multi_query operation')\n\n    try:\n        results = {}\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            for query_name, query in queries.items():\n                # Log the query being executed\n                logger.debug(f'Executing fetch query: {query}')\n\n                # Execute the fetch query and retrieve the records\n                result = await session.execute(query.limit(limit).offset(offset))\n                data = result.scalars().all()\n\n                # Convert the records to dictionaries for logging\n                data_dicts = [record.__dict__ for record in data]\n                logger.debug(f\"Fetch result for query '{query_name}': {data_dicts}\")\n\n                # Log the successful query execution\n                logger.info(\n                    f'Fetch query executed successfully: {query_name} with {len(data)} records'\n                )\n\n                # Store the records in the results dictionary\n                results[query_name] = data\n        return results\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the query execution\n        logger.error(f'Exception occurred: {ex}')\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.read_one_record","title":"<code>read_one_record(query)</code>  <code>async</code>","text":"<p>Retrieves a single record from the database based on the provided query.</p> <p>This asynchronous method accepts a SQL query object and returns the first record that matches the query. If no record matches the query, it returns None. This method is useful for fetching specific data when the expected result is a single record.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Select</code> <p>An instance of the SQLAlchemy Select class,</p> required <p>Returns:</p> Name Type Description <code>Result</code> <p>The first record that matches the query or None if no record matches.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the database operation.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# read one record\nrecord = await db_ops.read_one_record(select(User).where(User.name == 'John Doe'))\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def read_one_record(self, query):\n    \"\"\"\n    Retrieves a single record from the database based on the provided query.\n\n    This asynchronous method accepts a SQL query object and returns the\n    first record that matches the query. If no record matches the query, it\n    returns None. This method is useful for fetching specific data\n    when the expected result is a single record.\n\n    Parameters:\n        query (Select): An instance of the SQLAlchemy Select class,\n        representing the query to be executed.\n\n    Returns:\n        Result: The first record that matches the query or None if no record matches.\n\n    Raises:\n        Exception: If any error occurs during the database operation.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # read one record\n        record = await db_ops.read_one_record(select(User).where(User.name == 'John Doe'))\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug(f'Starting read_one_record operation for {query}')\n\n    try:\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the start of the record retrieval\n            logger.debug(f'Getting record with query: {query}')\n\n            # Execute the query and retrieve the first record\n            result = await session.execute(query)\n            record = result.scalar_one()\n\n            # Log the successful record retrieval\n            logger.info(f'Record retrieved successfully: {record}')\n\n            return record\n\n    except NoResultFound:\n        # No record was found\n        logger.info('No record found')\n        return None\n\n    except Exception as ex:  # pragma: no cover\n        # Handle any exceptions that occur during the record retrieval\n        logger.error(f'Exception occurred: {ex}')  # pragma: no cover\n        return handle_exceptions(ex)  # pragma: no cover\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.read_query","title":"<code>read_query(query, limit=500, offset=0)</code>  <code>async</code>","text":"<p>Executes a fetch query on the database and returns a list of records that match the query.</p> <p>This asynchronous method accepts a SQLAlchemy <code>Select</code> query object along with optional limit and offset parameters. It returns a list of records that match the query, with the number of records controlled by the limit, and the starting point of the records determined by the offset.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Select</code> <p>A SQLAlchemy <code>Select</code> query object specifying the</p> required <code>conditions</code> <code>to fetch records for. limit (int</code> <p>The maximum</p> required <code>optional)</code> <p>The number of records to skip before starting to return</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of records that match the query.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the execution of the query.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# read query\nrecords = await db_ops.read_query(select(User).where(User.age &gt; 30), limit=10)\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def read_query(self, query, limit=500, offset=0):\n    \"\"\"\n    Executes a fetch query on the database and returns a list of records\n    that match the query.\n\n    This asynchronous method accepts a SQLAlchemy `Select` query object\n    along with optional limit and offset parameters. It returns a list of\n    records that match the query, with the number of records controlled by\n    the limit, and the starting point of the records determined by the\n    offset.\n\n    Parameters:\n        query (Select): A SQLAlchemy `Select` query object specifying the\n        conditions to fetch records for. limit (int, optional): The maximum\n        number of records to return. Defaults to 500. offset (int,\n        optional): The number of records to skip before starting to return\n        records. Defaults to 0.\n\n    Returns:\n        list: A list of records that match the query.\n\n    Raises:\n        Exception: If any error occurs during the execution of the query.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # read query\n        records = await db_ops.read_query(select(User).where(User.age &gt; 30), limit=10)\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug('Starting read_query operation')\n\n    try:\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the query being executed\n            logger.debug(\n                f'Executing fetch query: {query} with limit: {limit} and offset: {offset}'\n            )\n\n            # Execute the fetch query and retrieve the records\n            result = await session.execute(query.limit(limit).offset(offset))\n            records = result.scalars().all()\n            logger.debug(f'read_query result: {records}')\n            # Log the successful query execution\n            if all(isinstance(record, tuple) for record in records):\n                logger.debug(f'read_query result is a tuple {type(records)}')\n                # If all records are tuples, convert them to dictionaries\n                records_data = [\n                    dict(zip(('request_group_id', 'count'), record, strict=False))\n                    for record in records\n                ]\n            else:\n                logger.debug(f'read_query result is a dictionary {type(records)}')\n                # Otherwise, try to convert the records to dictionaries using the __dict__ attribute\n                records_data = [record.__dict__ for record in records]\n\n            logger.info(f'Fetch query executed successfully. Records: {records_data}')\n\n            return records\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the query execution\n        logger.error(f'Exception occurred: {ex}')\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.update_one","title":"<code>update_one(table, record_id, new_values)</code>  <code>async</code>","text":"<p>Updates a single record in the database identified by its ID.</p> <p>This asynchronous method takes a SQLAlchemy <code>Table</code> object, a record ID, and a dictionary of new values to update the record. It updates the specified record in the given table with the new values. The method does not allow updating certain fields, such as 'id' or 'date_created'.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The SQLAlchemy <code>Table</code> object representing the table</p> required <code>in</code> <code>the database. record_id (str</code> <p>The ID of the record to be</p> required <code>updated.</code> <code>new_values (dict</code> <p>A dictionary containing the fields to</p> required <p>Returns:</p> Name Type Description <code>Base</code> <p>The updated record if successful; otherwise, an error</p> <p>dictionary.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the update operation.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# update one record\nrecord = await db_ops.update_one(User, 1, {'name': 'John Smith'})\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def update_one(self, table, record_id: str, new_values: dict):\n    \"\"\"\n    Updates a single record in the database identified by its ID.\n\n    This asynchronous method takes a SQLAlchemy `Table` object, a record ID,\n    and a dictionary of new values to update the record. It updates the\n    specified record in the given table with the new values. The method does\n    not allow updating certain fields, such as 'id' or 'date_created'.\n\n    Parameters:\n        table (Table): The SQLAlchemy `Table` object representing the table\n        in the database. record_id (str): The ID of the record to be\n        updated. new_values (dict): A dictionary containing the fields to\n        update and their new values.\n\n    Returns:\n        Base: The updated record if successful; otherwise, an error\n        dictionary.\n\n    Raises:\n        Exception: If any error occurs during the update operation.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # update one record\n        record = await db_ops.update_one(User, 1, {'name': 'John Smith'})\n        ```\n    \"\"\"\n    non_updatable_fields = ['id', 'date_created']\n\n    # Log the start of the operation\n    logger.debug(\n        f'Starting update_one operation for record_id: {record_id} in table: {table.__name__}'\n    )\n\n    try:\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the record being fetched\n            logger.debug(f'Fetching record with id: {record_id}')\n\n            # Fetch the record\n            record = await session.get(table, record_id)\n            if not record:\n                # Log the error if no record is found\n                logger.error(f'No record found with pkid: {record_id}')\n                return {\n                    'error': 'Record not found',\n                    'details': f'No record found with pkid {record_id}',\n                }\n\n            # Log the record being updated\n            logger.debug(f'Updating record with new values: {new_values}')\n\n            # Update the record with the new values\n            for key, value in new_values.items():\n                if key not in non_updatable_fields:\n                    setattr(record, key, value)\n            await session.commit()\n\n            # Log the successful record update\n            logger.info(f'Record updated successfully: {record.pkid}')\n            return record\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the record update\n        logger.error(f'Exception occurred: {ex}')\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.handle_exceptions","title":"<code>handle_exceptions(ex)</code>","text":"<p>Handles exceptions for database operations.</p> <p>This function checks the type of the exception, logs an appropriate error message, and returns a dictionary containing the error details.</p> <p>Parameters:</p> Name Type Description Default <code>ex</code> <code>Exception</code> <p>The exception to handle.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, str]</code> <p>A dictionary containing the error details. The dictionary has two</p> <code>keys</code> <code>Dict[str, str]</code> <p>'error' and 'details'.</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import database_operations\n\ntry:\n    # Some database operation that might raise an exception pass\nexcept Exception as ex:\n    error_details = database_operations.handle_exceptions(ex)\n    print(error_details)\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>def handle_exceptions(ex: Exception) -&gt; Dict[str, str]:\n    \"\"\"\n    Handles exceptions for database operations.\n\n    This function checks the type of the exception, logs an appropriate error\n    message, and returns a dictionary containing the error details.\n\n    Args:\n        ex (Exception): The exception to handle.\n\n    Returns:\n        dict: A dictionary containing the error details. The dictionary has two\n        keys: 'error' and 'details'.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import database_operations\n\n    try:\n        # Some database operation that might raise an exception pass\n    except Exception as ex:\n        error_details = database_operations.handle_exceptions(ex)\n        print(error_details)\n    ```\n    \"\"\"\n    # Extract the error message before the SQL statement\n    error_only = str(ex).split('[SQL:')[0]\n\n    # Check the type of the exception\n    if isinstance(ex, IntegrityError):\n        # Log the error and return the error details\n        logger.error(f'IntegrityError occurred: {ex}')\n        return {'error': 'IntegrityError', 'details': error_only}\n    elif isinstance(ex, SQLAlchemyError):\n        # Log the error and return the error details\n        logger.error(f'SQLAlchemyError occurred: {ex}')\n        return {'error': 'SQLAlchemyError', 'details': error_only}\n    else:\n        # Log the error and return the error details\n        logger.error(f'Exception occurred: {ex}')\n        return {'error': 'General Exception', 'details': str(ex)}\n</code></pre>"},{"location":"fastapi/http_codes/","title":"Reference","text":""},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes","title":"<code>dsg_lib.fastapi_functions.http_codes</code>","text":"<p>http_codes.py</p> <p>This module provides a dictionary of HTTP status codes and their descriptions.</p> <p>The dictionary <code>ALL_HTTP_CODES</code> contains the HTTP status codes as keys. Each key maps to another dictionary that contains a description of the status code, an extended description, and a link to its documentation on the Mozilla Developer Network (MDN).</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Get the description, extended description, and link for HTTP status code 200\nstatus_200 = http_codes.ALL_HTTP_CODES[200]\nprint(status_200)\n# {'description': 'OK', 'extended_description': 'The request has succeeded', 'link': 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200'}\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>ALL_HTTP_CODES</code> <code>dict</code> <p>A dictionary of HTTP status codes. Each key is an</p>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes.DELETE_CODES","title":"<code>DELETE_CODES = generate_code_dict(common_codes + [202, 204, 205, 409])</code>  <code>module-attribute</code>","text":"<p>DELETE_CODES is a dictionary of HTTP status codes for DELETE requests. It includes all the common codes, plus some additional codes that are specific to DELETE requests.</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Print the dictionary of HTTP status codes for DELETE requests\nprint(http_codes.DELETE_CODES)\n</code></pre></p>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes.GET_CODES","title":"<code>GET_CODES = generate_code_dict(common_codes + [206, 304, 307, 410, 502])</code>  <code>module-attribute</code>","text":"<p>GET_CODES is a dictionary of HTTP status codes for GET requests. It includes all the common codes, plus some additional codes that are specific to GET requests.</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Print the dictionary of HTTP status codes for GET requests\nprint(http_codes.GET_CODES)\n</code></pre></p>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes.PATCH_CODES","title":"<code>PATCH_CODES = generate_code_dict(common_codes + [202, 204, 206, 409, 412, 413])</code>  <code>module-attribute</code>","text":"<p>PATCH_CODES is a dictionary of HTTP status codes for PATCH requests. It includes all the common codes, plus some additional codes that are specific to PATCH requests.</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Print the dictionary of HTTP status codes for PATCH requests\nprint(http_codes.PATCH_CODES)\n</code></pre></p>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes.POST_CODES","title":"<code>POST_CODES = generate_code_dict(common_codes + [201, 202, 205, 307, 409, 413, 415])</code>  <code>module-attribute</code>","text":"<p>POST_CODES is a dictionary of HTTP status codes for POST requests. It includes all the common codes, plus some additional codes that are specific to POST requests.</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Print the dictionary of HTTP status codes for POST requests\nprint(http_codes.POST_CODES)\n</code></pre></p>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes.PUT_CODES","title":"<code>PUT_CODES = generate_code_dict(common_codes + [202, 204, 206, 409, 412, 413])</code>  <code>module-attribute</code>","text":"<p>PUT_CODES is a dictionary of HTTP status codes for PUT requests. It includes all the common codes, plus some additional codes that are specific to PUT requests.</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Print the dictionary of HTTP status codes for PUT requests\nprint(http_codes.PUT_CODES)\n</code></pre></p>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes.generate_code_dict","title":"<code>generate_code_dict(codes, description_only=False)</code>","text":"<p>Generate a dictionary of specific HTTP error codes from the http_codes dictionary.</p> <p>This function takes a list of HTTP status codes and an optional boolean flag. If the flag is True, the function returns a dictionary where each key is an HTTP status code from the input list and each value is the corresponding description from the ALL_HTTP_CODES dictionary. If the flag is False, the function returns a dictionary where each key is an HTTP status code from the input list and each value is the corresponding dictionary from the ALL_HTTP_CODES dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>codes</code> <code>list</code> <p>A list of HTTP status codes.</p> required <code>description_only</code> <code>bool</code> <p>If True, only the description of the codes will be returned.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where each key is an HTTP error code from the input</p> <p>list and each value depends on the description_only parameter. If</p> <p>description_only is True, the value is the description string. If</p> <p>description_only is False, the value is a dictionary with keys</p> <p>'description', 'extended_description', and 'link'.</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Generate a dictionary for HTTP status codes 200 and 404\nstatus_dict = http_codes.generate_code_dict([200, 404])\nprint(status_dict)\n# {200: {'description': 'OK', 'extended_description': 'The request has succeeded', 'link': 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200'},\n#  404: {'description': 'Not Found', 'extended_description': 'The requested resource could not be found', 'link': 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404'}}\n\n# Generate a dictionary for HTTP status codes 200 and 404 with only descriptions\nstatus_dict = http_codes.generate_code_dict([200, 404], description_only=True)\nprint(status_dict)  # {200: 'OK', 404: 'Not Found'}\n</code></pre></p> Source code in <code>dsg_lib/fastapi_functions/http_codes.py</code> <pre><code>def generate_code_dict(codes, description_only=False):\n    \"\"\"\n    Generate a dictionary of specific HTTP error codes from the http_codes\n    dictionary.\n\n    This function takes a list of HTTP status codes and an optional boolean\n    flag. If the flag is True, the function returns a dictionary where each key\n    is an HTTP status code from the input list and each value is the\n    corresponding description from the ALL_HTTP_CODES dictionary. If the flag is\n    False, the function returns a dictionary where each key is an HTTP status\n    code from the input list and each value is the corresponding dictionary from\n    the ALL_HTTP_CODES dictionary.\n\n    Args:\n        codes (list): A list of HTTP status codes.\n        description_only (bool, optional): If True, only the description of the codes will be returned.\n        Defaults to False.\n\n    Returns:\n        dict: A dictionary where each key is an HTTP error code from the input\n        list and each value depends on the description_only parameter. If\n        description_only is True, the value is the description string. If\n        description_only is False, the value is a dictionary with keys\n        'description', 'extended_description', and 'link'.\n\n    Example:\n    ```python\n\n    from dsg_lib.fastapi_functions import http_codes\n\n    # Generate a dictionary for HTTP status codes 200 and 404\n    status_dict = http_codes.generate_code_dict([200, 404])\n    print(status_dict)\n    # {200: {'description': 'OK', 'extended_description': 'The request has succeeded', 'link': 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200'},\n    #  404: {'description': 'Not Found', 'extended_description': 'The requested resource could not be found', 'link': 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404'}}\n\n    # Generate a dictionary for HTTP status codes 200 and 404 with only descriptions\n    status_dict = http_codes.generate_code_dict([200, 404], description_only=True)\n    print(status_dict)  # {200: 'OK', 404: 'Not Found'}\n    ```\n    \"\"\"\n\n    if description_only:\n        # Log the operation\n        logger.debug(f'description_only is True and returning HTTP codes: {codes}')\n\n        # If description_only is True, return a dictionary where each key is an\n        # HTTP error code from the input list and each value is the\n        # corresponding description from the ALL_HTTP_CODES dictionary.\n        return {\n            code: ALL_HTTP_CODES[code]['description'] for code in codes if code in ALL_HTTP_CODES\n        }\n    else:\n        # Log the operation\n        logger.debug(f'returning HTTP codes: {codes}')\n\n        # If description_only is False, return a dictionary where each key is an\n        # HTTP error code from the input list and each value is the\n        # corresponding dictionary from the ALL_HTTP_CODES dictionary.\n        return {code: ALL_HTTP_CODES[code] for code in codes if code in ALL_HTTP_CODES}\n</code></pre>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions._all_codes","title":"<code>dsg_lib.fastapi_functions._all_codes</code>","text":"<p>This module contains a dictionary mapping HTTP status codes to their descriptions, extended descriptions, and links to their documentation.</p> <p>Each key in this dictionary is an HTTP status code, and each value is another dictionary with keys 'description', 'extended_description', and 'link'.</p> <p>The 'description' key maps to a brief string that describes the HTTP status code. The 'extended_description' key maps to a more detailed explanation of the status code. The 'link' key maps to a string that is a link to the documentation for the HTTP status code.</p> Example <pre><code>from dsg_lib.fastapi_functions.http_codes import ALL_HTTP_CODES\n\n# Get the dictionary for HTTP status code 200\nstatus_200 = ALL_HTTP_CODES[200]\nprint(status_200)\n# Output: {'description': 'OK', 'extended_description': 'The request has succeeded.', 'link': 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200'}\n\n# Get the description for HTTP status code 404\ndescription_404 = ALL_HTTP_CODES[404]['description']\nprint(description_404)  # Output: 'Not Found'\n\n# Get the extended description for HTTP status code 200\nextended_description_200 = ALL_HTTP_CODES[200]['extended_description']\nprint(extended_description_200)  # Output: 'The request has succeeded.'\n\n# Get the link to the documentation for HTTP status code 500\nlink_500 = ALL_HTTP_CODES[500]['link']\nprint(link_500)  # Output: 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'\n</code></pre>"},{"location":"fastapi/system_health_endpoints/","title":"Reference","text":""},{"location":"fastapi/system_health_endpoints/#dsg_lib.fastapi_functions.system_health_endpoints","title":"<code>dsg_lib.fastapi_functions.system_health_endpoints</code>","text":"<p>This module provides a configurable health endpoint for a FastAPI application. It includes the following routes:</p> <ul> <li> <p><code>/api/health/status</code>: Returns the status of the application. If the   application is running, it will return <code>{\"status\": \"UP\"}</code>. This endpoint can   be enabled or disabled using the configuration.</p> </li> <li> <p><code>/api/health/uptime</code>: Returns the uptime of the application in a dictionary   with the keys \"Days\", \"Hours\", \"Minutes\", and \"Seconds\". The uptime is   calculated from the time the application was started. This endpoint can be   enabled or disabled using the configuration.</p> </li> <li> <p><code>/api/health/heapdump</code>: Returns a heap dump of the application. The heap dump   is a list of dictionaries, each representing a line of code. Each dictionary   includes the filename, line number, size of memory consumed, and the number of   times the line is referenced. This endpoint can be enabled or disabled using   the configuration.</p> </li> </ul> <p>The module uses the <code>FastAPI</code>, <code>time</code>, <code>tracemalloc</code>, <code>loguru</code>, <code>packaging</code>, and <code>dsg_lib.fastapi.http_codes</code> modules.</p> <p>Functions:</p> Name Description <code>create_health_router</code> <p>dict) -&gt; FastAPI.APIRouter: Creates a FastAPI router with health endpoints based on the provided configuration.</p> Example <pre><code>from FastAPI import FastAPI\nfrom dsg_lib.fastapi_functions import\nsystem_health_endpoints\n\napp = FastAPI()\n\n# User configuration\nconfig = {\n    \"enable_status_endpoint\": True,\n    \"enable_uptime_endpoint\": False,\n    \"enable_heapdump_endpoint\": True,\n}\n\n# Health router\nhealth_router =\nsystem_health_endpoints.create_health_router(config)\napp.include_router(health_router, prefix=\"/api/health\",\ntags=[\"system-health\"])\n\n# Get the status of the application\nresponse = client.get(\"/api/health/status\")\nprint(response.json())  # {\"status\": \"UP\"}\n\n# Get the uptime of the application response =\nclient.get(\"/api/health/uptime\")\nprint(response.json())\n# {\"uptime\": {\"Days\": 0, \"Hours\": 0, \"Minutes\": 1, \"Seconds\": 42.17}}\n\n# Get the heap dump of the application response =\nclient.get(\"/api/health/heapdump\")\nprint(response.json())\n# {\"memory_use\":{\"current\": \"123456\", \"peak\": \"789012\"}, \"heap_dump\": [{\"filename\": \"main.py\", \"lineno\": 10, \"size\": 1234, \"count\": 1}, ...]}\n</code></pre>"},{"location":"fastapi/system_health_endpoints/#dsg_lib.fastapi_functions.system_health_endpoints.create_health_router","title":"<code>create_health_router(config)</code>","text":"<p>Create a health router with the following endpoints:</p> <ul> <li> <p><code>/status</code>: Returns the status of the application. This endpoint can be   enabled or disabled using the <code>enable_status_endpoint</code> key in the   configuration.</p> </li> <li> <p><code>/uptime</code>: Returns the uptime of the application. This endpoint can be   enabled or disabled using the <code>enable_uptime_endpoint</code> key in the   configuration.</p> </li> <li> <p><code>/heapdump</code>: Returns a heap dump of the application. This endpoint can be   enabled or disabled using the <code>enable_heapdump_endpoint</code> key in the   configuration.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary with the configuration for the endpoints.</p> required <p>Returns:</p> Name Type Description <code>APIRouter</code> <p>A FastAPI router with the configured endpoints.</p> Example <pre><code>from FastAPI import FastAPI\nfrom dsg_lib.fastapi_functions import\nsystem_health_endpoints\n\napp = FastAPI()\n\n# User configuration\nconfig = {\n    \"enable_status_endpoint\": True,\n    \"enable_uptime_endpoint\": False,\n    \"enable_heapdump_endpoint\": True,\n}\n\n# Health router\nhealth_router =\nsystem_health_endpoints.create_health_router(config)\napp.include_router(health_router, prefix=\"/api/health\",\ntags=[\"system-health\"])\n\n# Get the status of the application\nresponse = client.get(\"/api/health/status\")\nprint(response.json())  # {\"status\": \"UP\"}\n\n# Get the uptime of the application response =\nclient.get(\"/api/health/uptime\")\nprint(response.json())\n# {\"uptime\": {\"Days\": 0, \"Hours\": 0, \"Minutes\": 1, \"Seconds\": 42.17}}\n\n# Get the heap dump of the application response =\nclient.get(\"/api/health/heapdump\")\nprint(response.json())\n# {\"memory_use\":{\"current\": \"123456\", \"peak\": \"789012\"}, \"heap_dump\": [{\"filename\": \"main.py\", \"lineno\": 10, \"size\": 1234, \"count\": 1}, ...]}\n</code></pre> Source code in <code>dsg_lib/fastapi_functions/system_health_endpoints.py</code> <pre><code>def create_health_router(config: dict):\n    \"\"\"\n    Create a health router with the following endpoints:\n\n    - `/status`: Returns the status of the application. This endpoint can be\n      enabled or disabled using the `enable_status_endpoint` key in the\n      configuration.\n\n    - `/uptime`: Returns the uptime of the application. This endpoint can be\n      enabled or disabled using the `enable_uptime_endpoint` key in the\n      configuration.\n\n    - `/heapdump`: Returns a heap dump of the application. This endpoint can be\n      enabled or disabled using the `enable_heapdump_endpoint` key in the\n      configuration.\n\n    Args:\n        config (dict): A dictionary with the configuration for the endpoints.\n        Each key should be the name of an endpoint (e.g.,\n        `enable_status_endpoint`) and the value should be a boolean indicating\n        whether the endpoint is enabled or not.\n\n    Returns:\n        APIRouter: A FastAPI router with the configured endpoints.\n\n    Example:\n        ```python\n        from FastAPI import FastAPI\n        from dsg_lib.fastapi_functions import\n        system_health_endpoints\n\n        app = FastAPI()\n\n        # User configuration\n        config = {\n            \"enable_status_endpoint\": True,\n            \"enable_uptime_endpoint\": False,\n            \"enable_heapdump_endpoint\": True,\n        }\n\n        # Health router\n        health_router =\n        system_health_endpoints.create_health_router(config)\n        app.include_router(health_router, prefix=\"/api/health\",\n        tags=[\"system-health\"])\n\n        # Get the status of the application\n        response = client.get(\"/api/health/status\")\n        print(response.json())  # {\"status\": \"UP\"}\n\n        # Get the uptime of the application response =\n        client.get(\"/api/health/uptime\")\n        print(response.json())\n        # {\"uptime\": {\"Days\": 0, \"Hours\": 0, \"Minutes\": 1, \"Seconds\": 42.17}}\n\n        # Get the heap dump of the application response =\n        client.get(\"/api/health/heapdump\")\n        print(response.json())\n        # {\"memory_use\":{\"current\": \"123456\", \"peak\": \"789012\"}, \"heap_dump\": [{\"filename\": \"main.py\", \"lineno\": 10, \"size\": 1234, \"count\": 1}, ...]}\n\n        ```\n    \"\"\"\n    # Try to import FastAPI, handle ImportError if FastAPI is not installed\n    try:\n        import fastapi\n        from fastapi import APIRouter, HTTPException, status\n        from fastapi.responses import ORJSONResponse\n    except ImportError:  # pragma: no cover\n        APIRouter = HTTPException = status = ORJSONResponse = fastapi = None  # pragma: no cover\n\n    # Check FastAPI version\n    min_version = '0.100.0'  # replace with your minimum required version\n    if fastapi is not None and packaging_version.parse(\n        fastapi.__version__\n    ) &lt; packaging_version.parse(min_version):\n        raise ImportError(\n            f'FastAPI version &gt;= {min_version} required, run `pip install --upgrade fastapi`'\n        )  # pragma: no cover\n\n    # Store the start time of the application\n    app_start_time = time.time()\n\n    # TODO: determine method to shutdown/restart python application\n\n    status_response = generate_code_dict([400, 405, 500], description_only=False)\n\n    tracemalloc.start()\n    # Create a new router\n    router = APIRouter()\n\n    # Check if the status endpoint is enabled in the configuration\n    if config.get('enable_status_endpoint', True):\n        # Define the status endpoint\n        @router.get(\n            '/status',\n            tags=['system-health'],\n            status_code=status.HTTP_200_OK,\n            response_class=ORJSONResponse,\n            responses=status_response,\n        )\n        async def health_status():\n            \"\"\"\n            Returns the status of the application.\n\n            This endpoint returns a dictionary with the status of the\n            application. If the application is running, it will return\n            `{\"status\": \"UP\"}`.\n\n            Returns:\n                dict: A dictionary with the status of the application. The\n                dictionary has a single key, \"status\", and the value is a string\n                that indicates the status of the application.\n\n            Raises:\n                HTTPException: If there is an error while getting the status of\n                the application.\n\n            Example:\n                ```python\n                from FastAPI import FastAPI\n                from dsg_lib.fastapi_functions import\n                system_health_endpoints\n\n                app = FastAPI()\n\n                # User configuration\n                config = {\n                    \"enable_status_endpoint\": True,\n                    \"enable_uptime_endpoint\": False,\n                    \"enable_heapdump_endpoint\": True,\n                }\n\n                # Health router\n                health_router =\n                system_health_endpoints.create_health_router(config)\n                app.include_router(health_router, prefix=\"/api/health\",\n                tags=[\"system-health\"])\n\n                # Get the status of the application\n                response = client.get(\"/api/health/status\")\n                print(response.json())  # {\"status\": \"UP\"}\n            ```\n            \"\"\"\n            # Log the status request\n            logger.info('Health status of up returned')\n            # Return a dictionary with the status of the application\n            return {'status': 'UP'}\n\n    # Check if the uptime endpoint is enabled in the configuration\n    if config.get('enable_uptime_endpoint', True):\n        # Define the uptime endpoint\n        @router.get('/uptime', response_class=ORJSONResponse, responses=status_response)\n        async def get_uptime():\n            \"\"\"\n            Calculate and return the uptime of the application.\n\n            This endpoint returns a dictionary with the uptime of the\n            application. The uptime is calculated from the time the application\n            was started and is returned in a dictionary with the keys \"Days\",\n            \"Hours\", \"Minutes\", and \"Seconds\".\n\n            Returns:\n                dict: A dictionary with the uptime of the application. The\n                dictionary has keys for \"Days\", \"Hours\", \"Minutes\", and\n                \"Seconds\".\n\n            Raises:\n                HTTPException: If there is an error while calculating the uptime\n                of the application.\n\n            Example:\n                ```python\n                from FastAPI import FastAPI\n                from dsg_lib.fastapi_functions import\n                system_health_endpoints\n\n                app = FastAPI()\n\n                # User configuration\n                config = {\n                    \"enable_status_endpoint\": True,\n                    \"enable_uptime_endpoint\": False,\n                    \"enable_heapdump_endpoint\": True,\n                }\n\n                # Health router\n                health_router =\n                system_health_endpoints.create_health_router(config)\n                app.include_router(health_router, prefix=\"/api/health\",\n                tags=[\"system-health\"])\n\n                # Get the uptime of the application response =\n                client.get(\"/api/health/uptime\")\n                print(response.json())\n                # {\"uptime\": {\"Days\": 0, \"Hours\": 0, \"Minutes\": 1, \"Seconds\": 42.17}}\n\n                ```\n            \"\"\"\n            # Calculate the total uptime in seconds This is done by subtracting\n            # the time when the application started from the current time\n            uptime_seconds = time.time() - app_start_time\n\n            # Convert the uptime from seconds to days, hours, minutes, and\n            # seconds\n            days, rem = divmod(uptime_seconds, 86400)\n            hours, rem = divmod(rem, 3600)\n            minutes, seconds = divmod(rem, 60)\n\n            # Log the uptime\n            logger.info(\n                f'Uptime: {int(days)} days, {int(hours)} hours, {int(minutes)} minutes, {round(seconds, 2)} seconds'\n            )\n\n            # Return a dictionary with the uptime The dictionary has keys for\n            # days, hours, minutes, and seconds\n            return {\n                'uptime': {\n                    'Days': int(days),\n                    'Hours': int(hours),\n                    'Minutes': int(minutes),\n                    'Seconds': round(seconds, 2),\n                }\n            }\n\n    if config.get('enable_heapdump_endpoint', True):\n\n        @router.get('/heapdump', response_class=ORJSONResponse, responses=status_response)\n        async def get_heapdump():\n            \"\"\"\n            Returns a heap dump of the application.\n\n            This endpoint returns a snapshot of the current memory usage of the\n            application. The heap dump is a list of dictionaries, each\n            representing a line of code. Each dictionary includes the filename,\n            line number, size of memory consumed, and the number of times the\n            line is referenced.\n\n            Returns:\n                dict: A dictionary with the current and peak memory usage, and\n                the heap dump of the application. The dictionary has two keys,\n                \"memory_use\" and \"heap_dump\". The \"memory_use\" key contains a\n                dictionary with the current and peak memory usage. The\n                \"heap_dump\" key contains a list of dictionaries, each\n                representing a line of code.\n\n            Raises:\n                HTTPException: If there is an error while getting the heap dump\n                of the application.\n\n            Example:\n                ```python\n                from FastAPI import FastAPI\n                from dsg_lib.fastapi_functions import\n                system_health_endpoints\n\n                app = FastAPI()\n\n                # User configuration\n                config = {\n                    \"enable_status_endpoint\": True,\n                    \"enable_uptime_endpoint\": False,\n                    \"enable_heapdump_endpoint\": True,\n                }\n\n                # Health router\n                health_router =\n                system_health_endpoints.create_health_router(config)\n                app.include_router(health_router, prefix=\"/api/health\",\n                tags=[\"system-health\"])\n\n                # Get the heap dump of the application response =\n                client.get(\"/api/health/heapdump\")\n                print(response.json())\n                # {\"memory_use\":{\"current\": \"123456\", \"peak\": \"789012\"}, \"heap_dump\": [{\"filename\": \"main.py\", \"lineno\": 10, \"size\": 1234, \"count\": 1}, ...]}\n\n                ```\n            \"\"\"\n\n            try:\n                # Take a snapshot of the current memory usage\n                snapshot = tracemalloc.take_snapshot()\n                # Get the top 10 lines consuming memory\n                top_stats = snapshot.statistics('traceback')\n\n                heap_dump = []\n                for stat in top_stats[:10]:\n                    # Get the first frame from the traceback\n                    frame = stat.traceback[0]\n                    # Add the frame to the heap dump\n                    heap_dump.append(\n                        {\n                            'filename': frame.filename,\n                            'lineno': frame.lineno,\n                            'size': stat.size,\n                            'count': stat.count,\n                        }\n                    )\n\n                logger.debug(f'Heap dump returned {heap_dump}')\n                memory_use = tracemalloc.get_traced_memory()\n                return {\n                    'memory_use': {\n                        'current': f'{memory_use[0]:,}',\n                        'peak': f'{memory_use[1]:,}',\n                    },\n                    'heap_dump': heap_dump,\n                }\n            except Exception as ex:\n                logger.error(f'Error in get_heapdump: {ex}')\n                raise HTTPException(status_code=500, detail=f'Error in get_heapdump: {ex}')\n\n    return router\n</code></pre>"},{"location":"recipes/asyncDatabase/","title":"Examples","text":"<p>Here are a few examples of how to use the database functions</p>"},{"location":"recipes/asyncDatabase/#asyncio-script-example","title":"Asyncio Script Example","text":"<p>Example of how to use in a script</p> <pre><code>import asyncio\nfrom sqlalchemy import select\nfrom dsg_lib.async_database_functions import database_config, async_database, database_operations\n\n# Configuration\nconfig = {\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    \"pool_recycle\": 3600,\n}\n\n# Create a DBConfig instance\ndb_config = database_config.DBConfig(config)\n\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n\n# User class\nclass User(async_db.Base):\n    __tablename__ = \"users\"\n    first_name = Column(String, unique=False, index=True)\n    last_name = Column(String, unique=False, index=True)\n    email = Column(String, unique=True, index=True, nullable=True)\n\n# Async function to get all users\nasync def get_all_users():\n    # Create a select query\n    query = select(User)\n\n    # Execute the query and fetch all results\n    users = await db_ops.read_query(query)\n\n    # Print the users\n    for user in users:\n        print(f\"User: {user.first_name} {user.last_name}, Email: {user.email}\")\n\n# Run the async function\nasyncio.run(get_all_users())\n</code></pre>"},{"location":"recipes/asyncDatabase/#fastapi-example","title":"FastAPI Example","text":"<pre><code># -*- coding: utf-8 -*-\n\nfrom contextlib import asynccontextmanager\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import RedirectResponse\nfrom loguru import logger\nfrom tqdm import tqdm\n\nfrom dsg_lib import logging_config\n\nlogging_config.config_log(\n    logging_level=\"Debug\", log_serializer=False, log_name=\"log.log\"\n)\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    logger.info(\"starting up\")\n    # Create the tables in the database\n    await async_db.create_tables()\n\n    create_users = True\n    if create_users:\n        await create_a_bunch_of_users(single_entry=23, many_entries=100)\n    yield\n    logger.info(\"shutting down\")\n\n\n# Create an instance of the FastAPI class\napp = FastAPI(\n    title=\"FastAPI Example\",  # The title of the API\n    description=\"This is an example of a FastAPI application using the DevSetGo Toolkit.\",  # A brief description of the API\n    version=\"0.1.0\",  # The version of the API\n    docs_url=\"/docs\",  # The URL where the API documentation will be served\n    redoc_url=\"/redoc\",  # The URL where the ReDoc documentation will be served\n    openapi_url=\"/openapi.json\",  # The URL where the OpenAPI schema will be served\n    debug=True,  # Enable debug mode\n    middleware=[],  # A list of middleware to include in the application\n    routes=[],  # A list of routes to include in the application\n    lifespan=lifespan,\n)\n\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"\n    Root endpoint of API\n    Returns:\n        Redrects to openapi document\n    \"\"\"\n    # redirect to openapi docs\n    logger.info(\"Redirecting to OpenAPI docs\")\n    response = RedirectResponse(url=\"/docs\")\n    return response\n\nfrom sqlalchemy import Column, Delete, Select, String, Update\n\nfrom dsg_lib import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n)\n\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n\n\n# User class inherits from SchemaBase and async_db.Base\n# This class represents the User table in the database\nclass User(base_schema.SchemaBase, async_db.Base):\n    __tablename__ = \"users\"  # Name of the table in the database\n\n    # Define the columns of the table\n    first_name = Column(String, unique=False, index=True)  # First name of the user\n    last_name = Column(String, unique=False, index=True)  # Last name of the user\n    email = Column(\n        String, unique=True, index=True, nullable=True\n    )  # Email of the user, must be unique\n\nasync def create_a_bunch_of_users(single_entry=0, many_entries=0):\n    logger.info(f\"single_entry: {single_entry}\")\n    await async_db.create_tables()\n    # Create a list to hold the user data\n\n    # Create a loop to generate user data\n\n    for i in tqdm(range(single_entry), desc=\"executing one\"):\n        value = secrets.token_hex(16)\n        user = User(\n            first_name=f\"First{value}\",\n            last_name=f\"Last{value}\",\n            email=f\"user{value}@example.com\",\n        )\n        logger.info(f\"created_users: {user}\")\n        await db_ops.create_one(user)\n\n    users = []\n    # Create a loop to generate user data\n    for i in tqdm(range(many_entries), desc=\"executing many\"):\n        value_one = secrets.token_hex(4)\n        value_two = secrets.token_hex(8)\n        user = User(\n            first_name=f\"First{value_one}{i}{value_two}\",\n            last_name=f\"Last{value_one}{i}{value_two}\",\n            email=f\"user{value_one}{i}{value_two}@example.com\",\n        )\n        logger.info(f\"created_users: {user.first_name}\")\n        users.append(user)\n\n    # Use db_ops to add the users to the database\n    await db_ops.create_many(users)\n\n\n@app.get(\"/database/get-count\")\nasync def get_count():\n    count = await db_ops.count_query(Select(User))\n    return {\"count\": count}\n\n\n# endpoint to get list of user\n@app.get(\"/database/get-all\")\nasync def get_all(offset: int = 0, limit: int = 100):\n    records = await db_ops.read_query(Select(User).offset(offset).limit(limit))\n    return {\"records\": records}\n\n\n@app.get(\"/database/get-primary-key\")\nasync def table_primary_key():\n    pk = await db_ops.get_primary_keys(User)\n    return {\"pk\": pk}\n\n\n@app.get(\"/database/get-column-details\")\nasync def table_column_details():\n    columns = await db_ops.get_columns_details(User)\n    return {\"columns\": columns}\n\n\n@app.get(\"/database/get-tables\")\nasync def table_table_details():\n    tables = await db_ops.get_table_names()\n    return {\"table_names\": tables}\n\n\n@app.get(\"/database/get-one-record\")\nasync def get_one_record(record_id: str):\n    record = await db_ops.get_one_record(Select(User).where(User.pkid == record_id))\n    return {\"record\": record}\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"127.0.0.1\", port=5000)\n</code></pre>"},{"location":"recipes/fastapi/","title":"Full Example of FastAPI with Aync Database and Endpoints","text":"<p>You can find this in the examples folder of the repository.</p>"},{"location":"recipes/fastapi/#install-dependencies","title":"Install dependencies","text":"<pre><code>pip install dsg_lib[all] tqdm\n</code></pre>"},{"location":"recipes/fastapi/#make-app","title":"Make App","text":"<p>Copy the fastapi code below after installing. (assumption is main.py)</p> <pre><code># -*- coding: utf-8 -*-\n\nimport datetime\nimport secrets\nimport time\nfrom contextlib import asynccontextmanager\n\nfrom fastapi import Body, FastAPI, Query\nfrom fastapi.responses import RedirectResponse\nfrom loguru import logger\nfrom pydantic import BaseModel, EmailStr\nfrom sqlalchemy import Column, ForeignKey, Select, String\nfrom sqlalchemy.orm import relationship\nfrom tqdm import tqdm\n\nfrom dsg_lib.async_database_functions import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n)\nfrom dsg_lib.common_functions import logging_config\n\nlogging_config.config_log(\n    logging_level=\"INFO\", log_serializer=False, log_name=\"log.log\"\n)\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    logger.info(\"starting up\")\n    # Create the tables in the database\n    await async_db.create_tables()\n\n    create_users = True\n    if create_users:\n        await create_a_bunch_of_users(single_entry=24, many_entries=2000)\n    yield\n    logger.info(\"shutting down\")\n\n\n# Create an instance of the FastAPI class\napp = FastAPI(\n    title=\"FastAPI Example\",  # The title of the API\n    description=\"This is an example of a FastAPI application using the DevSetGo Toolkit.\",  # A brief description of the API\n    version=\"0.1.0\",  # The version of the API\n    docs_url=\"/docs\",  # The URL where the API documentation will be served\n    redoc_url=\"/redoc\",  # The URL where the ReDoc documentation will be served\n    openapi_url=\"/openapi.json\",  # The URL where the OpenAPI schema will be served\n    debug=True,  # Enable debug mode\n    middleware=[],  # A list of middleware to include in the application\n    routes=[],  # A list of routes to include in the application\n    lifespan=lifespan,  # this is the replacement for the startup and shutdown events\n)\n\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"\n    Root endpoint of API\n    Returns:\n        Redrects to openapi document\n    \"\"\"\n    # redirect to openapi docs\n    logger.info(\"Redirecting to OpenAPI docs\")\n    response = RedirectResponse(url=\"/docs\")\n    return response\n\n\nfrom dsg_lib.fastapi_functions import (  # , system_tools_endpoints\n    system_health_endpoints,\n)\n\nconfig_health = {\n    \"enable_status_endpoint\": True,\n    \"enable_uptime_endpoint\": True,\n    \"enable_heapdump_endpoint\": True,\n}\napp.include_router(\n    system_health_endpoints.create_health_router(config=config_health),\n    prefix=\"/api/health\",\n    tags=[\"system-health\"],\n)\n\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n\n\nclass User(base_schema.SchemaBase, async_db.Base):\n    \"\"\"\n    User table storing user details like first name, last name, and email\n    \"\"\"\n\n    __tablename__ = \"users\"\n    __table_args__ = {\n        \"comment\": \"User table storing user details like first name, last name, and email\"\n    }\n\n    first_name = Column(String(50), unique=False, index=True)  # First name of the user\n    last_name = Column(String(50), unique=False, index=True)  # Last name of the user\n    email = Column(\n        String(200), unique=True, index=True, nullable=True\n    )  # Email of the user, must be unique\n    addresses = relationship(\n        \"Address\", order_by=\"Address.pkid\", back_populates=\"user\"\n    )  # Relationship to the Address class\n\n\nclass Address(base_schema.SchemaBase, async_db.Base):\n    \"\"\"\n    Address table storing address details like street, city, and zip code\n    \"\"\"\n\n    __tablename__ = \"addresses\"\n    __table_args__ = {\n        \"comment\": \"Address table storing address details like street, city, and zip code\"\n    }\n\n    street = Column(String(200), unique=False, index=True)  # Street of the address\n    city = Column(String(200), unique=False, index=True)  # City of the address\n    zip = Column(String(50), unique=False, index=True)  # Zip code of the address\n    user_id = Column(\n        String(36), ForeignKey(\"users.pkid\")\n    )  # Foreign key to the User table\n    user = relationship(\n        \"User\", back_populates=\"addresses\"\n    )  # Relationship to the User class\n\n\nasync def create_a_bunch_of_users(single_entry=0, many_entries=0):\n    logger.info(f\"single_entry: {single_entry}\")\n    await async_db.create_tables()\n    # Create a list to hold the user data\n\n    # Create a loop to generate user data\n\n    for i in tqdm(range(single_entry), desc=\"executing one\"):\n        value = secrets.token_hex(16)\n        user = User(\n            first_name=f\"First{value}\",\n            last_name=f\"Last{value}\",\n            email=f\"user{value}@example.com\",\n        )\n        logger.info(f\"created_users: {user}\")\n        await db_ops.create_one(user)\n\n    users = []\n    # Create a loop to generate user data\n    for i in tqdm(range(many_entries), desc=\"executing many\"):\n        value_one = secrets.token_hex(4)\n        value_two = secrets.token_hex(8)\n        user = User(\n            first_name=f\"First{value_one}{i}{value_two}\",\n            last_name=f\"Last{value_one}{i}{value_two}\",\n            email=f\"user{value_one}{i}{value_two}@example.com\",\n        )\n        logger.info(f\"created_users: {user.first_name}\")\n        users.append(user)\n\n    # Use db_ops to add the users to the database\n    await db_ops.create_many(users)\n\n\n@app.get(\"/database/get-primary-key\", tags=[\"Database Examples\"])\nasync def table_primary_key():\n    logger.info(\"Getting primary key of User table\")\n    pk = await db_ops.get_primary_keys(User)\n    logger.info(f\"Primary key of User table: {pk}\")\n    return {\"pk\": pk}\n\n\n@app.get(\"/database/get-column-details\", tags=[\"Database Examples\"])\nasync def table_column_details():\n    logger.info(\"Getting column details of User table\")\n    columns = await db_ops.get_columns_details(User)\n    logger.info(f\"Column details of User table: {columns}\")\n    return {\"columns\": columns}\n\n\n@app.get(\"/database/get-tables\", tags=[\"Database Examples\"])\nasync def table_table_details():\n    logger.info(\"Getting table names\")\n    tables = await db_ops.get_table_names()\n    logger.info(f\"Table names: {tables}\")\n    return {\"table_names\": tables}\n\n\n@app.get(\"/database/get-count\", tags=[\"Database Examples\"])\nasync def get_count():\n    logger.info(\"Getting count of users\")\n    count = await db_ops.count_query(Select(User))\n    logger.info(f\"Count of users: {count}\")\n    return {\"count\": count}\n\n\n@app.get(\"/database/get-all\", tags=[\"Database Examples\"])\nasync def get_all(offset: int = 0, limit: int = Query(100, le=100000, ge=1)):\n    logger.info(f\"Getting all users with offset {offset} and limit {limit}\")\n    records = await db_ops.read_query(Select(User).offset(offset).limit(limit))\n    logger.info(f\"Retrieved {len(records)} users\")\n    return {\"records\": records}\n\n\n@app.get(\"/database/get-one-record\", tags=[\"Database Examples\"])\nasync def read_one_record(record_id: str):\n    logger.info(f\"Reading one record with id {record_id}\")\n    record = await db_ops.read_one_record(Select(User).where(User.pkid == record_id))\n    logger.info(f\"Record with id {record_id}: {record}\")\n    return record\n\n\nclass UserBase(BaseModel):\n    first_name: str\n    last_name: str\n    email: EmailStr\n\n\nclass UserCreate(UserBase):\n    pass\n\n\n@app.post(\"/database/create-one-record\", status_code=201, tags=[\"Database Examples\"])\nasync def create_one_record(new_user: UserCreate):\n    logger.info(f\"Creating one record: {new_user}\")\n    user = User(**new_user.dict())\n    record = await db_ops.create_one(user)\n    logger.info(f\"Created record: {record}\")\n    return record\n\n\n@app.post(\"/database/create-many-records\", status_code=201, tags=[\"Database Examples\"])\nasync def create_many_records(number_of_users: int = Query(100, le=1000, ge=1)):\n    logger.info(f\"Creating {number_of_users} records\")\n    t0 = time.time()\n    users = []\n    # Create a loop to generate user data\n    for i in tqdm(range(number_of_users), desc=\"executing many\"):\n        value_one = secrets.token_hex(4)\n        value_two = secrets.token_hex(8)\n        user = User(\n            first_name=f\"First{value_one}{i}{value_two}\",\n            last_name=f\"Last{value_one}{i}{value_two}\",\n            email=f\"user{value_one}{i}{value_two}@example.com\",\n        )\n        logger.info(f\"Created user: {user.first_name}\")\n        users.append(user)\n\n    # Use db_ops to add the users to the database\n    await db_ops.create_many(users)\n    t1 = time.time()\n    process_time = format(t1 - t0, \".4f\")\n    logger.info(f\"Created {number_of_users} records in {process_time} seconds\")\n    return {\"number_of_users\": number_of_users, \"process_time\": process_time}\n\n\n@app.put(\"/database/update-one-record\", status_code=200, tags=[\"Database Examples\"])\nasync def update_one_record(\n    id: str = Body(\n        ...,\n        description=\"UUID to update\",\n        examples=[\"6087cce8-0bdd-48c2-ba96-7d557dae843e\"],\n    ),\n    first_name: str = Body(..., examples=[\"Agent\"]),\n    last_name: str = Body(..., examples=[\"Smith\"]),\n    email: str = Body(..., examples=[\"jim@something.com\"]),\n):\n    logger.info(f\"Updating one record with id {id}\")\n    # adding date_updated to new_values as it is not supported in sqlite \\\n    # and other database may not either.\n    new_values = {\n        \"first_name\": first_name,\n        \"last_name\": last_name,\n        \"email\": email,\n        \"date_updated\": datetime.datetime.now(datetime.timezone.utc),\n    }\n    record = await db_ops.update_one(table=User, record_id=id, new_values=new_values)\n    logger.info(f\"Updated record with id {id}\")\n    return record\n\n\n@app.delete(\"/database/delete-one-record\", status_code=200, tags=[\"Database Examples\"])\nasync def delete_one_record(record_id: str = Body(...)):\n    logger.info(f\"Deleting one record with id {record_id}\")\n    record = await db_ops.delete_one(table=User, record_id=record_id)\n    logger.info(f\"Deleted record with id {record_id}\")\n    return record\n\n\n@app.delete(\n    \"/database/delete-many-records-aka-this-is-a-bad-idea\",\n    status_code=201,\n    tags=[\"Database Examples\"],\n)\nasync def delete_many_records(\n    id_values: list = Body(...), id_column_name: str = \"pkid\"\n):\n    logger.info(f\"Deleting many records with ids {id_values}\")\n    record = await db_ops.delete_many(\n        table=User, id_column_name=\"pkid\", id_values=id_values\n    )\n    logger.info(f\"Deleted records with ids {id_values}\")\n    return record\n\n\n@app.get(\n    \"/database/get-list-of-records-to-paste-into-delete-many-records\",\n    tags=[\"Database Examples\"],\n)\nasync def read_list_of_records(\n    offset: int = Query(0, le=1000, ge=0), limit: int = Query(100, le=10000, ge=1)\n):\n    logger.info(f\"Reading list of records with offset {offset} and limit {limit}\")\n    records = await db_ops.read_query(Select(User), offset=offset, limit=limit)\n    records_list = []\n    for record in records:\n        records_list.append(record.pkid)\n    logger.info(f\"Read list of records: {records_list}\")\n    return records_list\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"127.0.0.1\", port=5000)\n</code></pre>"},{"location":"recipes/fastapi/#run-code","title":"Run Code","text":"<p>In the console (linux) run the code below. Open browser to http://127.0.0.1:5000 to see app.</p> <pre><code>python3 main.py\n</code></pre>"},{"location":"recipes/loggingExample/","title":"Logging Example","text":"<pre><code># -*- coding: utf-8 -*-\nimport logging\nimport secrets\nfrom uuid import uuid4\n\nfrom loguru import logger\nfrom tqdm import tqdm\n\nfrom dsg_lib.common_functions import logging_config\n\nlogging_config.config_log(\n    logging_directory=\"log\",\n    # or None and defaults to logging\n    # log_name=\"log.json\",\n    # or None and defaults to \"log.log\"\n    logging_level=\"debug\",\n    # or \"info\" or \"debug\" or \"warning\" or \"error\" or \"critical\"\n    # or None and defaults to \"info\"\n    log_rotation=\"10 MB\",\n    # or None and default is 10 MB\n    log_retention=\"1 Day\",\n    # or None and defaults to \"14 Days\"\n    log_backtrace=True,\n    # or None and defaults to False\n    app_name=\"my_app\",\n    # app name is used to identify the application\n    # this is an optional field\n    enable_trace_id=True,\n    # service id is used to identify the service\n    # this is an optional field\n    append_app_name=True,\n    # append app name to log file name defaults to false\n    append_trace_id=True,\n    # append app name and service name to log file name defaults to false\n)\n\n# after configuring logging\n# user loguru to log messages\nlogger.debug(\"This is a debug message\")\nlogger.info(\"This is an info message\")\nlogger.error(\"This is an error message\")\nlogger.warning(\"This is a warning message\")\nlogger.critical(\"This is a critical message\")\n\n# will intercept all standard logging messages also\nlogging.debug(\"This is a debug message\")\nlogging.info(\"This is an info message\")\nlogging.error(\"This is an error message\")\nlogging.warning(\"This is a warning message\")\nlogging.critical(\"This is a critical message\")\n\n\ndef div_zero(x, y):\n    try:\n        return x / y\n    except ZeroDivisionError as e:\n        logger.error(f\"{e}\")\n        logging.error(f\"{e}\")\n\n\n@logger.catch\ndef div_zero_two(x, y):\n    return x / y\n\n\na = div_zero(x=1, y=0)\nb = div_zero_two(x=1, y=0)\n\nfor _ in tqdm(range(5), ascii=True):\n    # log a lot of data\n    logging.debug(f\"Lets make this a big message {secrets.token_urlsafe(32)}\")\n</code></pre>"},{"location":"recipes/patterns/","title":"Patterns","text":"<pre><code># -*- coding: utf-8 -*-\nimport pprint\nfrom random import randint\n\nfrom dsg_lib.common_functions.patterns import pattern_between_two_char\n\nASCII_LIST = [\n    \" \",\n    \"!\",\n    '\"\"',\n    \"#\",\n    \"$\",\n    \"%\",\n    \"&amp;\",\n    \"'\",\n    \"(\",\n    \")\",\n    \"*\",\n    \"+\",\n    \",\",\n    \"-\",\n    \".\",\n    \"/\",\n    \"0\",\n    \"1\",\n    \"2\",\n    \"3\",\n    \"4\",\n    \"5\",\n    \"6\",\n    \"7\",\n    \"8\",\n    \"9\",\n    \":\",\n    \";\",\n    \"&lt;\",\n    \"=\",\n    \"&gt;\",\n    \"?\",\n    \"@\",\n    \"A\",\n    \"B\",\n    \"C\",\n    \"D\",\n    \"E\",\n    \"F\",\n    \"G\",\n    \"H\",\n    \"I\",\n    \"J\",\n    \"K\",\n    \"L\",\n    \"M\",\n    \"N\",\n    \"O\",\n    \"P\",\n    \"Q\",\n    \"R\",\n    \"S\",\n    \"T\",\n    \"U\",\n    \"V\",\n    \"W\",\n    \"X\",\n    \"Y\",\n    \"Z\",\n    \"[\",\n    \"\\\\\",\n    \"]\",\n    \"^\",\n    \"_\",\n    \"`\",\n    \"a\",\n    \"b\",\n    \"c\",\n    \"d\",\n    \"e\",\n    \"f\",\n    \"g\",\n    \"h\",\n    \"i\",\n    \"j\",\n    \"k\",\n    \"l\",\n    \"m\",\n    \"n\",\n    \"o\",\n    \"p\",\n    \"q\",\n    \"r\",\n    \"s\",\n    \"t\",\n    \"u\",\n    \"v\",\n    \"w\",\n    \"x\",\n    \"y\",\n    \"z\",\n    \"{\",\n    \"|\",\n    \"}\",\n    \"~\",\n    \"\u20ac\",\n    \"\u201a\",\n    \"\u0192\",\n    \"\u201e\",\n    \"\u2026\",\n    \"\u2020\",\n    \"\u2021\",\n    \"\u02c6\",\n    \"\u2030\",\n    \"\u0160\",\n    \"\u2039\",\n    \"\u0152\",\n    \"\u017d\",\n    \"\u2018\",\n    \"\u2019\",\n    \"\u201c\",\n    \"\u201d\",\n    \"\u2022\",\n    \"\u2013\",\n    \"\u2014\",\n    \"\u02dc\",\n    \"\u2122\",\n    \"\u0161\",\n    \"\u203a\",\n    \"\u0153\",\n    \"\u017e\",\n    \"\u0178\",\n    \"\u00a1\",\n    \"\u00a2\",\n    \"\u00a3\",\n    \"\u00a4\",\n    \"\u00a5\",\n    \"\u00a6\",\n    \"\u00a7\",\n    \"\u00a8\",\n    \"\u00a9\",\n    \"\u00aa\",\n    \"\u00ab\",\n    \"\u00ac\",\n    \"\u00ae\",\n    \"\u00af\",\n    \"\u00b0\",\n    \"\u00b1\",\n    \"\u00b2\",\n    \"\u00b3\",\n    \"\u00b4\",\n    \"\u00b5\",\n    \"\u00b6\",\n    \"\u00b7\",\n    \"\u00b8\",\n    \"\u00b9\",\n    \"\u00ba\",\n    \"\u00bb\",\n    \"\u00bc\",\n    \"\u00bd\",\n    \"\u00be\",\n    \"\u00bf\",\n    \"\u00c0\",\n    \"\u00c1\",\n    \"\u00c2\",\n    \"\u00c3\",\n    \"\u00c4\",\n    \"\u00c5\",\n    \"\u00c6\",\n    \"\u00c7\",\n    \"\u00c8\",\n    \"\u00c9\",\n    \"\u00ca\",\n    \"\u00cb\",\n    \"\u00cc\",\n    \"\u00cd\",\n    \"\u00ce\",\n    \"\u00cf\",\n    \"\u00d0\",\n    \"\u00d1\",\n    \"\u00d2\",\n    \"\u00d3\",\n    \"\u00d4\",\n    \"\u00d5\",\n    \"\u00d6\",\n    \"\u00d7\",\n    \"\u00d8\",\n    \"\u00d9\",\n    \"\u00da\",\n    \"\u00db\",\n    \"\u00dc\",\n    \"\u00dd\",\n    \"\u00de\",\n    \"\u00df\",\n    \"\u00e0\",\n    \"\u00e1\",\n    \"\u00e2\",\n    \"\u00e3\",\n    \"\u00e4\",\n    \"\u00e5\",\n    \"\u00e6\",\n    \"\u00e7\",\n    \"\u00e8\",\n    \"\u00e9\",\n    \"\u00ea\",\n    \"\u00eb\",\n    \"\u00ec\",\n    \"\u00ed\",\n    \"\u00ee\",\n    \"\u00ef\",\n    \"\u00f0\",\n    \"\u00f1\",\n    \"\u00f2\",\n    \"\u00f3\",\n    \"\u00f4\",\n    \"\u00f5\",\n    \"\u00f6\",\n    \"\u00f7\",\n    \"\u00f8\",\n    \"\u00f9\",\n    \"\u00fa\",\n    \"\u00fb\",\n    \"\u00fc\",\n    \"\u00fd\",\n    \"\u00fe\",\n    \"\u00ff\",\n]\n\npp = pprint.PrettyPrinter(indent=4)\n\n\ndef pattern_find(left_char: str, right_char: str, text_block: str):\n    data = pattern_between_two_char(text_block, left_char, right_char)\n    pp.pprint(data)\n\n\ndef run_examples():\n    text_block = \"Lfound oneR Lfound twoR\"\n    left_char = \"L\"\n    right_char = \"R\"\n    pattern_find(left_char=left_char, right_char=right_char, text_block=text_block)\n\n    for _ in range(100):\n        long_input = \"xyz\" * randint(100, 100000)\n        long_text = f\"{long_input}abc&lt;one&gt;123&lt;two&gt;456&lt;three&gt;{long_input}\"\n\n        result = pattern_between_two_char(\n            text_string=long_text, left_characters=\"&lt;\", right_characters=\"&gt;\"\n        )\n        print(result[\"found\"])\n\n\nif __name__ == \"__main__\":\n    run_examples()\n</code></pre>"}]}