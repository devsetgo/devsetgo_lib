{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Support Python Versions</p> <p> </p> <p>CI/CD Pipeline:</p> <p> </p> <p>SonarCloud:</p> <p> </p>"},{"location":"#devsetgo-common-library","title":"DevSetGo Common Library","text":"<p> <code>devsetgo_lib</code> is a versatile library designed to provide common functions for Python applications. Its main goal is to increase reusability and reduce the need to rewrite the same functions across multiple applications. This also allows for quick defect resolution and propagation of fixes across all dependent projects.</p> <p>Read the Full Documentation here.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#common-functions","title":"Common Functions:","text":"<ul> <li> <p>File Operations:</p> <ul> <li>CSV, JSON, and Text File Functions: Create, read, write, and manipulate various file types with ease.</li> <li>Folder Functions: Create and remove directories, list directory contents, and manage file system operations efficiently.</li> </ul> </li> <li> <p>File Moving:     Move files from one directory to another, with an option to compress the file being moved.</p> </li> <li> <p>Logging:     Comprehensive logging setup using the Loguru Library. Provides extensive customization options for log configuration, including log rotation, retention, and formatting. Includes improvements for multiprocessing environments to ensure log messages are handled correctly across multiple processes.</p> </li> <li> <p>Calendar Functions:       Convert between month names and numbers seamlessly.</p> </li> <li> <p>Pattern Matching:       Powerful tools for searching patterns in text using regular expressions.</p> </li> </ul>"},{"location":"#fastapi-endpoints","title":"FastAPI Endpoints:","text":"<ul> <li>Pre-built endpoints for system health checks, status, and uptime monitoring.</li> <li>Functions to generate HTTP response codes easily.</li> </ul>"},{"location":"#async-database","title":"Async Database:","text":"<ul> <li>Configuration and management of asynchronous database sessions.</li> <li>CRUD operations with async support.</li> </ul>"},{"location":"#quick-reference","title":"Quick Reference","text":"<ul> <li>Logging &amp; Config Setup</li> <li>FastAPI Endpoints</li> <li>Calendar &amp; Date Utilities</li> <li>Pattern Matching</li> <li>CSV &amp; JSON Helpers</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To install <code>devsetgo_lib</code>, use pip:</p> <pre><code>pip install devsetgo-lib\n\n# For async database setup with SQLite or PostgreSQL\npip install devsetgo-lib[sqlite]\npip install devsetgo-lib[postgres]\n\n# Experimental support for other databases\npip install devsetgo-lib[oracle]\npip install devsetgo-lib[mssql]\npip install devsetgo-lib[mysql]\n\n# For adding FastAPI endpoints\npip install devsetgo-lib[fastapi]\n\n# Install everything\npip install devsetgo-lib[all]\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Here's a quick example to demonstrate how you can use some of the key features of <code>devsetgo_lib</code>:</p> <pre><code>from devsetgo_lib.common_functions import file_functions, logging_config, patterns, calendar_functions\n\n# File Operations\nfile_functions.create_sample_files(\"example\", 100)\ncontent = file_functions.read_from_file(\"example.csv\")\nprint(content)\n\n# Logging\nlogging_config.config_log(logging_directory='logs', log_name='app.log', logging_level='DEBUG')\nlogger = logging.getLogger('app_logger')\nlogger.info(\"This is an info message\")\n\n# Pattern Matching\ntext = \"Hello, my name is 'John Doe' and I live in 'New York'.\"\nresults = patterns.pattern_between_two_char(text, \"'\", \"'\")\nprint(results)\n\n# Calendar Functions\nprint(calendar_functions.get_month(1))  # Output: 'January'\nprint(calendar_functions.get_month_number('January'))  # Output: 1\n</code></pre> <p>For detailed documentation on each module and function, please refer to the official documentation.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our contributing guidelines for more details.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for more details.</p>"},{"location":"#contact","title":"Contact","text":"<p>For any questions or issues, please open an issue on GitHub or contact us at devsetgo@example.com.</p>"},{"location":"about/","title":"About","text":"<p>DevSetGo Library is a collection of tools and utilities to help developers with their projects. It has been primarily developed to help with the development of the DevSetGo.com website and other projects in my personal and professional life. It is a work in progress and will be updated as needed.</p> <p>The driving force behind the library is to limit the amount of boilerplate code that I (Mike Ryan) have to write for each project. Copying code from one project to another causes issues to get fixed in one project, but not always get updated in others. This library is an attempt to fix that issue and make it easier to maintain code across multiple projects.</p> <p>The library is written in Python and is available on PyPi. It is open source and available on GitHub. Feel free to use it in your projects and contribute to the library.</p>"},{"location":"about/#about-me","title":"About Me","text":"<p>I am a software engineering manager with an eclectic background in various industries (finance, manufacturing, and metrology). I am passionate about software development and love to learn new things.</p> <ul> <li>DevSetGo.com</li> <li>Github</li> </ul>"},{"location":"contribute/","title":"Contributing","text":"<p>Please feel to contribute to this project. Adding common functions is the intent and if you have one to add or improve an existing it is greatly appreciated.</p>"},{"location":"contribute/#ways-to-contribute","title":"Ways to Contribute!","text":"<ul> <li>Add or improve a function</li> <li>Add or improve documentation</li> <li>Add or improve Tests</li> <li>Report or fix a bug</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#install","title":"Install","text":"<pre><code>pip install devsetgo-lib\n\n# Aysync database setup\npip install devsetgo-lib[sqlite]\npip install devsetgo-lib[postgres]\n\n# Consider these experimental and untested\npip install devsetgo-lib[oracle]\npip install devsetgo-lib[mssql]\npip install devsetgo-lib[mysql]\n\n# For adding FastAPI endpoints\npip install devsetgo-lib[fastapi]\n\n# Install everything\npip install devsetgo-lib[all]\n</code></pre> <p>See documentation for more examples of library use</p>"},{"location":"release-notes/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog</p>"},{"location":"release-notes/#latest-changes","title":"Latest Changes","text":""},{"location":"release-notes/#extending-execute-one-and-many-functions-breaking-change-v2025891","title":"Extending Execute One and Many functions - breaking change (v2025.8.9.1)","text":"<p> Read the Full Documentation here.</p>"},{"location":"release-notes/#whats-changed","title":"What's Changed","text":"<ul> <li>520 support multiple versions of documentation (#522) @devsetgo</li> <li>Improve Execute One and Many query capability (#521) @devsetgo</li> <li>pip(deps): bump ruff from 0.12.2 to 0.12.7 (#514) @dependabot[bot]</li> <li>github actionts(deps): bump SonarSource/sonarqube-scan-action from 5.2.0 to 5.3.0 (#515) @dependabot[bot]</li> <li>pip(deps): bump pytest-asyncio from 1.0.0 to 1.1.0 (#516) @dependabot[bot]</li> <li>pip(deps): bump sqlalchemy from 2.0.41 to 2.0.42 (#517) @dependabot[bot]</li> <li>pip(deps): bump mkdocstrings[python,shell] from 0.29.1 to 0.30.0 (#518) @dependabot[bot]</li> <li>pip(deps): bump fastapi[all] from 0.115.14 to 0.116.1 (#519) @dependabot[bot]</li> <li>pip(deps): update sqlalchemy-cockroachdb requirement from &lt;2.0.2 to &lt;2.0.4 (#510) @dependabot[bot]</li> <li>updating requirements (#513) @devsetgo</li> <li>pip(deps): update sqlalchemy-cockroachdb requirement from &lt;2.0.2 to &lt;2.0.3 (#505) @dependabot[bot]</li> <li>pip(deps): bump ruff from 0.11.11 to 0.11.12 (#506) @dependabot[bot]</li> <li>github actionts(deps): bump SonarSource/sonarqube-scan-action from 5.1.0 to 5.2.0 (#507) @dependabot[bot]</li> <li>update of requirements and tests (#504) @devsetgo</li> </ul> <p>Built to help reduce copy/paste from multiple projects and uses calendar versioning (year.month.day.build) from BumpCalver.</p> <p>Published Date: 2025 August 09, 21:54</p>"},{"location":"release-notes/#adding-log-propagate-v2025541","title":"Adding Log Propagate (v2025.5.4.1)","text":"<p> Read the Full Documentation here.</p>"},{"location":"release-notes/#whats-changed_1","title":"What's Changed","text":"<ul> <li>Adding Log Propagate (#503) @devsetgo</li> </ul> <p>Built to help reduce copy/paste from multiple projects and uses calendar versioning (year.month.day.build) from BumpCalver.</p> <p>Published Date: 2025 May 04, 17:46</p>"},{"location":"release-notes/#improving-sqlalchemy-queries-and-improved-documentation-v20254171","title":"Improving SQLAlchemy Queries and Improved Documentation (v2025.4.17.1)","text":"<p> Read the Full Documentation here.</p>"},{"location":"release-notes/#whats-changed_2","title":"What's Changed","text":"<ul> <li>fixing sonar issue (#494) @devsetgo</li> <li>updating release drafter (#493) @devsetgo</li> <li>Improving Examples (#492) @devsetgo</li> <li>Fix of issue with distinct queries in read queries (#491) @devsetgo</li> <li>updating footer (#490) @devsetgo</li> <li>adding raw link to release drafter (#489) @devsetgo</li> <li>Improving documentation (#488) @devsetgo</li> <li>adding 3.13 testing (#486) @devsetgo</li> </ul> <p>Built to help reduce copy/paste from multiple projects and uses calendar versioning (year.month.day.build) from BumpCalver.</p> <p>Published Date: 2025 April 17, 15:47</p>"},{"location":"release-notes/#new-functions-and-updated-documentation-v2025451","title":"New Functions and Updated Documentation (v2025.4.5.1)","text":""},{"location":"release-notes/#whats-changed_3","title":"What's Changed","text":"<ul> <li>Fix of Calendar Version Pattern (#485) @devsetgo</li> <li>Working on Improving Documentation and Tests (#474) @devsetgo</li> <li>pip(deps): bump pytest-asyncio from 0.25.3 to 0.26.0 (#480) @dependabot[bot]</li> <li>pip(deps): bump mkdocstrings[python,shell] from 0.27.0 to 0.29.1 (#481) @dependabot[bot]</li> <li>pip(deps): bump black from 24.10.0 to 25.1.0 (#482) @dependabot[bot]</li> <li>pip(deps): bump ruff from 0.9.9 to 0.11.2 (#483) @dependabot[bot]</li> <li>pip(deps): bump pre-commit from 4.0.1 to 4.2.0 (#484) @dependabot[bot]</li> <li>pip(deps): bump pytest-asyncio from 0.25.0 to 0.25.3 (#475) @dependabot[bot]</li> <li>pip(deps): bump ruff from 0.9.4 to 0.9.9 (#476) @dependabot[bot]</li> <li>pip(deps): bump flake8 from 7.1.1 to 7.1.2 (#477) @dependabot[bot]</li> <li>pip(deps): bump twine from 6.0.1 to 6.1.0 (#478) @dependabot[bot]</li> <li>pip(deps): bump structlog from 24.4.0 to 25.1.0 (#479) @dependabot[bot]</li> <li>pip(deps): bump pymdown-extensions from 10.13 to 10.14.3 (#473) @dependabot[bot]</li> <li>pip(deps): bump python-json-logger from 2.0.7 to 3.2.1 (#469) @dependabot[bot]</li> <li>pip(deps): bump ruff from 0.8.1 to 0.9.4 (#470) @dependabot[bot]</li> <li>pip(deps): bump pytest from 8.3.3 to 8.3.4 (#471) @dependabot[bot]</li> <li>pip(deps): bump pygments from 2.18.0 to 2.19.1 (#472) @dependabot[bot]</li> <li>pip(deps): bump loguru from 0.7.2 to 0.7.3 (#463) @dependabot[bot]</li> <li>pip(deps): bump hatchling from 1.26.3 to 1.27.0 (#464) @dependabot[bot]</li> <li>pip(deps): bump bumpcalver from 2024.11.8 to 2024.12.14.1 (#466) @dependabot[bot]</li> <li>pip(deps): bump pymdown-extensions from 10.12 to 10.13 (#465) @dependabot[bot]</li> <li>pip(deps): bump pytest-asyncio from 0.24.0 to 0.25.0 (#467) @dependabot[bot]</li> <li>pip(deps): bump mkdocs-material from 9.5.46 to 9.5.47 (#460) @dependabot[bot]</li> <li>pip(deps): bump twine from 5.1.1 to 6.0.1 (#461) @dependabot[bot]</li> <li>pip(deps): bump ruff from 0.8.0 to 0.8.1 (#462) @dependabot[bot]</li> </ul> <p>Published Date: 2025 April 05, 20:56</p>"},{"location":"release-notes/#adding-new-db-functions-v202411281","title":"Adding new db functions (v2024.11.28.1)","text":""},{"location":"release-notes/#whats-changed_4","title":"What's Changed","text":"<ul> <li>Adding new general execute queries and adding deprecation (#459) @devsetgo</li> <li>pip(deps): bump tox from 4.23.0 to 4.23.2 (#455) @dependabot</li> <li>pip(deps): bump fastapi[all] from 0.115.2 to 0.115.4 (#454) @dependabot</li> <li>pip(deps): bump tqdm from 4.66.5 to 4.66.6 (#456) @dependabot</li> <li>pip(deps): bump pymdown-extensions from 10.11.2 to 10.12 (#457) @dependabot</li> <li>pip(deps): bump ruff from 0.7.0 to 0.7.1 (#458) @dependabot</li> </ul> <p>Published Date: 2024 November 28, 22:01</p>"},{"location":"release-notes/#moving-to-calendar-versioning-202410201","title":"Moving to Calendar Versioning (2024.10.20.1)","text":""},{"location":"release-notes/#whats-changed_5","title":"What's Changed","text":"<ul> <li>moving to calendar versioning (#453) @devsetgo</li> <li>pip(deps): bump tox from 4.21.0 to 4.23.0 (#452) @dependabot</li> <li>pip(deps): bump fastapi[all] from 0.114.2 to 0.115.0 (#451) @dependabot</li> <li>pip(deps): bump tox from 4.18.1 to 4.21.0 (#450) @dependabot</li> <li>pip(deps): bump watchdog from 5.0.2 to 5.0.3 (#449) @dependabot</li> <li>pip(deps): bump pylint from 3.2.7 to 3.3.1 (#448) @dependabot</li> <li>pip(deps): bump ruff from 0.6.5 to 0.6.8 (#447) @dependabot</li> </ul> <p>Published Date: 2024 October 20, 16:30</p>"},{"location":"release-notes/#complete-replacement-of-cx-oracle-for-oracledb-v0144","title":"Complete Replacement of CX-Oracle for OracleDB (v0.14.4)","text":""},{"location":"release-notes/#whats-changed_6","title":"What's Changed","text":"<ul> <li>Remove CX-Oracle for OracleDB cleanup (#446) @devsetgo</li> <li>pip(deps): bump pylint from 3.2.6 to 3.2.7 (#442) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.5.33 to 9.5.34 (#443) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#444) @dependabot</li> <li>github actionts(deps): bump actions/setup-python from 2 to 5 (#445) @dependabot</li> </ul> <p>Published Date: 2024 September 15, 15:28</p>"},{"location":"release-notes/#standard-logging-suppression-by-default-v0143","title":"Standard Logging Suppression by Default (v0.14.3)","text":""},{"location":"release-notes/#whats-changed_7","title":"What's Changed","text":"<ul> <li>Limit Standard Logging being Displayed (#441) @devsetgo</li> </ul> <p>Published Date: 2024 August 31, 17:33</p>"},{"location":"release-notes/#improvements-and-fixes-v0142","title":"Improvements and fixes (v0.14.2)","text":""},{"location":"release-notes/#whats-changed_8","title":"What's Changed","text":"<ul> <li>Improvements and fixes (#440) @devsetgo</li> </ul>"},{"location":"release-notes/#breaking-changes","title":"Breaking changes","text":"<ul> <li>save_text function no longer adds .txt by default.</li> <li>Change from cx-oracle to oracledb</li> <li>Improvements to documentation</li> </ul> <p>Published Date: 2024 August 31, 00:02</p>"},{"location":"release-notes/#adding-db-disconnect-v0141","title":"Adding DB Disconnect (v0.14.1)","text":""},{"location":"release-notes/#whats-changed_9","title":"What's Changed","text":"<ul> <li>Adding Database Disconnect (#439) @devsetgo</li> <li>pip(deps): bump pre-commit from 3.7.1 to 3.8.0 (#434) @dependabot</li> <li>updates to deal with stashing pages (#437) @devsetgo</li> <li>working on issue for deployment (#436) @devsetgo</li> <li>Adding MKDocs Workflow (#435) @devsetgo</li> <li>Version 0.14.0 (#433) @devsetgo</li> </ul> <p>Published Date: 2024 August 25, 18:47</p>"},{"location":"release-notes/#fix-of-version-for-pypi-v0140-a","title":"Fix of version for Pypi (v0.14.0-a)","text":""},{"location":"release-notes/#whats-changed_10","title":"What's Changed","text":"<ul> <li>Version 0.14.0 (#433) @devsetgo</li> </ul> <p>Published Date: 2024 July 27, 22:40</p>"},{"location":"release-notes/#high-speed-multi-processing-improvements-v0140","title":"High Speed Multi-Processing Improvements (v0.14.0)","text":""},{"location":"release-notes/#whats-changed_11","title":"What's Changed","text":"<ul> <li>High Speed Logging for Loguru Multi-Processing (#432) @devsetgo</li> <li>Resilient Sink Fixes (#431) @devsetgo</li> <li>Fix of bug in resilient sink (#430) @devsetgo</li> <li>Adding Resiliency to Logging Config (#429) @devsetgo</li> <li>pip(deps): bump mkdocs-print-site-plugin from 2.4.1 to 2.5.0 (#422) @dependabot</li> <li>pip(deps): bump ruff from 0.4.5 to 0.4.7 (#420) @dependabot</li> <li>pip(deps): bump autopep8 from 2.1.1 to 2.2.0 (#421) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.5.24 to 9.5.25 (#423) @dependabot</li> </ul> <p>Published Date: 2024 July 27, 22:28</p>"},{"location":"release-notes/#v0130-republish","title":"(v0.13.0-republish)","text":""},{"location":"release-notes/#whats-changed_12","title":"What's Changed","text":"<p>Republishing v0.13.0 for pypi.</p> <p>Published Date: 2024 May 26, 17:13</p>"},{"location":"release-notes/#v0130","title":"(v0.13.0)","text":""},{"location":"release-notes/#whats-changed_13","title":"What's Changed","text":"<ul> <li>Breaking Change: Removing Limit and Offset from read queries (#419) @devsetgo</li> </ul> <p>Published Date: 2024 May 26, 15:44</p>"},{"location":"release-notes/#adding-missing-requirement-v0124","title":"Adding missing requirement (v0.12.4)","text":""},{"location":"release-notes/#whats-changed_14","title":"What's Changed","text":"<ul> <li>adding missing requirement (#417) @devsetgo</li> </ul> <p>Published Date: 2024 May 16, 14:40</p>"},{"location":"release-notes/#adding-email-validation-v0123","title":"Adding Email Validation (v0.12.3)","text":""},{"location":"release-notes/#whats-changed_15","title":"What's Changed","text":"<ul> <li>bump to 0.12.3 (#416) @devsetgo</li> <li>Add email validation capabilities (#415) @devsetgo</li> <li>pip(deps): bump mkdocs-material from 9.5.20 to 9.5.21 (#414) @dependabot</li> <li>pip(deps): bump ruff from 0.4.2 to 0.4.4 (#413) @dependabot</li> <li>pip(deps): bump coverage-badge from 1.1.0 to 1.1.1 (#409) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.5.18 to 9.5.20 (#408) @dependabot</li> <li>pip(deps): bump mkdocstrings[python,shell] from 0.24.3 to 0.25.0 (#407) @dependabot</li> <li>pip(deps): bump ruff from 0.4.1 to 0.4.2 (#410) @dependabot</li> <li>pip(deps): bump tqdm from 4.66.2 to 4.66.3 (#412) @dependabot</li> </ul> <p>Published Date: 2024 May 16, 14:19</p>"},{"location":"release-notes/#logging-changes-v0122","title":"logging changes (v0.12.2)","text":""},{"location":"release-notes/#whats-changed_16","title":"What's Changed","text":"<ul> <li>Logging Changes, Documentation Updates, Using Ruff (#406) @devsetgo</li> </ul> <p>Published Date: 2024 April 22, 16:17</p>"},{"location":"release-notes/#updates-for-metadata-and-all-http-codes-v0121","title":"Updates for MetaData and All HTTP Codes (v0.12.1)","text":""},{"location":"release-notes/#whats-changed_17","title":"What's Changed","text":"<ul> <li>Enhancements and fixes (#405) @devsetgo</li> </ul> <p>Published Date: 2024 April 19, 18:52</p>"},{"location":"release-notes/#breaking-change-base-schema-per-database-type-v0120","title":"Breaking Change: Base Schema per Database Type (v0.12.0)","text":""},{"location":"release-notes/#whats-changed_18","title":"What's Changed","text":"<ul> <li>Adding new base schema for database types (#402) @devsetgo</li> <li>creating main release for 0.11.2 (#390) @devsetgo</li> <li>Working on bug in read_query (#389) @devsetgo</li> <li>Reorganizing Library Structure (#388) @devsetgo</li> <li>Python Build and Publish fix (#382) @devsetgo</li> </ul>"},{"location":"release-notes/#_1","title":"Release Notes","text":"<p>Breaking Change - SchemaBase is now SchemaBaseSQLite</p> <p>Published Date: 2024 April 13, 22:55</p>"},{"location":"release-notes/#reorganizing-library-stucture-v0112-main","title":"Reorganizing Library Stucture (v0.11.2-main)","text":""},{"location":"release-notes/#whats-changed_19","title":"What's Changed","text":"<ul> <li>creating main release for 0.11.2 (#390) @devsetgo</li> <li>Working on bug in read_query (#389) @devsetgo</li> <li>Reorganizing Library Structure (#388) @devsetgo</li> <li>Python Build and Publish fix (#382) @devsetgo</li> </ul> <p>Breaking Changes are included in this release for import paths. See documents for more information.</p> <p>Published Date: 2024 February 17, 19:09</p>"},{"location":"release-notes/#read-query-fix-beta-testing-v0112-beta1","title":"Read Query Fix Beta Testing (v0.11.2-beta1)","text":""},{"location":"release-notes/#whats-changed_20","title":"What's Changed","text":"<ul> <li>Working on bug in read_query (#389) @devsetgo</li> <li>Reorganizing Library Structure (#388) @devsetgo</li> <li>Python Build and Publish fix (#382) @devsetgo</li> </ul> <p>Published Date: 2024 February 16, 22:01</p>"},{"location":"release-notes/#pre-release-to-test-new-structure-and-publishing-v0112-beta","title":"Pre-Release to test new structure and publishing (v0.11.2-beta)","text":""},{"location":"release-notes/#whats-changed_21","title":"What's Changed","text":"<ul> <li>Reorganizing Library Structure (#388) @devsetgo</li> <li>Python Build and Publish fix (#382) @devsetgo</li> </ul> <p>Published Date: 2024 February 10, 21:16</p>"},{"location":"release-notes/#v0112-fix2","title":"(v0.11.2-fix2)","text":""},{"location":"release-notes/#whats-changed_22","title":"What's Changed","text":"<ul> <li>Python Build and Publish fix (#382) @devsetgo</li> </ul> <p>Published Date: 2024 January 21, 15:01</p>"},{"location":"release-notes/#adding-delete-many-and-minor-fixes-v0112","title":"Adding Delete Many and minor fixes (v0.11.2)","text":""},{"location":"release-notes/#whats-changed_23","title":"What's Changed","text":"<ul> <li>Adding Delete Many and Other Updates  (#381) @devsetgo</li> <li>pip(deps): bump mkdocs-material from 9.5.2 to 9.5.3 (#377) @dependabot</li> <li>pip(deps): bump fastapi[all] from 0.105.0 to 0.108.0 (#375) @dependabot</li> <li>pip(deps): bump sqlalchemy from 2.0.23 to 2.0.24 (#374) @dependabot</li> <li>pip(deps): bump pytest from 7.4.3 to 7.4.4 (#373) @dependabot</li> <li>pip(deps): bump black from 23.12.0 to 23.12.1 (#376) @dependabot</li> <li>github actionts(deps): bump actions/setup-python from 4 to 5 (#378) @dependabot</li> </ul> <p>Published Date: 2024 January 20, 00:07</p>"},{"location":"release-notes/#breaking-change-v0111","title":"Breaking Change (v0.11.1)","text":""},{"location":"release-notes/#whats-changed_24","title":"What's Changed","text":"<ul> <li>Bump of Version to 0.11.1 (#371) @devsetgo</li> <li>Query Improvement (#370) @devsetgo</li> <li>368 get one record should return an empty value when called (#369) @devsetgo</li> <li>updating docs from v0.11.0 release (#367) @devsetgo</li> </ul> <p>Published Date: 2023 December 23, 10:49</p>"},{"location":"release-notes/#full-release-of-new-features-v0110","title":"Full Release of New Features (v0.11.0)","text":""},{"location":"release-notes/#whats-changed_25","title":"What's Changed","text":"<ul> <li>Prep for Release (#366) @devsetgo</li> <li>Fixing sonar settings (#365) @devsetgo</li> <li>Fixes and improvements (#364) @devsetgo</li> <li>Dev (#362) @devsetgo</li> <li>Fix of issues from Beta release (#361) @devsetgo</li> <li>359 tables are created before create tables is called (#360) @devsetgo</li> <li>Change Log (#358) @devsetgo</li> <li>fixing latest-changes (#357) @devsetgo</li> <li>removing jinja template from Latest Changes Action (#356) @devsetgo</li> <li>Action fixing adding main (#355) @devsetgo</li> <li>Fixing actions (#354) @devsetgo</li> <li>Fixing Beta Publishing issues and Documentation Improvements (#353) @devsetgo</li> <li>Update setup.py for sub packages (#352) @devsetgo</li> <li>Import Bug Fix (#351) @devsetgo</li> <li>Latest Changes Action Fix (#350) @devsetgo</li> <li>Next Release (#349) @devsetgo</li> <li>Dev (#348) @devsetgo</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 17, 22:00</p>"},{"location":"release-notes/#beta-release-with-fixes-for-multiple-issues-v0110-beta3-fix1","title":"Beta Release with fixes for multiple issues (v0.11.0-beta3-fix1)","text":""},{"location":"release-notes/#whats-changed_26","title":"What's Changed","text":"<ul> <li>Dev (#362) @devsetgo</li> <li>Fix of issues from Beta release (#361) @devsetgo</li> <li>359 tables are created before create tables is called (#360) @devsetgo</li> <li>Change Log (#358) @devsetgo</li> <li>fixing latest-changes (#357) @devsetgo</li> <li>removing jinja template from Latest Changes Action (#356) @devsetgo</li> <li>Action fixing adding main (#355) @devsetgo</li> <li>Fixing actions (#354) @devsetgo</li> <li>Fixing Beta Publishing issues and Documentation Improvements (#353) @devsetgo</li> <li>Update setup.py for sub packages (#352) @devsetgo</li> <li>Import Bug Fix (#351) @devsetgo</li> <li>Latest Changes Action Fix (#350) @devsetgo</li> <li>Next Release (#349) @devsetgo</li> <li>Dev (#348) @devsetgo</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 17, 16:23</p>"},{"location":"release-notes/#fixing-asyncdatabase-create-tables-v0110-beta3","title":"Fixing AsyncDatabase create tables (v0.11.0-beta3)","text":""},{"location":"release-notes/#whats-changed_27","title":"What's Changed","text":"<ul> <li>Fix of issues from Beta release (#361) @devsetgo</li> <li>359 tables are created before create tables is called (#360) @devsetgo</li> <li>Change Log (#358) @devsetgo</li> <li>fixing latest-changes (#357) @devsetgo</li> <li>removing jinja template from Latest Changes Action (#356) @devsetgo</li> <li>Action fixing adding main (#355) @devsetgo</li> <li>Fixing actions (#354) @devsetgo</li> <li>Fixing Beta Publishing issues and Documentation Improvements (#353) @devsetgo</li> <li>Update setup.py for sub packages (#352) @devsetgo</li> <li>Import Bug Fix (#351) @devsetgo</li> <li>Latest Changes Action Fix (#350) @devsetgo</li> <li>Next Release (#349) @devsetgo</li> <li>Dev (#348) @devsetgo</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 17, 16:18</p>"},{"location":"release-notes/#build-updates-v0110-beta2","title":"Build Updates  (v0.11.0-beta2)","text":""},{"location":"release-notes/#whats-changed_28","title":"What's Changed","text":"<ul> <li>Change Log (#358) @devsetgo</li> <li>fixing latest-changes (#357) @devsetgo</li> <li>removing jinja template from Latest Changes Action (#356) @devsetgo</li> <li>Action fixing adding main (#355) @devsetgo</li> <li>Fixing actions (#354) @devsetgo</li> <li>Fixing Beta Publishing issues and Documentation Improvements (#353) @devsetgo</li> <li>Update setup.py for sub packages (#352) @devsetgo</li> <li>Import Bug Fix (#351) @devsetgo</li> <li>Latest Changes Action Fix (#350) @devsetgo</li> <li>Next Release (#349) @devsetgo</li> <li>Dev (#348) @devsetgo</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 16, 20:34</p>"},{"location":"release-notes/#beta-release-with-fixes-for-multiple-issues-v0110-beta1-fix5","title":"Beta Release with fixes for multiple issues (v0.11.0-beta1-fix5)","text":""},{"location":"release-notes/#whats-changed_29","title":"What's Changed","text":"<ul> <li>fixing latest-changes (#357) @devsetgo</li> <li>removing jinja template from Latest Changes Action (#356) @devsetgo</li> <li>Action fixing adding main (#355) @devsetgo</li> <li>Fixing actions (#354) @devsetgo</li> <li>Fixing Beta Publishing issues and Documentation Improvements (#353) @devsetgo</li> <li>Update setup.py for sub packages (#352) @devsetgo</li> <li>Import Bug Fix (#351) @devsetgo</li> <li>Latest Changes Action Fix (#350) @devsetgo</li> <li>Next Release (#349) @devsetgo</li> <li>Dev (#348) @devsetgo</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 16, 16:33</p>"},{"location":"release-notes/#build-fixes-v0110-beta1-fix4","title":"Build Fixes  (v0.11.0-beta1-fix4)","text":""},{"location":"release-notes/#whats-changed_30","title":"What's Changed","text":"<ul> <li>Update setup.py for sub packages (#352) @devsetgo</li> <li>Import Bug Fix (#351) @devsetgo</li> <li>Latest Changes Action Fix (#350) @devsetgo</li> <li>Next Release (#349) @devsetgo</li> <li>Dev (#348) @devsetgo</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 12, 11:45</p>"},{"location":"release-notes/#async-database-and-fastapi-functions-v0110-beta0","title":"Async Database and FastAPI functions (v0.11.0-beta0)","text":""},{"location":"release-notes/#whats-changed_31","title":"What's Changed","text":"<ul> <li>Dev (#348) @devsetgo - New functionality and documentation for FastAPI Endpoints and Async Database Functionality</li> <li>pip(deps): bump autopep8 from 2.0.2 to 2.0.4 (#343) @dependabot</li> <li>pip(deps): bump wheel from 0.41.2 to 0.42.0 (#345) @dependabot</li> <li>pip(deps): bump mkdocstrings[python] from 0.21.2 to 0.24.0 (#346) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.3 to 1.5.3 (#347) @dependabot</li> <li>pip(deps): bump flake8 from 6.0.0 to 6.1.0 (#332) @dependabot</li> <li>pip(deps): bump click from 8.1.3 to 8.1.7 (#337) @dependabot</li> <li>pip(deps): bump wheel from 0.40.0 to 0.41.2 (#339) @dependabot</li> <li>github actionts(deps): bump actions/checkout from 2 to 4 (#340) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.17 to 9.4.2 (#341) @dependabot</li> <li>pip(deps): bump black from 23.3.0 to 23.9.1 (#342) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.15 to 9.1.17 (#326) @dependabot</li> <li>pip(deps): bump pytest from 7.3.1 to 7.4.0 (#327) @dependabot</li> <li>pip(deps): bump mkdocs from 1.4.2 to 1.4.3 (#328) @dependabot</li> <li>pip(deps): bump autoflake from 2.1.1 to 2.2.0 (#329) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.2 to 3.3.3 (#330) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.9 to 9.1.15 (#325) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.2 to 2.1.1 (#324) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.2.1 to 3.3.1 (#323) @dependabot</li> <li>pip(deps): bump tox from 4.4.11 to 4.5.2 (#322) @dependabot</li> <li>pip(deps): bump pytest-cov from 4.0.0 to 4.1.0 (#321) @dependabot</li> <li>pip(deps): bump loguru from 0.6.0 to 0.7.0 (#317) @dependabot</li> <li>pip(deps): bump mkdocs-gen-files from 0.4.0 to 0.5.0 (#314) @dependabot</li> <li>pip(deps): bump pylint from 2.17.2 to 2.17.4 (#319) @dependabot</li> <li>pip(deps): bump mkdocs-material from 9.1.6 to 9.1.9 (#320) @dependabot</li> <li>pip(deps): bump pytest from 7.3.0 to 7.3.1 (#318) @dependabot</li> </ul> <p>Published Date: 2023 December 10, 20:17</p>"},{"location":"release-notes/#pattern-analysis-update-and-bug-fix-v0101","title":"Pattern Analysis Update and Bug Fix (v0.10.1)","text":""},{"location":"release-notes/#whats-changed_32","title":"What's Changed","text":"<ul> <li>Improvement to the patterns analysis (#313) @devsetgo</li> <li>pip(deps): bump mkdocs-material from 9.1.3 to 9.1.5 (#308) @dependabot</li> <li>pip(deps): bump pre-commit from 3.2.0 to 3.2.1 (#310) @dependabot</li> <li>pip(deps): bump watchdog from 2.3.1 to 3.0.0 (#309) @dependabot</li> <li>pip(deps): bump pylint from 2.17.0 to 2.17.1 (#311) @dependabot</li> <li>pip(deps): bump tox from 4.4.7 to 4.4.8 (#312) @dependabot</li> </ul> <p>Published Date: 2023 April 08, 21:45</p>"},{"location":"release-notes/#chatgpt-driven-improvements-v0100","title":"ChatGPT Driven Improvements (v0.10.0)","text":""},{"location":"release-notes/#chatgpt","title":"ChatGPT","text":"<p>Using ChatGPT to improve tests, find bugs, and improve performance. Code coverage is at 100% and the code base appears to be performing better than before.</p> <p>Major changes are in PR #304 </p>"},{"location":"release-notes/#whats-changed_33","title":"What's Changed","text":"<ul> <li>latest change fix for regex pattern. (#307) @devsetgo</li> <li>Dev (#306) @devsetgo</li> <li>Workflow changes (#305) @devsetgo</li> <li>ChatGPT Driven Improvements (#304) @devsetgo</li> <li>pip(deps): bump pre-commit from 3.0.2 to 3.1.1 (#300) @dependabot</li> <li>pip(deps): bump pytest-xdist from 3.1.0 to 3.2.0 (#302) @dependabot</li> <li>pip(deps): bump autoflake from 2.0.0 to 2.0.1 (#299) @dependabot</li> <li>pip(deps): bump watchdog from 2.1.9 to 2.3.1 (#301) @dependabot</li> <li>pip(deps): bump pytest from 7.2.0 to 7.2.1 (#303) @dependabot</li> <li>pip(deps): bump pylint from 2.15.7 to 2.16.1 (#298) @dependabot</li> <li>pip(deps): bump autopep8 from 2.0.0 to 2.0.1 (#289) @dependabot</li> <li>pip(deps): bump pylint from 2.15.7 to 2.15.10 (#295) @dependabot</li> <li>pip(deps): bump black from 22.10.0 to 23.1.0 (#294) @dependabot</li> <li>pip(deps): bump tox from 3.27.1 to 4.4.4 (#296) @dependabot</li> <li>pip(deps): bump pre-commit from 2.20.0 to 3.0.2 (#297) @dependabot</li> </ul> <p>Published Date: 2023 April 01, 00:27</p>"},{"location":"release-notes/#open-csv-enhancements-and-library-updates-v090","title":"Open CSV enhancements and library updates (v0.9.0)","text":""},{"location":"release-notes/#whats-changed_34","title":"What's Changed","text":"<ul> <li>fix of latest changes (#288) @devsetgo</li> <li>Open_CSV Enhancements (#287) @devsetgo</li> <li>pip(deps): bump pytest-cov from 3.0.0 to 4.0.0 (#274) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.4.2 to 8.5.5 (#276) @dependabot</li> <li>pip(deps): bump autoflake from 1.5.3 to 1.6.1 (#275) @dependabot</li> <li>pip(deps): bump tqdm from 4.64.0 to 4.64.1 (#273) @dependabot</li> <li>pip(deps): bump pytest from 7.1.2 to 7.1.3 (#272) @dependabot</li> <li>pip(deps): bump mkdocs from 1.3.1 to 1.4.0 (#271) @dependabot</li> <li>pip(deps): bump tox from 3.25.1 to 3.26.0 (#269) @dependabot</li> <li>pip(deps): bump pylint from 2.15.0 to 2.15.3 (#270) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.3.9 to 8.4.2 (#268) @dependabot</li> <li>pip(deps): bump autopep8 from 1.6.0 to 1.7.0 (#264) @dependabot</li> <li>pip(deps): bump pylint from 2.14.5 to 2.15.0 (#265) @dependabot</li> <li>pip(deps): bump autoflake from 1.4 to 1.5.3 (#263) @dependabot</li> <li>pip(deps): bump black from 22.6.0 to 22.8.0 (#267) @dependabot</li> <li>pip(deps): bump flake8 from 5.0.1 to 5.0.4 (#266) @dependabot</li> <li>pip(deps): bump pre-commit from 2.19.0 to 2.20.0 (#260) @dependabot</li> <li>pip(deps): bump mkdocs from 1.3.0 to 1.3.1 (#261) @dependabot</li> <li>pip(deps): bump flake8 from 4.0.1 to 5.0.1 (#259) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.3.8 to 8.3.9 (#258) @dependabot</li> <li>pip(deps): bump pylint from 2.14.4 to 2.14.5 (#262) @dependabot</li> <li>pip(deps): bump twine from 4.0.0 to 4.0.1 (#252) @dependabot</li> <li>pip(deps): bump pylint from 2.14.0 to 2.14.4 (#251) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.2.16 to 8.3.8 (#253) @dependabot</li> <li>pip(deps): bump black from 22.3.0 to 22.6.0 (#254) @dependabot</li> <li>pip(deps): bump tox from 3.25.0 to 3.25.1 (#255) @dependabot</li> <li>pip(deps): bump watchdog from 2.1.8 to 2.1.9 (#256) @dependabot</li> <li>github actionts(deps): bump actions/setup-python from 3 to 4 (#257) @dependabot</li> <li>pip(deps): bump pylint from 2.13.7 to 2.14.0 (#250) @dependabot</li> <li>pip(deps): bump watchdog from 2.1.7 to 2.1.8 (#246) @dependabot</li> <li>pip(deps): bump pre-commit from 2.18.1 to 2.19.0 (#248) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.2.12 to 8.2.16 (#249) @dependabot</li> <li>pip(deps): bump tox from 3.24.5 to 3.25.0 (#242) @dependabot</li> <li>pip(deps): bump pre-commit from 2.17.0 to 2.18.1 (#243) @dependabot</li> <li>pip(deps): bump click from 8.1.2 to 8.1.3 (#245) @dependabot</li> <li>pip(deps): bump pylint from 2.13.4 to 2.13.7 (#240) @dependabot</li> <li>pip(deps): bump tqdm from 4.63.1 to 4.64.0 (#244) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.2.8 to 8.2.12 (#241) @dependabot</li> <li>pip(deps): bump pytest from 7.1.1 to 7.1.2 (#239) @dependabot</li> <li>pip(deps): bump watchdog from 2.1.6 to 2.1.7 (#238) @dependabot</li> <li>pip(deps): bump pylint from 2.12.2 to 2.13.4 (#237) @dependabot</li> <li>pip(deps): bump mkdocs from 1.2.3 to 1.3.0 (#234) @dependabot</li> <li>pip(deps): bump tqdm from 4.63.0 to 4.63.1 (#233) @dependabot</li> <li>pip(deps): bump black from 22.1.0 to 22.3.0 (#236) @dependabot</li> <li>pip(deps): bump pytest from 7.0.1 to 7.1.1 (#231) @dependabot</li> <li>pip(deps): bump click from 8.0.4 to 8.1.2 (#235) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.2.5 to 8.2.8 (#232) @dependabot</li> <li>pip(deps): bump twine from 3.8.0 to 4.0.0 (#230) @dependabot</li> <li>document updates (#229) @devsetgo</li> </ul> <p>Published Date: 2022 December 04, 16:55</p>"},{"location":"release-notes/#additional-logging-configuration-v080","title":"Additional Logging Configuration (v0.8.0)","text":""},{"location":"release-notes/#whats-changed_35","title":"What's Changed","text":"<ul> <li>New Logging Configuration items (#228) @devsetgo</li> <li>pip(deps): bump tqdm from 4.62.3 to 4.63.0 (#224) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.2.3 to 8.2.4 (#227) @dependabot</li> <li>github actionts(deps): bump actions/setup-python from 2.3.1 to 3 (#226) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.1.9 to 8.2.3 (#225) @dependabot</li> <li>pip(deps): bump twine from 3.7.1 to 3.8.0 (#223) @dependabot</li> <li>pip(deps): bump pytest from 6.2.5 to 7.0.1 (#222) @dependabot</li> <li>pip(deps): bump pytest-runner from 5.3.1 to 6.0.0 (#221) @dependabot</li> <li>pip(deps): bump loguru from 0.5.3 to 0.6.0 (#218) @dependabot</li> <li>pip(deps): bump black from 21.12b0 to 22.1.0 (#219) @dependabot</li> <li>pip(deps): bump mkdocs-material from 8.1.8 to 8.1.9 (#220) @dependabot</li> </ul> <p>Published Date: 2022 March 12, 21:07</p>"},{"location":"release-notes/#v071","title":"(v0.7.1)","text":""},{"location":"release-notes/#whats-changed_36","title":"What's Changed","text":"<ul> <li>Bump version: 0.7.0 \u2192 0.7.1 (#217) @devsetgo</li> <li>Hotfix for setup file (#216) @devsetgo</li> </ul> <p>Published Date: 2022 January 29, 01:51</p>"},{"location":"release-notes/#logging-to-beta-testing-v070","title":"Logging to Beta Testing (v0.7.0)","text":"<p>Logging is now has basic unit tests and is more ready to use with live application.</p>"},{"location":"release-notes/#whats-changed_37","title":"What's Changed","text":"<ul> <li>Adding Logging Config (#215) @devsetgo</li> <li>pip(deps): bump pre-commit from 2.15.0 to 2.16.0 (#210) @dependabot</li> <li>pip(deps): bump pylint from 2.12.1 to 2.12.2 (#211) @dependabot</li> <li>pip(deps): bump tox from 3.24.4 to 3.24.5 (#212) @dependabot</li> <li>pip(deps): bump black from 21.11b1 to 21.12b0 (#213) @dependabot</li> <li>pip(deps): bump twine from 3.6.0 to 3.7.1 (#214) @dependabot</li> <li>pip(deps): bump twine from 3.5.0 to 3.6.0 (#204) @dependabot</li> <li>pip(deps): bump coverage-badge from 1.0.2 to 1.1.0 (#205) @dependabot</li> <li>pip(deps): bump mkdocs-material from 7.3.6 to 8.0.2 (#206) @dependabot</li> <li>pip(deps): bump pylint from 2.11.1 to 2.12.1 (#207) @dependabot</li> <li>pip(deps): bump black from 21.10b0 to 21.11b1 (#208) @dependabot</li> <li>github actionts(deps): bump actions/setup-python from 2.2.2 to 2.3.1 (#209) @dependabot</li> <li>Dev (#203) @devsetgo</li> <li>pip(deps): bump tox from 3.24.3 to 3.24.4 (#193) @dependabot</li> <li>pip(deps): bump tqdm from 4.62.2 to 4.62.3 (#194) @dependabot</li> <li>pip(deps): bump pylint from 2.10.2 to 2.11.1 (#195) @dependabot</li> <li>pip(deps): bump mkdocs-material from 7.2.6 to 7.3.0 (#196) @dependabot</li> <li>pip(deps): bump black from 21.8b0 to 21.9b0 (#197) @dependabot</li> <li>pip(deps): bump mkdocs-material from 7.2.4 to 7.2.6 (#189) @dependabot</li> <li>pip(deps): bump pytest from 6.2.4 to 6.2.5 (#191) @dependabot</li> <li>pip(deps): bump watchdog from 2.1.3 to 2.1.5 (#192) @dependabot</li> <li>pip(deps): bump tox from 3.24.1 to 3.24.3 (#190) @dependabot</li> <li>pip(deps): bump pre-commit from 2.14.0 to 2.15.0 (#188) @dependabot</li> <li>pip(deps): bump black from 21.7b0 to 21.8b0 (#187) @dependabot</li> <li>pip(deps): bump pylint from 2.9.6 to 2.10.2 (#184) @dependabot</li> <li>pip(deps): bump tqdm from 4.62.0 to 4.62.2 (#185) @dependabot</li> <li>github actionts(deps): bump actions/setup-python from 1 to 2.2.2 (#182) @dependabot</li> <li>Bump wheel from 0.36.2 to 0.37.0 (#180) @dependabot</li> <li>Bump mkdocs-material from 7.2.2 to 7.2.4 (#181) @dependabot</li> <li>Bump tox from 3.24.0 to 3.24.1 (#177) @dependabot</li> <li>Bump mkdocs-material from 7.2.1 to 7.2.2 (#178) @dependabot</li> <li>Bump pre-commit from 2.13.0 to 2.14.0 (#179) @dependabot</li> <li>Bump pylint from 2.9.5 to 2.9.6 (#176) @dependabot</li> <li>Bump tqdm from 4.61.2 to 4.62.0 (#175) @dependabot</li> <li>Bump mkdocs-material from 7.1.10 to 7.2.1 (#174) @dependabot</li> <li>Bump twine from 3.4.1 to 3.4.2 (#171) @dependabot</li> <li>Bump pylint from 2.9.3 to 2.9.5 (#170) @dependabot</li> <li>Bump mkdocs from 1.2.1 to 1.2.2 (#173) @dependabot</li> <li>documentation update (#169) @devsetgo</li> <li>README fix (#168) @devsetgo</li> </ul> <p>Published Date: 2022 January 29, 01:42</p>"},{"location":"release-notes/#logging-configuration-v060","title":"Logging Configuration (v0.6.0)","text":""},{"location":"release-notes/#whats-changed_38","title":"What's Changed","text":"<ul> <li>Adding Logging and Cleanup (#167) @devsetgo</li> <li>Bump tqdm from 4.61.1 to 4.61.2 (#166) @dependabot</li> <li>Bump pylint from 2.8.3 to 2.9.3 (#165) @dependabot</li> <li>Bump watchdog from 2.1.2 to 2.1.3 (#164) @dependabot</li> <li>Bump mkdocs-material from 7.1.8 to 7.1.9 (#163) @dependabot</li> <li>Bump tqdm from 4.61.0 to 4.61.1 (#162) @dependabot</li> <li>Bump mkdocs-material from 7.1.7 to 7.1.8 (#161) @dependabot</li> <li>Bump mkdocs from 1.1.2 to 1.2.1 (#159) @dependabot</li> <li>Bump black from 21.5b2 to 21.6b0 (#158) @dependabot</li> <li>Bump mkdocs-material from 7.1.6 to 7.1.7 (#160) @dependabot</li> <li>Bump pytest-cov from 2.12.0 to 2.12.1 (#154) @dependabot</li> <li>Bump pylint from 2.8.2 to 2.8.3 (#155) @dependabot</li> <li>Bump black from 21.5b1 to 21.5b2 (#156) @dependabot</li> <li>Bump mkdocs-material from 7.1.5 to 7.1.6 (#157) @dependabot</li> <li>Bump tqdm from 4.60.0 to 4.61.0 (#153) @dependabot</li> <li>Bump pre-commit from 2.12.1 to 2.13.0 (#151) @dependabot</li> <li>Bump pytest-runner from 5.3.0 to 5.3.1 (#152) @dependabot</li> <li>Bump mkdocs-material from 7.1.4 to 7.1.5 (#150) @dependabot</li> <li>Bump watchdog from 2.1.1 to 2.1.2 (#149) @dependabot</li> <li>Bump click from 7.1.2 to 8.0.1 (#148) @dependabot</li> <li>Bump black from 21.5b0 to 21.5b1 (#147) @dependabot</li> <li>Bump watchdog from 2.1.0 to 2.1.1 (#146) @dependabot</li> <li>Bump pytest-cov from 2.11.1 to 2.12.0 (#145) @dependabot</li> <li>Bump flake8 from 3.9.1 to 3.9.2 (#143) @dependabot</li> <li>Bump pytest from 6.2.3 to 6.2.4 (#139) @dependabot</li> <li>Bump watchdog from 2.0.3 to 2.1.0 (#138) @dependabot</li> <li>Bump black from 21.4b2 to 21.5b0 (#140) @dependabot</li> <li>Bump mkdocs-material from 7.1.3 to 7.1.4 (#141) @dependabot</li> <li>Dev (#142) @devsetgo</li> <li>Bump tox from 3.23.0 to 3.23.1 (#137) @dependabot</li> <li>Bump autopep8 from 1.5.6 to 1.5.7 (#136) @dependabot</li> <li>Bump pylint from 2.7.4 to 2.8.2 (#135) @dependabot</li> <li>Bump black from 20.8b1 to 21.4b2 (#134) @dependabot</li> <li>Bump mkdocs-material from 7.1.2 to 7.1.3 (#133) @dependabot</li> <li>Adding SonarCloud Code Coverage (#130) @devsetgo</li> <li>Bump mkdocs-material from 7.1.1 to 7.1.2 (#132) @dependabot</li> <li>Bump watchdog from 2.0.2 to 2.0.3 (#131) @dependabot</li> <li>Bump pre-commit from 2.12.0 to 2.12.1 (#129) @dependabot</li> <li>Bump flake8 from 3.9.0 to 3.9.1 (#128) @dependabot</li> <li>Bump mkdocs-material from 7.1.0 to 7.1.1 (#127) @dependabot</li> <li>Bump tqdm from 4.59.0 to 4.60.0 (#124) @dependabot</li> <li>Bump pytest from 6.2.2 to 6.2.3 (#125) @dependabot</li> <li>Bump pre-commit from 2.11.1 to 2.12.0 (#126) @dependabot</li> <li>Bump pylint from 2.7.2 to 2.7.4 (#122) @dependabot</li> <li>Bump mkdocs-material from 7.0.6 to 7.1.0 (#123) @dependabot</li> <li>Bump mkdocs-material from 7.0.5 to 7.0.6 (#121) @dependabot</li> <li>Bump flake8 from 3.8.4 to 3.9.0 (#120) @dependabot</li> <li>Bump twine from 3.3.0 to 3.4.1 (#118) @dependabot</li> <li>Bump autopep8 from 1.5.5 to 1.5.6 (#119) @dependabot</li> </ul> <p>Published Date: 2021 July 16, 23:44</p>"},{"location":"release-notes/#fixing-publish-v050-2","title":"Fixing Publish (v0.5.0-2)","text":""},{"location":"release-notes/#whats-changed_39","title":"What's Changed","text":"<ul> <li>adding update for publish (#117) @devsetgo</li> </ul> <p>Published Date: 2021 March 18, 17:19</p>"},{"location":"release-notes/#calendar-and-regex-function-documentation-v050","title":"Calendar and RegEx Function + Documentation (v0.5.0)","text":""},{"location":"release-notes/#whats-changed_40","title":"What's Changed","text":"<ul> <li>Adding Calendar Functions (#116) @devsetgo</li> <li>Bump pre-commit from 2.10.1 to 2.11.1 (#113) @dependabot</li> <li>update to Saturday (#115) @devsetgo</li> <li>Bump tqdm from 4.58.0 to 4.59.0 (#112) @dependabot</li> <li>Bump mkdocs-material from 7.0.4 to 7.0.5 (#114) @dependabot</li> <li>fixes for mkdoc material update (#111) @devsetgo</li> <li>Bump tox from 3.22.0 to 3.23.0 (#109) @dependabot</li> <li>Bump mkdocs-material from 7.0.2 to 7.0.4 (#108) @dependabot</li> <li>Bump pylint from 2.7.1 to 2.7.2 (#107) @dependabot</li> <li>Bump coverage from 5.4 to 5.5 (#110) @dependabot</li> <li>Bump pylint from 2.6.2 to 2.7.1 (#103) @dependabot</li> <li>Bump mkdocs-material from 6.2.8 to 7.0.2 (#104) @dependabot</li> <li>Bump watchdog from 2.0.1 to 2.0.2 (#105) @dependabot</li> <li>Bump tqdm from 4.57.0 to 4.58.0 (#106) @dependabot</li> <li>Bump tox from 3.21.4 to 3.22.0 (#101) @dependabot</li> <li>Bump watchdog from 2.0.0 to 2.0.1 (#99) @dependabot</li> <li>Bump pylint from 2.6.0 to 2.6.2 (#102) @dependabot</li> <li>Bump tqdm from 4.56.2 to 4.57.0 (#100) @dependabot</li> <li>Bump pytest-runner from 5.2 to 5.3.0 (#98) @dependabot</li> <li>Bump tqdm from 4.56.0 to 4.56.2 (#97) @dependabot</li> <li>Bump watchdog from 1.0.2 to 2.0.0 (#96) @dependabot</li> <li>Bump pre-commit from 2.10.0 to 2.10.1 (#95) @dependabot</li> <li>Bump mkdocs-material from 6.2.6 to 6.2.8 (#94) @dependabot</li> <li>Bump tox from 3.21.3 to 3.21.4 (#93) @dependabot</li> <li>Bump autopep8 from 1.5.4 to 1.5.5 (#92) @dependabot</li> <li>Bump tox from 3.21.2 to 3.21.3 (#87) @dependabot</li> <li>Bump mkdocs-material from 6.2.5 to 6.2.6 (#88) @dependabot</li> <li>Bump pytest from 6.2.1 to 6.2.2 (#89) @dependabot</li> <li>Bump coverage from 5.3.1 to 5.4 (#91) @dependabot</li> <li>Bump pre-commit from 2.9.3 to 2.10.0 (#90) @dependabot</li> <li>Bump tox from 3.21.1 to 3.21.2 (#84) @dependabot</li> <li>Bump mkdocs-material from 6.2.4 to 6.2.5 (#85) @dependabot</li> <li>Bump pytest-cov from 2.10.1 to 2.11.1 (#86) @dependabot</li> <li>Bump tox from 3.20.1 to 3.21.1 (#81) @dependabot</li> <li>Bump mkdocs-material from 6.2.3 to 6.2.4 (#82) @dependabot</li> <li>Bump tqdm from 4.55.1 to 4.56.0 (#83) @dependabot</li> <li>Bump tqdm from 4.55.0 to 4.55.1 (#80) @dependabot</li> <li>Bump mkdocs-material from 6.2.2 to 6.2.3 (#79) @dependabot</li> </ul> <p>Published Date: 2021 March 18, 17:06</p>"},{"location":"release-notes/#minor-updates-and-library-updates-v041","title":"Minor updates and library updates. (v0.4.1)","text":""},{"location":"release-notes/#whats-changed_41","title":"What's Changed","text":"<ul> <li>Updates and Minor updates (#78) @devsetgo</li> <li>Bump tqdm from 4.54.1 to 4.55.0 (#77) @dependabot</li> <li>Bump twine from 3.2.0 to 3.3.0 (#76) @dependabot</li> <li>Bump coverage from 5.3 to 5.3.1 (#74) @dependabot</li> <li>Bump mkdocs-material from 6.1.7 to 6.2.2 (#75) @dependabot</li> <li>Bump watchdog from 0.10.4 to 1.0.2 (#73) @dependabot</li> <li>Bump pytest from 6.1.2 to 6.2.1 (#71) @dependabot</li> <li>Bump wheel from 0.36.1 to 0.36.2 (#70) @dependabot</li> <li>Bump tqdm from 4.54.0 to 4.54.1 (#67) @dependabot</li> <li>Bump mkdocs-material from 6.1.6 to 6.1.7 (#68) @dependabot</li> <li>Bump pre-commit from 2.9.2 to 2.9.3 (#69) @dependabot</li> <li>Bump wheel from 0.36.0 to 0.36.1 (#66) @dependabot</li> <li>Bump wheel from 0.35.1 to 0.36.0 (#64) @dependabot</li> <li>Bump tqdm from 4.53.0 to 4.54.0 (#65) @dependabot</li> <li>Bump pre-commit from 2.8.2 to 2.9.2 (#61) @dependabot</li> <li>Bump mkdocs-material from 6.1.5 to 6.1.6 (#60) @dependabot</li> <li>Bump tqdm from 4.52.0 to 4.53.0 (#62) @dependabot</li> <li>Bump watchdog from 0.10.3 to 0.10.4 (#63) @dependabot</li> <li>Bump tqdm from 4.51.0 to 4.52.0 (#59) @dependabot</li> <li>Bump mkdocs-material from 6.1.4 to 6.1.5 (#58) @dependabot</li> <li>Bump mkdocs-material from 6.1.2 to 6.1.4 (#57) @dependabot</li> <li>Bump pre-commit from 2.8.0 to 2.8.2 (#55) @dependabot</li> <li>Bump mkdocs-material from 6.1.0 to 6.1.2 (#56) @dependabot</li> <li>Bump pytest from 6.1.1 to 6.1.2 (#52) @dependabot</li> <li>Bump pre-commit from 2.7.1 to 2.8.0 (#53) @dependabot</li> <li>Bump tqdm from 4.50.2 to 4.51.0 (#54) @dependabot</li> <li>Bump mkdocs-material from 6.0.2 to 6.1.0 (#51) @dependabot</li> <li>Bump tqdm from 4.50.1 to 4.50.2 (#49) @dependabot</li> <li>Bump tox from 3.20.0 to 3.20.1 (#50) @dependabot</li> <li>Bump pytest from 6.1.0 to 6.1.1 (#48) @dependabot</li> <li>Bump mkdocs-material from 6.0.1 to 6.0.2 (#47) @dependabot</li> <li>Bump flake8 from 3.8.3 to 3.8.4 (#45) @dependabot</li> <li>Bump tqdm from 4.50.0 to 4.50.1 (#44) @dependabot</li> <li>Bump bump2version from 1.0.0 to 1.0.1 (#46) @dependabot</li> <li>Bump tqdm from 4.49.0 to 4.50.0 (#42) @dependabot</li> <li>Bump black from 19.10b0 to 20.8b1 (#43) @dependabot</li> <li>Bump tqdm from 4.46.0 to 4.49.0 (#40) @dependabot</li> <li>Bump pytest from 5.4.2 to 6.1.0 (#39) @dependabot</li> <li>Bump coverage from 5.1 to 5.3 (#38) @dependabot</li> <li>Bump autoflake from 1.3.1 to 1.4 (#41) @dependabot</li> <li>Bump twine from 3.1.1 to 3.2.0 (#37) @dependabot</li> <li>Bump wheel from 0.34.2 to 0.35.1 (#34) @dependabot</li> <li>Bump pytest-cov from 2.9.0 to 2.10.1 (#36) @dependabot</li> <li>Bump watchdog from 0.10.2 to 0.10.3 (#35) @dependabot</li> <li>Bump mkdocs-material from 5.2.2 to 6.0.1 (#33) @dependabot</li> <li>Bump pylint from 2.5.2 to 2.6.0 (#32) @dependabot-preview</li> <li>Bump pre-commit from 2.4.0 to 2.7.1 (#31) @dependabot-preview</li> <li>Bump tox from 3.15.1 to 3.20.0 (#30) @dependabot-preview</li> <li>Bump flake8 from 3.8.2 to 3.8.3 (#29) @dependabot-preview</li> <li>Bump autopep8 from 1.5.2 to 1.5.4 (#28) @dependabot-preview</li> </ul> <p>Published Date: 2020 December 26, 23:51</p>"},{"location":"release-notes/#040-save_csv-options-v040","title":"0.4.0 - save_csv options (v0.4.0)","text":""},{"location":"release-notes/#040-examples-and-data","title":"[0.4.0] - Examples and Data","text":""},{"location":"release-notes/#added","title":"Added","text":"<ul> <li>skipping version 0.3.0 and adding to 0.4.0</li> <li>Adding delimiter option to save_csv<ul> <li>Tests to check if delimiter &gt; 1 character</li> <li>set ',' if none</li> </ul> </li> <li>Adding quotechar option to save_csv</li> <li>Tests to check if quotechar &gt; 1 character<ul> <li>set '\"' if none</li> </ul> </li> <li>Add test of non-list to save_csv</li> </ul>"},{"location":"release-notes/#030-examples-and-data","title":"[0.3.0] - Examples and Data","text":""},{"location":"release-notes/#added_1","title":"Added","text":"<ul> <li>Adding examples (see examples folder)</li> <li>Adding file_function documentation</li> <li>Adding documents site - https://devsetgo.github.io/devsetgo_lib/</li> </ul> <p>Published Date: 2020 April 16, 21:54</p>"},{"location":"release-notes/#improvements-v020","title":"Improvements (v0.2.0)","text":"<ul> <li>Improved Tests</li> <li>Improved Errors</li> <li>Adding more logging</li> </ul> <p>Published Date: 2020 January 26, 21:08</p>"},{"location":"release-notes/#v011-v011","title":"v0.1.1 (v0.1.1)","text":"<ul> <li>New documentation</li> <li>fixes to pypi deployment</li> </ul> <p>Published Date: 2020 January 26, 17:26</p>"},{"location":"release-notes/#beta-release-v010b2","title":"Beta Release (v0.1.0b2)","text":"<p>Basic Function (file and folder) Publish to PyPi (fixing PyPi publishing issues) Needs documentation.</p> <p>Published Date: 2020 January 26, 13:03</p>"},{"location":"release-notes/#pypi-beta-release-v010b","title":"Pypi Beta Release (v0.1.0b)","text":"<p>Change to semantic versioning - Publish to Pypi - Base Functions</p> <p>Published Date: 2020 January 26, 12:53</p>"},{"location":"common_functions/calendar_functions/","title":"Reference","text":""},{"location":"common_functions/calendar_functions/#dsg_lib.common_functions.calendar_functions","title":"<code>dsg_lib.common_functions.calendar_functions</code>","text":"<p>This module provides two main functions to convert between month numbers and their corresponding names.</p> <p>Functions:</p> Name Description <code>get_month</code> <p>int) -&gt; str: Converts an integer month number to its corresponding month name.</p> <p>Args:     month (int): An integer between 1 and 12 representing the month     number.</p> <p>Returns:     str: The full name of the month corresponding to the input month     number.          If the input is not within the range of 1-12, returns \"Invalid          month number\". If the input is not an integer, returns \"Invalid          input, integer is required\".</p> <code>get_month_number</code> <p>str) -&gt; int: Converts a month name to its corresponding month number.</p> <p>Args:     month_name (str): A string containing the full name of a month.</p> <p>Returns:     int: The month number corresponding to the input month name.          If the input is not a valid month name, returns -1. If the          input is not a string, returns \"Invalid input, string is          required\".</p> <p>Example: <pre><code>from dsg_lib.common_functions.calendar_functions import get_month,\n\nget_month_number print(get_month(1))\n\n# Outputs: 'January'\n\nprint(get_month_number('January'))\n\n# Outputs: 1\n</code></pre></p> <p>This module is part of the dsg_lib package and is used for handling and converting between month numbers and names.</p> <p>Author: Mike Ryan Date: 2024/05/16 License: MIT</p>"},{"location":"common_functions/calendar_functions/#dsg_lib.common_functions.calendar_functions.get_month","title":"<code>get_month(month)</code>","text":"<p>Converts an integer month number to its corresponding month name.</p> <p>Parameters:</p> Name Type Description Default <code>month</code> <code>int</code> <p>An integer or integer-like float between 1 and 12</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The full name of the month corresponding to the input month number.  If the input is not within the range of 1-12, returns \"Invalid  month number\". If the input is not an integer or integer-like  float, returns \"Invalid input, integer is required\".</p> Source code in <code>dsg_lib/common_functions/calendar_functions.py</code> <pre><code>def get_month(month: int) -&gt; str:\n    \"\"\"\n    Converts an integer month number to its corresponding month name.\n\n    Args:\n        month (int): An integer or integer-like float between 1 and 12\n        representing the month number.\n\n    Returns:\n        str: The full name of the month corresponding to the input month number.\n             If the input is not within the range of 1-12, returns \"Invalid\n             month number\". If the input is not an integer or integer-like\n             float, returns \"Invalid input, integer is required\".\n    \"\"\"\n\n    # Define a tuple containing the names of all months\n    months = (\n        \"January\",\n        \"February\",\n        \"March\",\n        \"April\",\n        \"May\",\n        \"June\",\n        \"July\",\n        \"August\",\n        \"September\",\n        \"October\",\n        \"November\",\n        \"December\",\n    )\n\n    # Convert integer-like floats to integers\n    if isinstance(month, float) and month.is_integer():\n        month = int(month)\n\n    # Check if the input month is an integer\n    if not isinstance(month, int):\n        logger.error(\"Invalid input: %s, integer is required\", month)\n        return \"Invalid input, integer is required\"\n\n    # Check if the input month is within the range of 1-12\n    if 1 &lt;= month &lt;= 12:\n        logger.info(\"Returning month name for month number: %s\", month)\n        return months[month - 1]\n    else:\n        logger.error(\n            \"Invalid input: %s, month number should be between 1 and 12\", month\n        )\n        return \"Invalid month number\"\n</code></pre>"},{"location":"common_functions/calendar_functions/#dsg_lib.common_functions.calendar_functions.get_month_number","title":"<code>get_month_number(month_name)</code>","text":"<p>Converts a month name to its corresponding month number.</p> <p>Parameters:</p> Name Type Description Default <code>month_name</code> <code>str</code> <p>A string containing the full name of a month.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The month number corresponding to the input month name.  If the input is not a valid month name or not a string, returns -1.</p> Source code in <code>dsg_lib/common_functions/calendar_functions.py</code> <pre><code>def get_month_number(month_name: str) -&gt; int:\n    \"\"\"\n    Converts a month name to its corresponding month number.\n\n    Args:\n        month_name (str): A string containing the full name of a month.\n\n    Returns:\n        int: The month number corresponding to the input month name.\n             If the input is not a valid month name or not a string, returns -1.\n    \"\"\"\n\n    # Define a dictionary mapping month names to month numbers\n    month_dict = {\n        \"January\": 1,\n        \"February\": 2,\n        \"March\": 3,\n        \"April\": 4,\n        \"May\": 5,\n        \"June\": 6,\n        \"July\": 7,\n        \"August\": 8,\n        \"September\": 9,\n        \"October\": 10,\n        \"November\": 11,\n        \"December\": 12,\n    }\n\n    # Check if the input month name is a string\n    if not isinstance(month_name, str):\n        logger.error(\"Invalid input, string is required\")\n        return -1\n\n    # Convert the input string to title case and remove leading/trailing spaces\n    month_name = month_name.strip().title()\n\n    # Check if the input month name is a valid key in the dictionary\n    if month_name in month_dict:\n        return month_dict[month_name]\n    else:\n        logger.error(\"Invalid month name: %s\", month_name)\n        return -1\n</code></pre>"},{"location":"common_functions/file_functions/","title":"Reference","text":""},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions","title":"<code>dsg_lib.common_functions.file_functions</code>","text":"<p>file_functions.py</p> <p>This module provides a function to delete a file with a specified name from a specified directory.</p> <p>Functions:</p> Name Description <code>delete_file</code> <p>str) -&gt; str: Deletes a file with the specified file name from the directory specified by the <code>directory_to_files</code> variable. The file type is determined by the file extension, and the file is deleted from the subdirectory corresponding to the file type.</p> <p>Args:     file_name (str): The name of the file to be deleted.</p> <p>Returns:     str: A string indicating that the file has been deleted.</p> <p>Raises:     TypeError: If the file name is not a string. ValueError: If the file     name contains a forward slash or backslash, or if the file type is     not supported. FileNotFoundError: If the file does not exist.</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\nfile_functions.delete_file(\"test.csv\")\n\n# Outputs: 'complete'\n</code></pre></p> <p>Author: Mike Ryan Date: 2024/05/16 License: MIT</p>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.append_csv","title":"<code>append_csv(file_name, data, root_folder=None, delimiter=',', quotechar='\"')</code>","text":"<p>Appends a list of rows to an existing CSV file with the specified file name in the specified directory. Each element of the <code>data</code> list should be a row (list of values), and the header in <code>data[0]</code> must match the existing CSV's header.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the CSV file to append data to. Can be</p> required <code>provided</code> <code>without the '.csv' extension. data (list</code> <p>Rows to append</p> required <code>optional)</code> <p>The root directory where the file is located. If None, the</p> required <code>default</code> <code>directory is used. Defaults to None. delimiter (str</code> required <code>quotechar</code> <code>str</code> <p>The character used to quote fields in the CSV</p> <code>'\"'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Returns \"appended\" if the rows were successfully appended.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the CSV file does not exist.</p> <code>ValueError</code> <p>If the header row in <code>data</code> does not match the existing</p> <code>header in the file. TypeError</code> <p>If <code>data</code> is not a list or <code>file_name</code> is</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\ncsv_rows = [\n    [\"column1\", \"column2\"],\n    [\"appended_value1\", \"appended_value2\"]\n]\nresult = file_functions.append_csv(\n    file_name=\"test.csv\",\n    data=csv_rows,\n    root_folder=\"/path/to/directory\"\n)\n# result would be \"appended\" on success\n</code></pre></p> Additional usage info <ul> <li>Ideal for appending more rows to an existing CSV with matching header.</li> <li>Defaults to \"data/csv\" if no root_folder is provided.</li> <li>You can supply any valid file path in root_folder to override.</li> </ul> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def append_csv(\n    file_name: str,\n    data: list,\n    root_folder: str = None,\n    delimiter: str = \",\",\n    quotechar: str = '\"',\n) -&gt; str:\n    \"\"\"\n    Appends a list of rows to an existing CSV file with the specified file name\n    in the specified directory. Each element of the `data` list should be a row\n    (list of values), and the header in `data[0]` must match the existing CSV's\n    header.\n\n    Args:\n        file_name (str): The name of the CSV file to append data to. Can be\n        provided without the '.csv' extension. data (list): Rows to append\n        (list of lists), where the first row is the header. root_folder (str,\n        optional): The root directory where the file is located. If None, the\n        default directory is used. Defaults to None. delimiter (str, optional):\n        The character used to separate fields in the CSV file. Defaults to ','.\n        quotechar (str, optional): The character used to quote fields in the CSV\n        file. Defaults to '\"'.\n\n    Returns:\n        str: Returns \"appended\" if the rows were successfully appended.\n\n    Raises:\n        FileNotFoundError: If the CSV file does not exist.\n        ValueError: If the header row in `data` does not match the existing\n        header in the file. TypeError: If `data` is not a list or `file_name` is\n        not valid.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    csv_rows = [\n        [\"column1\", \"column2\"],\n        [\"appended_value1\", \"appended_value2\"]\n    ]\n    result = file_functions.append_csv(\n        file_name=\"test.csv\",\n        data=csv_rows,\n        root_folder=\"/path/to/directory\"\n    )\n    # result would be \"appended\" on success\n    ```\n\n    Additional usage info:\n        - Ideal for appending more rows to an existing CSV with matching header.\n        - Defaults to \"data/csv\" if no root_folder is provided.\n        - You can supply any valid file path in root_folder to override.\n    \"\"\"\n    target_folder = Path(root_folder) if root_folder else Path(\"data/csv\")\n    file_path = target_folder / (\n        file_name if file_name.endswith(\".csv\") else f\"{file_name}.csv\"\n    )\n\n    if not file_path.is_file():\n        raise FileNotFoundError(f\"CSV not found: {file_path}\")\n\n    if not isinstance(data, list):\n        raise TypeError(\"data must be a list of rows\")\n\n    # Read existing CSV header\n    with file_path.open(\"r\", encoding=\"utf-8\") as csv_file:\n        reader = csv.reader(csv_file, delimiter=delimiter, quotechar=quotechar)\n        existing_header = next(reader)\n\n    # Check new data's header\n    new_header = data[0]\n    if existing_header != new_header:\n        raise ValueError(\"Headers do not match. Cannot append.\")\n\n    # Append the new rows\n    with file_path.open(\"a\", encoding=\"utf-8\", newline=\"\") as csv_file:\n        writer = csv.writer(csv_file, delimiter=delimiter, quotechar=quotechar)\n        # Skip first row (header) to avoid duplication\n        writer.writerows(data[1:])\n\n    return \"appended\"\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.create_sample_files","title":"<code>create_sample_files(file_name, sample_size)</code>","text":"<p>Create sample CSV and JSON files with random data.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The base name for the sample files (without extension).</p> required <code>sample_size</code> <code>int</code> <p>The number of rows to generate for the sample files.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs while creating the sample files.</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\nfile_functions.create_sample_files(file_name=\"test.csv\", sample_size=100)\n# Creates 'test.csv' and 'test.json' each with 100 rows of random data\n</code></pre> Additional usage info:     - Creates CSV and JSON sample files with random data.     - Useful for testing or seeding databases.</p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def create_sample_files(file_name: str, sample_size: int) -&gt; None:\n    \"\"\"\n    Create sample CSV and JSON files with random data.\n\n    Args:\n        file_name (str): The base name for the sample files (without extension).\n        sample_size (int): The number of rows to generate for the sample files.\n\n    Returns:\n        None\n\n    Raises:\n        Exception: If an error occurs while creating the sample files.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    file_functions.create_sample_files(file_name=\"test.csv\", sample_size=100)\n    # Creates 'test.csv' and 'test.json' each with 100 rows of random data\n    ```\n    Additional usage info:\n        - Creates CSV and JSON sample files with random data.\n        - Useful for testing or seeding databases.\n    \"\"\"\n    logger.debug(f\"Creating sample files for {file_name} with {sample_size} rows.\")\n\n    try:\n        # Generate the CSV data\n        csv_header = [\"name\", \"birth_date\", \"number\"]\n        csv_data: List[List[str]] = [csv_header]\n\n        # Generate rows for CSV data\n        for i in range(1, sample_size + 1):\n            r_int: int = random.randint(0, len(first_name) - 1)\n            name = first_name[r_int]\n            row: List[str] = [name, generate_random_date(), str(i)]\n            csv_data.append(row)\n\n        # Save the CSV file\n        csv_file = f\"{file_name}.csv\"\n        save_csv(csv_file, csv_data)\n\n        # Generate the JSON data\n        json_data: List[dict] = []\n\n        # Generate rows for JSON data\n        for _ in range(1, sample_size + 1):\n            r_int: int = random.randint(0, len(first_name) - 1)\n            name = first_name[r_int]\n            sample_dict: dict = {\n                \"name\": name,\n                \"birthday_date\": generate_random_date(),\n            }\n            json_data.append(sample_dict)\n\n        # Save the JSON file\n        json_file: str = f\"{file_name}.json\"\n        save_json(json_file, json_data)\n\n        # Log the data\n        logger.debug(f\"CSV Data: {csv_data}\")\n        logger.debug(f\"JSON Data: {json_data}\")\n\n    except Exception as e:  # pragma: no cover\n        logger.exception(\n            f\"Error occurred while creating sample files: {e}\"\n        )  # pragma: no cover\n        raise  # pragma: no cover\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.delete_file","title":"<code>delete_file(file_name)</code>","text":"<p>Deletes a file with the specified file name from the specified directory. The file type is determined by the file extension.</p> <p>Parameters:</p> Name Type Description Default <code>directory_to_files</code> <code>str</code> <p>The directory where the file is located.</p> required <code>file_name</code> <code>str</code> <p>The name of the file to be deleted.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A message indicating whether the file has been deleted successfully</p> <code>str</code> <p>or an error occurred.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the directory or file name is not a string. ValueError: If</p> <code>is not supported. FileNotFoundError</code> <p>If the file does not exist.</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\nfile_functions.delete_file(file_name=\"test.csv\")\n\n# Outputs: 'complete'\n</code></pre> Additional usage info:     - Returns \"complete\" if file is successfully deleted.     - Ensure correct file permissions and directory structure.</p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def delete_file(file_name: str) -&gt; str:\n    \"\"\"\n    Deletes a file with the specified file name from the specified directory.\n    The file type is determined by the file extension.\n\n    Args:\n        directory_to_files (str): The directory where the file is located.\n        file_name (str): The name of the file to be deleted.\n\n    Returns:\n        str: A message indicating whether the file has been deleted successfully\n        or an error occurred.\n\n    Raises:\n        TypeError: If the directory or file name is not a string. ValueError: If\n        the file name contains a forward slash or backslash, or if the file type\n        is not supported. FileNotFoundError: If the file does not exist.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    file_functions.delete_file(file_name=\"test.csv\")\n\n    # Outputs: 'complete'\n    ```\n    Additional usage info:\n        - Returns \"complete\" if file is successfully deleted.\n        - Ensure correct file permissions and directory structure.\n    \"\"\"\n    logger.info(f\"Deleting file: {file_name}\")\n\n    # Check that the file name is a string\n    if not isinstance(file_name, str):\n        raise TypeError(f\"{file_name} is not a valid string\")\n\n    # Split the file name into its name and extension components\n    file_name, file_ext = os.path.splitext(file_name)\n\n    # Check that the file name does not contain a forward slash or backslash\n    if os.path.sep in file_name:\n        raise ValueError(f\"{file_name} cannot contain {os.path.sep}\")\n\n    # Check that the file type is supported\n    if file_ext not in directory_map:\n        raise ValueError(\n            f\"unsupported file type: {file_ext}. Supported file types are: {', '.join(directory_map.keys())}\"\n        )\n\n    # Construct the full file path\n    file_directory = Path.cwd() / directory_to_files / directory_map[file_ext]\n    file_path = file_directory / f\"{file_name}{file_ext}\"\n\n    # Check that the file exists\n    if not file_path.is_file():\n        raise FileNotFoundError(f\"file not found: {file_name}{file_ext}\")\n\n    # Delete the file\n    os.remove(file_path)\n    logger.info(f\"File {file_name}{file_ext} deleted from file path: {file_path}\")\n\n    # Return a string indicating that the file has been deleted\n    return \"complete\"\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.generate_random_date","title":"<code>generate_random_date()</code>","text":"<p>Generate a random datetime string in the format yyyy-mm-dd hh:mm:ss.ffffff.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A randomly generated datetime string.</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\nrandom_timestamp = file_functions.generate_random_date()\n# random_timestamp might look like '1992-03-15 10:30:45.123456'\n</code></pre> Additional usage info:     - Can help produce test data with random timestamps.</p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def generate_random_date() -&gt; str:\n    \"\"\"\n    Generate a random datetime string in the format yyyy-mm-dd hh:mm:ss.ffffff.\n\n    Returns:\n        str: A randomly generated datetime string.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    random_timestamp = file_functions.generate_random_date()\n    # random_timestamp might look like '1992-03-15 10:30:45.123456'\n    ```\n    Additional usage info:\n        - Can help produce test data with random timestamps.\n    \"\"\"\n    # Define the minimum and maximum years for the date range\n    min_year: int = 1905\n    max_year: int = datetime.now().year\n\n    # Generate random values for the year, month, day, hour, minute, and second\n    year: int = random.randrange(min_year, max_year + 1)\n    month: int = random.randint(1, 12)\n    day: int = random.randint(1, 28)\n    hour: int = random.randint(0, 12)\n    minute: int = random.randint(0, 59)\n    second: int = random.randint(0, 59)\n\n    # Create a datetime object with the random values\n    date_value: datetime = datetime(year, month, day, hour, minute, second)\n\n    # Format the datetime string and return it\n    return f\"{date_value:%Y-%m-%d %H:%M:%S.%f}\"\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.open_csv","title":"<code>open_csv(file_name, delimiter=',', quote_level='minimal', skip_initial_space=True, **kwargs)</code>","text":"<p>Opens a CSV file with the specified file name and returns its contents as a list of dictionaries.</p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def open_csv(\n    file_name: str,\n    delimiter: str = \",\",\n    quote_level: str = \"minimal\",\n    skip_initial_space: bool = True,\n    **kwargs,\n) -&gt; list:\n    \"\"\"\n    Opens a CSV file with the specified file name and returns its contents\n    as a list of dictionaries.\n    \"\"\"\n    # A dictionary that maps quote levels to csv quoting constants\n    quote_levels = {\n        \"none\": csv.QUOTE_NONE,\n        \"minimal\": csv.QUOTE_MINIMAL,\n        \"all\": csv.QUOTE_ALL,\n    }\n    # Check that file name is a string\n    if not isinstance(file_name, str):\n        error = f\"{file_name} is not a valid string\"\n        logger.error(error)\n        raise TypeError(error)\n\n    # Check delimiter is single character\n    if len(delimiter) != 1:\n        raise TypeError(f\"{delimiter} can only be a single character\")\n\n    # Reject any 'quotechar' usage for now\n    if \"quotechar\" in kwargs:\n        raise TypeError(\"quotechar is not supported in open_csv\")\n\n    # Validate quote_level\n    quote_level = quote_level.lower()\n    if quote_level not in quote_levels:\n        error = f\"Invalid quote level: {quote_level}. Valid levels are: {', '.join(quote_levels)}\"\n        logger.error(error)\n        raise ValueError(error)\n    quoting = quote_levels[quote_level]\n\n    file_directory = Path.cwd().joinpath(directory_to_files).joinpath(\"csv\")\n    file_path = file_directory.joinpath(file_name)\n\n    if not file_path.is_file():\n        error = f\"File not found: {file_path}\"\n        logger.error(error)\n        raise FileNotFoundError(error)\n\n    data = []\n    with file_path.open(encoding=\"utf-8\") as f:\n        reader = csv.DictReader(\n            f,\n            delimiter=delimiter,\n            quoting=quoting,\n            skipinitialspace=skip_initial_space,\n        )\n        for row in reader:\n            data.append(dict(row))\n\n    logger.info(f\"File opened: {file_name}\")\n    return data\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.open_json","title":"<code>open_json(file_name)</code>","text":"<p>Open a JSON file and load its contents into a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the JSON file to open.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The contents of the JSON file as a dictionary.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the file name is not a string. FileNotFoundError: If the</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\nresult_dict = file_functions.open_json(file_name=\"test.json\")\n# result_dict is a dictionary loaded from 'test.json'\n</code></pre> Additional usage info:     - Returns a dictionary loaded from the JSON file.     - Commonly used for reading app settings or user data.</p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def open_json(file_name: str) -&gt; dict:\n    \"\"\"\n    Open a JSON file and load its contents into a dictionary.\n\n    Args:\n        file_name (str): The name of the JSON file to open.\n\n    Returns:\n        dict: The contents of the JSON file as a dictionary.\n\n    Raises:\n        TypeError: If the file name is not a string. FileNotFoundError: If the\n        file does not exist.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    result_dict = file_functions.open_json(file_name=\"test.json\")\n    # result_dict is a dictionary loaded from 'test.json'\n    ```\n    Additional usage info:\n        - Returns a dictionary loaded from the JSON file.\n        - Commonly used for reading app settings or user data.\n    \"\"\"\n    # Check if file name is a string\n    if not isinstance(file_name, str):\n        error = f\"{file_name} is not a valid string\"\n        logger.error(error)\n        raise TypeError(error)\n\n    file_directory = Path(directory_to_files) / directory_map[\".json\"]\n    file_save = file_directory / file_name\n\n    # Check if path correct\n    if not file_save.is_file():\n        error = f\"file not found error: {file_save}\"\n        logger.exception(error)\n        raise FileNotFoundError(error)\n\n    # open file\n    with open(file_save) as read_file:\n        # load file into data variable\n        result = json.load(read_file)\n\n    logger.info(f\"File Opened: {file_name}\")\n    return result\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.open_text","title":"<code>open_text(file_name)</code>","text":"<p>Opens a text file with the specified file name and returns its contents as a string.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to open. Should include the '.txt'</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The contents of the text file as a string.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the <code>file_name</code> parameter is not a string or contains a</p> <code>forward slash. FileNotFoundError</code> <p>If the file does not exist.</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\ntext_content = file_functions.open_text(file_name=\"test.txt\")\n# text_content is the file's contents as a single string\n</code></pre> Additional usage info:     - Retrieves contents as a single string.     - Handy for reading simple text resources or logs.</p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def open_text(file_name: str) -&gt; str:\n    \"\"\"\n    Opens a text file with the specified file name and returns its contents as a\n    string.\n\n    Args:\n        file_name (str): The name of the file to open. Should include the '.txt'\n        extension.\n\n    Returns:\n        str: The contents of the text file as a string.\n\n    Raises:\n        TypeError: If the `file_name` parameter is not a string or contains a\n        forward slash. FileNotFoundError: If the file does not exist.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    text_content = file_functions.open_text(file_name=\"test.txt\")\n    # text_content is the file's contents as a single string\n    ```\n    Additional usage info:\n        - Retrieves contents as a single string.\n        - Handy for reading simple text resources or logs.\n    \"\"\"\n    # Replace backslashes with forward slashes in the file name\n    if \"\\\\\" in file_name:  # pragma: no cover\n        file_name = file_name.replace(\"\\\\\", \"/\")  # pragma: no cover\n\n    # Check that file_name does not contain invalid characters\n    if \"/\" in file_name:\n        logger.error(f\"{file_name} cannot contain /\")\n        raise TypeError(f\"{file_name} cannot contain /\")\n\n    # Get the path to the text directory and the file path\n    file_directory = os.path.join(directory_to_files, \"text\")\n    file_path = Path.cwd().joinpath(file_directory, file_name)\n\n    # Check if the file exists\n    if not file_path.is_file():\n        raise FileNotFoundError(f\"file not found error: {file_path}\")\n\n    # Open the file and read the data\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        data = file.read()\n\n    logger.info(f\"File opened: {file_path}\")\n    return data\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.save_csv","title":"<code>save_csv(file_name, data, root_folder=None, delimiter=',', quotechar='\"')</code>","text":"<p>Saves a list of dictionaries as a CSV file with the specified file name in the specified directory. Each dictionary in the list should represent a row in the CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to save the data in. Should</p> required <code>include</code> <code>the '.csv' extension. data (list</code> <p>The data to be saved. Each</p> required <code>optional)</code> <p>The root directory where the file will be saved. If None, the</p> required <code>(str,</code> <code>optional</code> <p>The character used to separate fields in the CSV file.</p> required <code>Defaults</code> <code>to ','. quotechar (str</code> <p>The character used to quote</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A message indicating whether the file has been saved successfully</p> <code>str</code> <p>or an error occurred.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the data is not a list, or the file name, delimiter, or</p> <code>quotechar is not a string. ValueError</code> <p>If the file name does not end</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\ncsv_data = [\n    [\"column1\", \"column2\"],\n    [\"value1\", \"value2\"]\n]\nfile_functions.save_csv(file_name=\"test.csv\", data=csv_data,root_folder=\"/path/to/directory\", delimiter=\";\", quotechar=\"'\")\n\n# Saves data to '/path/to/directory/test.csv'\n</code></pre> Additional usage info:     - Ideal for exporting data for spreadsheet analysis.     - Returns \"complete\" if file is saved successfully. Additional clarification:     - Defaults to \"data/csv\" if no root_folder is provided.     - You can supply any valid file path in root_folder to override.</p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def save_csv(\n    file_name: str,\n    data: list,\n    root_folder: str = None,\n    delimiter: str = \",\",\n    quotechar: str = '\"',\n) -&gt; str:\n    \"\"\"\n    Saves a list of dictionaries as a CSV file with the specified file name in\n    the specified directory. Each dictionary in the list should represent a row\n    in the CSV file.\n\n    Args:\n        file_name (str): The name of the file to save the data in. Should\n        include the '.csv' extension. data (list): The data to be saved. Each\n        element of the list should be a dictionary where the keys are column\n        names and the values are the data for those columns. root_folder (str,\n        optional): The root directory where the file will be saved. If None, the\n        file will be saved in the current directory. Defaults to None. delimiter\n        (str, optional): The character used to separate fields in the CSV file.\n        Defaults to ','. quotechar (str, optional): The character used to quote\n        fields in the CSV file. Defaults to '\"'.\n\n    Returns:\n        str: A message indicating whether the file has been saved successfully\n        or an error occurred.\n\n    Raises:\n        TypeError: If the data is not a list, or the file name, delimiter, or\n        quotechar is not a string. ValueError: If the file name does not end\n        with '.csv'.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    csv_data = [\n        [\"column1\", \"column2\"],\n        [\"value1\", \"value2\"]\n    ]\n    file_functions.save_csv(file_name=\"test.csv\", data=csv_data,root_folder=\"/path/to/directory\", delimiter=\";\", quotechar=\"'\")\n\n    # Saves data to '/path/to/directory/test.csv'\n    ```\n    Additional usage info:\n        - Ideal for exporting data for spreadsheet analysis.\n        - Returns \"complete\" if file is saved successfully.\n    Additional clarification:\n        - Defaults to \"data/csv\" if no root_folder is provided.\n        - You can supply any valid file path in root_folder to override.\n    \"\"\"\n    target_folder = Path(root_folder) if root_folder else Path(\"data/csv\")\n    target_folder.mkdir(parents=True, exist_ok=True)\n\n    # Check that delimiter and quotechar are single characters\n    if len(delimiter) != 1:\n        raise TypeError(f\"{delimiter} can only be a single character\")\n    if len(quotechar) != 1:\n        raise TypeError(f\"{quotechar} can only be a single character\")\n\n    # Check that data is a list\n    if not isinstance(data, list):\n        raise TypeError(f\"{data} is not a valid list\")\n\n    # Check that file_name is a string and does not contain invalid characters\n    if not isinstance(file_name, str) or \"/\" in file_name or \"\\\\\" in file_name:\n        raise TypeError(f\"{file_name} is not a valid file name\")\n\n    # Add extension to file_name if needed\n    if not file_name.endswith(\".csv\"):\n        file_name += \".csv\"\n\n    # Create the file path\n    file_path = target_folder / file_name\n\n    # Write data to file\n    with open(file_path, \"w\", encoding=\"utf-8\", newline=\"\") as csv_file:\n        csv_writer = csv.writer(csv_file, delimiter=delimiter, quotechar=quotechar)\n        csv_writer.writerows(data)\n\n    logger.info(f\"File Create: {file_name}\")\n    return \"complete\"\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.save_json","title":"<code>save_json(file_name, data, root_folder=None)</code>","text":"<p>Saves a dictionary or a list as a JSON file with the specified file name in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to save the data in. Should</p> required <code>include</code> <code>the '.json' extension. data (list or dict</code> <p>The data to be</p> required <code>saved.</code> <code>root_folder (str</code> <p>The root directory where the file</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A message indicating whether the file has been saved successfully</p> <code>str</code> <p>or an error occurred.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the data is not a list or a dictionary, or the file name</p> <code>or directory is not a string. ValueError</code> <p>If the file name contains a</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\njson_data = {\"key\": \"value\"}\nfile_functions.save_json(file_name=\"test.json\", data=json_data, root_folder=\"/path/to/directory\")\n\n# Saves data to '/path/to/directory/test.json'\n</code></pre> Additional usage info:     - Suitable for config files, logs, or structured application data.     - Returns \"File saved successfully\" on success. Additional clarification:     - Defaults to \"data/json\" if no root_folder is provided.     - You can supply any valid file path in root_folder to override.</p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def save_json(file_name: str, data, root_folder: str = None) -&gt; str:\n    \"\"\"\n    Saves a dictionary or a list as a JSON file with the specified file name in\n    the specified directory.\n\n    Args:\n        file_name (str): The name of the file to save the data in. Should\n        include the '.json' extension. data (list or dict): The data to be\n        saved. root_folder (str, optional): The root directory where the file\n        will be saved. Defaults to None, which means the file will be saved in\n        the 'data' directory.\n\n    Returns:\n        str: A message indicating whether the file has been saved successfully\n        or an error occurred.\n\n    Raises:\n        TypeError: If the data is not a list or a dictionary, or the file name\n        or directory is not a string. ValueError: If the file name contains a\n        forward slash or backslash, or if the file name does not end with\n        '.json'.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    json_data = {\"key\": \"value\"}\n    file_functions.save_json(file_name=\"test.json\", data=json_data, root_folder=\"/path/to/directory\")\n\n    # Saves data to '/path/to/directory/test.json'\n    ```\n    Additional usage info:\n        - Suitable for config files, logs, or structured application data.\n        - Returns \"File saved successfully\" on success.\n    Additional clarification:\n        - Defaults to \"data/json\" if no root_folder is provided.\n        - You can supply any valid file path in root_folder to override.\n    \"\"\"\n    try:\n        if not isinstance(data, (list, dict)):\n            raise TypeError(\n                f\"data must be a list or a dictionary instead of type {type(data)}\"\n            )\n        if \"/\" in file_name or \"\\\\\" in file_name:\n            raise ValueError(f\"{file_name} cannot contain / or \\\\\")\n\n        if not file_name.endswith(\".json\"):\n            file_name += \".json\"\n\n        target_folder = Path(root_folder) if root_folder else Path(\"data/json\")\n        # Create the target folder if it doesn't exist\n        target_folder.mkdir(parents=True, exist_ok=True)\n\n        file_path = target_folder / file_name\n\n        with open(file_path, \"w\") as write_file:\n            json.dump(data, write_file)\n\n        logger.info(f\"File created: {file_path}\")\n        return \"File saved successfully\"\n\n    except (TypeError, ValueError) as e:\n        logger.error(f\"Error creating file {file_name}: {e}\")\n        raise\n</code></pre>"},{"location":"common_functions/file_functions/#dsg_lib.common_functions.file_functions.save_text","title":"<code>save_text(file_name, data, root_folder=None)</code>","text":"<p>Saves a string of text to a file with the specified file name in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the file to save the data in. Should not</p> required <code>include</code> <code>the '.txt' extension. data (str</code> <p>The text data to be saved.</p> required <code>root_folder</code> <code>str</code> <p>The root directory where the file will be</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A message indicating whether the file has been saved successfully</p> <code>str</code> <p>or an error occurred.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the <code>data</code> parameter is not a string, or the <code>file_name</code></p> <code>contains a forward slash or backslash. FileNotFoundError</code> <p>If the</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\ntext_data = \"This is a test text file.\"\nfile_functions.save_text(file_name=\"test.txt\", data=text_data, root_folder=\"/path/to/directory\")\n\n# Saves data to '/path/to/directory/test.txt'\n</code></pre> Additional usage info:     - Writes plain text data to a .txt file.     - Returns \"complete\" on success. Additional clarification:     - Defaults to \"data/text\" if no root_folder is provided.     - You can supply any valid file path in root_folder to override.</p> Source code in <code>dsg_lib/common_functions/file_functions.py</code> <pre><code>def save_text(file_name: str, data: str, root_folder: str = None) -&gt; str:\n    \"\"\"\n    Saves a string of text to a file with the specified file name in the\n    specified directory.\n\n    Args:\n        file_name (str): The name of the file to save the data in. Should not\n        include the '.txt' extension. data (str): The text data to be saved.\n        root_folder (str, optional): The root directory where the file will be\n        saved. If None, the file will be saved in the current directory.\n        Defaults to None.\n\n    Returns:\n        str: A message indicating whether the file has been saved successfully\n        or an error occurred.\n\n    Raises:\n        TypeError: If the `data` parameter is not a string, or the `file_name`\n        contains a forward slash or backslash. FileNotFoundError: If the\n        directory does not exist.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    text_data = \"This is a test text file.\"\n    file_functions.save_text(file_name=\"test.txt\", data=text_data, root_folder=\"/path/to/directory\")\n\n    # Saves data to '/path/to/directory/test.txt'\n    ```\n    Additional usage info:\n        - Writes plain text data to a .txt file.\n        - Returns \"complete\" on success.\n    Additional clarification:\n        - Defaults to \"data/text\" if no root_folder is provided.\n        - You can supply any valid file path in root_folder to override.\n    \"\"\"\n    # If no root folder is provided, use the default directory\n    if root_folder is None:  # pragma: no cover\n        root_folder = directory_to_files  # pragma: no cover\n\n    # Determine the directory for text files\n    text_directory = Path(root_folder) / \"text\"\n\n    # Construct the file path for text files\n    file_path = text_directory / file_name\n\n    # Create the text directory if it does not exist\n    text_directory.mkdir(parents=True, exist_ok=True)\n\n    # Check that data is a string and that file_name does not contain invalid\n    # characters\n    if not isinstance(data, str):\n        logger.error(f\"{file_name} is not a valid string\")\n        raise TypeError(f\"{file_name} is not a valid string\")\n    elif \"/\" in file_name or \"\\\\\" in file_name:\n        logger.error(f\"{file_name} cannot contain \\\\ or /\")\n        raise ValueError(f\"{file_name} cannot contain \\\\ or /\")\n\n    # Add extension to file_name if needed\n    if not file_name.endswith(\".txt\"):\n        file_name += \".txt\"\n    # Open or create the file and write the data\n    with open(file_path, \"w+\", encoding=\"utf-8\") as file:\n        file.write(data)\n\n    logger.info(f\"File created: {file_path}\")\n    return \"complete\"\n</code></pre>"},{"location":"common_functions/file_mover/","title":"Reference","text":""},{"location":"common_functions/file_mover/#dsg_lib.common_functions.file_mover","title":"<code>dsg_lib.common_functions.file_mover</code>","text":"<p>Module: file_mover Detailed file processing flow that continuously monitors and processes files from a source directory, optionally compresses them, and then moves them to a final destination. Ensures no files are lost during transfer.</p> <p>Functions:</p> Name Description <code>) -&gt; None</code> <p>Continuously monitors the source directory for files matching the given pattern, moves them to a temporary directory, optionally compresses them, and then transfers them to the final directory.</p> <code>_process_file</code> <p>Path, temp_path: Path, final_path: Path, compress: bool) -&gt; None: Handles the internal logic of moving and optionally compressing a single file.</p> <p>Usage Example: <pre><code>from dsg_lib.common_functions.file_mover import process_files_flow\n\nprocess_files_flow(\n    source_dir=\"/some/source\",\n    temp_dir=\"/some/temp\",\n    final_dir=\"/some/final\",\n    file_pattern=\"*.txt\",\n    compress=True\n)\n</code></pre></p>"},{"location":"common_functions/file_mover/#dsg_lib.common_functions.file_mover.process_files_flow","title":"<code>process_files_flow(source_dir, temp_dir, final_dir, file_pattern, compress=False, max_iterations=None)</code>","text":"<p>Continuously monitors a source directory for files. Moves files matching file_pattern to a temporary directory, optionally compresses them, then moves them to a final destination directory.</p> <p>Parameters:</p> Name Type Description Default <code>source_dir</code> <code>str</code> <p>Path to the source directory to watch.</p> required <code>temp_dir</code> <code>str</code> <p>Path to the temporary directory for processing.</p> required <code>final_dir</code> <code>str</code> <p>Path to the final destination directory.</p> required <code>file_pattern</code> <code>str</code> <p>Glob pattern for matching files (e.g. \"*.txt\").</p> required <code>compress</code> <code>bool</code> <p>If True, compress files before moving. Defaults to False.</p> <code>False</code> <code>max_iterations</code> <code>Optional[int]</code> <p>Limit iterations in watch loop. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>Exception</code> <p>Propagated if file operations fail.</p> Example <p>process_files_flow(\"/source\", \"/temp\", \"/final\", \"*.pdf\", compress=True)</p> Source code in <code>dsg_lib/common_functions/file_mover.py</code> <pre><code>def process_files_flow(\n    source_dir: str,\n    temp_dir: str,\n    final_dir: str,\n    file_pattern: str,\n    compress: bool = False,\n    max_iterations: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Continuously monitors a source directory for files. Moves files matching\n    file_pattern to a temporary directory, optionally compresses them, then\n    moves them to a final destination directory.\n\n    Args:\n        source_dir (str): Path to the source directory to watch.\n        temp_dir (str): Path to the temporary directory for processing.\n        final_dir (str): Path to the final destination directory.\n        file_pattern (str): Glob pattern for matching files (e.g. \"*.txt\").\n        compress (bool, optional): If True, compress files before moving. Defaults to False.\n        max_iterations (Optional[int], optional): Limit iterations in watch loop. Defaults to None.\n\n    Returns:\n        None\n\n    Raises:\n        Exception: Propagated if file operations fail.\n\n    Example:\n        process_files_flow(\"/source\", \"/temp\", \"/final\", \"*.pdf\", compress=True)\n    \"\"\"\n    temp_path: Path = Path(temp_dir)\n    final_path: Path = Path(final_dir)\n    source_path: Path = Path(source_dir)\n\n    # Ensure temporary and final directories exist.\n    for path in (temp_path, final_path):\n        path.mkdir(parents=True, exist_ok=True)\n\n    # Process existing files in the source directory at startup\n    logger.info(f\"Processing existing files in source directory: {source_dir}\")\n    for file in source_path.glob(file_pattern):\n        if file.is_file():\n            try:\n                logger.info(f\"Processing existing file: {file}\")\n                _process_file(file, temp_path, final_path, compress)\n            except Exception as e:\n                logger.error(f\"Error processing existing file '{file}': {e}\")\n                raise\n\n    # The clear_source deletion block has been removed so that files remain in the source directory\n    # if they have not already been processed.\n\n    logger.info(\n        f\"Starting file processing flow: monitoring '{source_dir}' for pattern '{file_pattern}'.\"\n    )\n\n    # Monitor the source directory for changes\n    changes_generator: Generator[Set[Tuple[int, str]], None, None] = watch(source_dir)\n    if max_iterations is not None:\n        changes_generator = islice(changes_generator, max_iterations)\n\n    for changes in changes_generator:\n        logger.debug(f\"Detected changes: {changes}\")\n        for _change_type, file_str in changes:\n            file_path: Path = Path(file_str)\n            # Only process files matching the pattern and that are files\n            if file_path.is_file() and file_path.match(file_pattern):\n                try:\n                    logger.info(f\"Detected file for processing: {file_path}\")\n                    _process_file(file_path, temp_path, final_path, compress)\n                except Exception as e:\n                    logger.error(f\"Error processing file '{file_path}': {e}\")\n                    raise\n        sleep(1)  # Small delay to minimize CPU usage\n</code></pre>"},{"location":"common_functions/folder_functions/","title":"Reference","text":""},{"location":"common_functions/folder_functions/#dsg_lib.common_functions.folder_functions","title":"<code>dsg_lib.common_functions.folder_functions</code>","text":"<p>This module contains functions for working with directories and files.</p> <p>Functions:</p> Name Description <code>last_data_files_changed</code> <p>Get the last modified file in a</p> <code>get_directory_list</code> <p>Get a list of directories in the</p> <code>specified directory. make_folder</code> <p>Make a folder in a</p> <code>specific directory. remove_folder</code> <p>Remove a folder from the</p> <p>Example: <pre><code>from dsg_lib.common_functions import folder_functions\n\n# Get the last modified file in a directory time_stamp, file_path =\nfolder_functions.last_data_files_changed(\"/path/to/directory\")  # Returns:\n(datetime.datetime(2022, 1, 1, 12, 0, 0), '/path/to/directory/test.txt')\n\n# Get a list of directories in the specified directory directories =\nfolder_functions.get_directory_list(\"/path/to/directory\")  # Returns:\n['/path/to/directory/dir1', '/path/to/directory/dir2']\n\n# Make a folder in a specific directory\nfolder_functions.make_folder(\"/path/to/directory/new_folder\")  # Creates a new\nfolder at '/path/to/directory/new_folder'\n\n# Remove a folder from the specified directory\nfolder_functions.remove_folder(\"/path/to/directory/old_folder\")  # Removes the\nfolder at '/path/to/directory/old_folder'\n</code></pre></p> <p>Author: Mike Ryan Date: 2024/05/16 License: MIT</p>"},{"location":"common_functions/folder_functions/#dsg_lib.common_functions.folder_functions.get_directory_list","title":"<code>get_directory_list(file_directory)</code>","text":"<p>Get a list of directories in the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>file_directory</code> <code>str</code> <p>The path of the directory to check.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of directories in the specified directory.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the directory does not exist.</p> <p>Example: <pre><code>from dsg_lib import file_functions\n\ndirectories = file_functions.get_directory_list(\"/path/to/directory\")\n\n# Returns: ['/path/to/directory/dir1', '/path/to/directory/dir2']\n</code></pre></p> Source code in <code>dsg_lib/common_functions/folder_functions.py</code> <pre><code>def get_directory_list(file_directory: str) -&gt; List[str]:\n    \"\"\"\n    Get a list of directories in the specified directory.\n\n    Args:\n        file_directory (str): The path of the directory to check.\n\n    Returns:\n        List[str]: A list of directories in the specified directory.\n\n    Raises:\n        FileNotFoundError: If the directory does not exist.\n\n    Example:\n    ```python\n    from dsg_lib import file_functions\n\n    directories = file_functions.get_directory_list(\"/path/to/directory\")\n\n    # Returns: ['/path/to/directory/dir1', '/path/to/directory/dir2']\n    ```\n    \"\"\"\n    # Create a Path object for the specified directory\n    file_path = Path.cwd().joinpath(file_directory)\n\n    try:\n        # Use a list comprehension to create a list of directories in the\n        # specified directory\n        direct_list = [x for x in file_path.iterdir() if x.is_dir()]\n\n        # Log a message indicating that the list of directories was retrieved\n        logger.info(f\"Retrieved list of directories: {file_directory}\")\n\n        # Return the list of directories\n        return direct_list\n\n    except FileNotFoundError as err:\n        # Log an error message if the specified directory does not exist\n        logger.error(err)\n</code></pre>"},{"location":"common_functions/folder_functions/#dsg_lib.common_functions.folder_functions.last_data_files_changed","title":"<code>last_data_files_changed(directory_path)</code>","text":"<p>Get the last modified file in a directory and return its modification time and path.</p> <p>Parameters:</p> Name Type Description Default <code>directory_path</code> <code>str</code> <p>The path of the directory to check.</p> required <p>Returns:</p> Type Description <code>datetime</code> <p>Tuple[datetime, str]: A tuple containing the modification time and path</p> <code>str</code> <p>of the last modified file.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the directory does not exist.</p> <p>Example: <pre><code>from dsg_lib import file_functions\n\ntime_stamp, file_path = file_functions.last_data_files_changed(\"/path/to/directory\")\n\n# Returns: (datetime.datetime(2022, 1, 1, 12, 0, 0), '/path/to/directory/test.txt')\n</code></pre></p> Source code in <code>dsg_lib/common_functions/folder_functions.py</code> <pre><code>def last_data_files_changed(directory_path: str) -&gt; Tuple[datetime, str]:\n    \"\"\"\n    Get the last modified file in a directory and return its modification time\n    and path.\n\n    Args:\n        directory_path (str): The path of the directory to check.\n\n    Returns:\n        Tuple[datetime, str]: A tuple containing the modification time and path\n        of the last modified file.\n\n    Raises:\n        FileNotFoundError: If the directory does not exist.\n\n    Example:\n    ```python\n    from dsg_lib import file_functions\n\n    time_stamp, file_path = file_functions.last_data_files_changed(\"/path/to/directory\")\n\n    # Returns: (datetime.datetime(2022, 1, 1, 12, 0, 0), '/path/to/directory/test.txt')\n    ```\n    \"\"\"\n    try:\n        # Use a generator expression to find the last modified file in the\n        # directory\n        time, file_path = max((f.stat().st_mtime, f) for f in directory_path.iterdir())\n\n        # Convert the modification time to a datetime object\n        time_stamp = datetime.fromtimestamp(time)\n\n        # Log a message to indicate that the directory was checked for the last\n        # modified file\n        logger.info(f\"Directory checked for last change: {directory_path}\")\n\n        # Return the modification time and path of the last modified file\n        return time_stamp, file_path\n\n    except Exception as err:\n        # Log an error message if an exception occurs, and return a default\n        # value to indicate an error\n        logger.error(err)\n        return None, None\n</code></pre>"},{"location":"common_functions/folder_functions/#dsg_lib.common_functions.folder_functions.remove_folder","title":"<code>remove_folder(file_directory)</code>","text":"<p>Remove a folder from the specified directory.</p> <p>Parameters:</p> Name Type Description Default <code>file_directory</code> <code>str</code> <p>The directory containing the folder to be removed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified directory does not exist. OSError:</p> <p>Example: <pre><code>from dsg_lib.common_functions import file_functions\n\nfile_functions.remove_folder(\"/path/to/directory/old_folder\")\n\n# Removes the folder at '/path/to/directory/old_folder'\n</code></pre></p> Source code in <code>dsg_lib/common_functions/folder_functions.py</code> <pre><code>def remove_folder(file_directory: str) -&gt; None:\n    \"\"\"\n    Remove a folder from the specified directory.\n\n    Args:\n        file_directory (str): The directory containing the folder to be removed.\n\n    Returns:\n        None.\n\n    Raises:\n        FileNotFoundError: If the specified directory does not exist. OSError:\n        If the specified folder could not be removed.\n\n    Example:\n    ```python\n    from dsg_lib.common_functions import file_functions\n\n    file_functions.remove_folder(\"/path/to/directory/old_folder\")\n\n    # Removes the folder at '/path/to/directory/old_folder'\n    ```\n    \"\"\"\n    try:\n        # Create a Path object for the specified directory\n        path = Path(file_directory)\n\n        # Use the rmdir method of the Path object to remove the folder\n        path.rmdir()\n\n        # Log a message indicating that the folder was removed\n        logger.info(f\"Folder removed: {file_directory}\")\n\n    except FileNotFoundError as err:\n        # Log an error message if the specified directory does not exist\n        logger.error(err)\n\n        # Raise the FileNotFoundError exception to be handled by the calling\n        # code\n        raise\n\n    except OSError as err:\n        # Log an error message if the folder could not be removed\n        logger.error(err)\n\n        # Raise the OSError exception to be handled by the calling code\n        raise\n</code></pre>"},{"location":"common_functions/logging/","title":"Reference","text":""},{"location":"common_functions/logging/#dsg_lib.common_functions.logging_config","title":"<code>dsg_lib.common_functions.logging_config</code>","text":"<p>This module provides a comprehensive logging setup using the loguru library, facilitating easy logging management for Python applications.</p> <p>The <code>config_log</code> function, central to this module, allows for extensive customization of logging behavior. It supports specifying the logging directory, log file name, logging level, and controls for log rotation, retention, and formatting among other features. Additionally, it offers advanced options like backtrace and diagnose for in-depth debugging, the ability to append the application name to the log file for clearer identification, and control over logger propagation via the <code>log_propagate</code> parameter.</p> <p>Usage example:</p> <pre><code>from dsg_lib.common_functions.logging_config import config_log\n\nconfig_log(\n    logging_directory='logs',  # Directory for storing logs\n    log_name='log',  # Base name for log files\n    logging_level='DEBUG',  # Minimum logging level\n    log_rotation='100 MB',  # Size threshold for log rotation\n    log_retention='30 days',  # Duration to retain old log files\n    enqueue=True,  # Enqueue log messages\n    log_propagate=False,  # Control log propagation\n)\n\n# Example log messages\nlogger.debug(\"This is a debug message\")\nlogger.info(\"This is an info message\")\nlogger.error(\"This is an error message\")\nlogger.warning(\"This is a warning message\")\nlogger.critical(\"This is a critical message\")\n</code></pre> Todo <ul> <li>Add support for additional logging handlers.</li> <li>Implement asynchronous logging.</li> </ul> Author <p>Mike Ryan</p> Date Created <p>2021/07/16</p> Date Updated <p>2024/07/27</p> License <p>MIT</p>"},{"location":"common_functions/logging/#dsg_lib.common_functions.logging_config.SafeFileSink","title":"<code>SafeFileSink</code>","text":"<p>A class to handle safe file logging with rotation and retention policies.</p> <p>This class provides mechanisms to manage log files by rotating them based on size and retaining them for a specified duration. It also supports optional compression of log files.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>str</code> <p>The path to the log file.</p> <code>rotation_size</code> <code>int</code> <p>The size threshold for log rotation in bytes.</p> <code>retention_days</code> <code>timedelta</code> <p>The duration to retain old log files.</p> <code>compression</code> <code>str</code> <p>The compression method to use for old log files.</p> <p>Methods:</p> Name Description <code>parse_size</code> <p>Parses a size string (e.g., '100MB') and returns the size in bytes.</p> <code>parse_duration</code> <p>Parses a duration string (e.g., '7 days') and returns a timedelta object.</p> Example <p>safe_file_sink = SafeFileSink(     path='logs/app.log',     rotation='100 MB',     retention='30 days',     compression='zip' )</p> Source code in <code>dsg_lib/common_functions/logging_config.py</code> <pre><code>class SafeFileSink:\n    \"\"\"\n    A class to handle safe file logging with rotation and retention policies.\n\n    This class provides mechanisms to manage log files by rotating them based on size and retaining them for a specified duration. It also supports optional compression of log files.\n\n    Attributes:\n        path (str): The path to the log file.\n        rotation_size (int): The size threshold for log rotation in bytes.\n        retention_days (timedelta): The duration to retain old log files.\n        compression (str, optional): The compression method to use for old log files.\n\n    Methods:\n        parse_size(size_str): Parses a size string (e.g., '100MB') and returns the size in bytes.\n        parse_duration(duration_str): Parses a duration string (e.g., '7 days') and returns a timedelta object.\n\n    Example:\n        safe_file_sink = SafeFileSink(\n            path='logs/app.log',\n            rotation='100 MB',\n            retention='30 days',\n            compression='zip'\n        )\n\n        # This will set up a log file at 'logs/app.log' with rotation at 100 MB,\n        # retention for 30 days, and compression using zip.\n    \"\"\"\n\n    def __init__(self, path, rotation, retention, compression=None):\n        self.path = path\n        self.rotation_size = self.parse_size(rotation)\n        self.retention_days = self.parse_duration(retention)\n        self.compression = compression\n\n    @staticmethod\n    def parse_size(size_str):  # pragma: no cover\n        \"\"\"\n        Parses a size string and returns the size in bytes.\n\n        Args:\n            size_str (str): The size string (e.g., '100MB').\n\n        Returns:\n            int: The size in bytes.\n        \"\"\"\n        size_str = size_str.upper()\n        if size_str.endswith(\"MB\"):\n            return int(size_str[:-2]) * 1024 * 1024\n        elif size_str.endswith(\"GB\"):\n            return int(size_str[:-2]) * 1024 * 1024 * 1024\n        elif size_str.endswith(\"KB\"):\n            return int(size_str[:-2]) * 1024\n        else:\n            return int(size_str)\n\n    @staticmethod\n    def parse_duration(duration_str):  # pragma: no cover\n        \"\"\"\n        Parses a duration string and returns a timedelta object.\n\n        Args:\n            duration_str (str): The duration string (e.g., '7 days').\n\n        Returns:\n            timedelta: The duration as a timedelta object.\n        \"\"\"\n        duration_str = duration_str.lower()\n        if \"day\" in duration_str:\n            return timedelta(days=int(duration_str.split()[0]))\n        elif \"hour\" in duration_str:\n            return timedelta(hours=int(duration_str.split()[0]))\n        elif \"minute\" in duration_str:\n            return timedelta(minutes=int(duration_str.split()[0]))\n        else:\n            return timedelta(days=0)\n\n    def __call__(self, message):  # pragma: no cover\n        \"\"\"\n        Handles the logging of a message, including writing, rotating, and applying retention policies.\n\n        Args:\n            message (str): The log message to be written.\n\n        This method ensures thread-safe logging by acquiring a lock before writing the message,\n        rotating the logs if necessary, and applying the retention policy to remove old log files.\n        \"\"\"\n        with rotation_lock:\n            self.write_message(message)\n            self.rotate_logs()\n            self.apply_retention()\n\n    def write_message(self, message):  # pragma: no cover\n        \"\"\"\n        Writes a log message to the log file.\n\n        Args:\n            message (str): The log message to be written.\n\n        This method opens the log file in append mode and writes the message to it.\n        \"\"\"\n        with open(self.path, \"a\") as f:\n            f.write(message)\n\n    def rotate_logs(self):  # pragma: no cover\n        \"\"\"\n        Rotates the log file if it exceeds the specified rotation size.\n\n        This method checks the size of the current log file. If the file size exceeds the specified rotation size, it renames the current log file by appending a timestamp to its name. Optionally, it compresses the rotated log file using the specified compression method and removes the original uncompressed file.\n\n        Args:\n            None\n\n        Returns:\n            None\n\n        Raises:\n            OSError: If there is an error renaming or compressing the log file.\n        \"\"\"\n        if os.path.getsize(self.path) &gt;= self.rotation_size:\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            rotated_path = f\"{self.path}.{timestamp}\"\n            os.rename(self.path, rotated_path)\n            if self.compression:\n                shutil.make_archive(\n                    rotated_path,\n                    self.compression,\n                    root_dir=os.path.dirname(rotated_path),\n                    base_dir=os.path.basename(rotated_path),\n                )\n                os.remove(rotated_path)\n\n    def apply_retention(self):  # pragma: no cover\n        \"\"\"\n        Applies the retention policy to remove old log files.\n\n        This method iterates through the log files in the directory of the current log file. It checks the modification time of each log file and removes those that are older than the specified retention period.\n\n        Args:\n            None\n\n        Returns:\n            None\n\n        Raises:\n            OSError: If there is an error removing a log file.\n        \"\"\"\n        now = datetime.now()\n        for filename in os.listdir(os.path.dirname(self.path)):\n            if (\n                filename.startswith(os.path.basename(self.path))\n                and len(filename.split(\".\")) &gt; 1\n            ):\n                file_path = os.path.join(os.path.dirname(self.path), filename)\n                file_time = datetime.fromtimestamp(os.path.getmtime(file_path))\n                if now - file_time &gt; self.retention_days:\n                    os.remove(file_path)\n</code></pre>"},{"location":"common_functions/logging/#dsg_lib.common_functions.logging_config.SafeFileSink--this-will-set-up-a-log-file-at-logsapplog-with-rotation-at-100-mb","title":"This will set up a log file at 'logs/app.log' with rotation at 100 MB,","text":""},{"location":"common_functions/logging/#dsg_lib.common_functions.logging_config.SafeFileSink--retention-for-30-days-and-compression-using-zip","title":"retention for 30 days, and compression using zip.","text":""},{"location":"common_functions/logging/#dsg_lib.common_functions.logging_config.SafeFileSink.__call__","title":"<code>__call__(message)</code>","text":"<p>Handles the logging of a message, including writing, rotating, and applying retention policies.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The log message to be written.</p> required <p>This method ensures thread-safe logging by acquiring a lock before writing the message, rotating the logs if necessary, and applying the retention policy to remove old log files.</p> Source code in <code>dsg_lib/common_functions/logging_config.py</code> <pre><code>def __call__(self, message):  # pragma: no cover\n    \"\"\"\n    Handles the logging of a message, including writing, rotating, and applying retention policies.\n\n    Args:\n        message (str): The log message to be written.\n\n    This method ensures thread-safe logging by acquiring a lock before writing the message,\n    rotating the logs if necessary, and applying the retention policy to remove old log files.\n    \"\"\"\n    with rotation_lock:\n        self.write_message(message)\n        self.rotate_logs()\n        self.apply_retention()\n</code></pre>"},{"location":"common_functions/logging/#dsg_lib.common_functions.logging_config.SafeFileSink.apply_retention","title":"<code>apply_retention()</code>","text":"<p>Applies the retention policy to remove old log files.</p> <p>This method iterates through the log files in the directory of the current log file. It checks the modification time of each log file and removes those that are older than the specified retention period.</p> <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If there is an error removing a log file.</p> Source code in <code>dsg_lib/common_functions/logging_config.py</code> <pre><code>def apply_retention(self):  # pragma: no cover\n    \"\"\"\n    Applies the retention policy to remove old log files.\n\n    This method iterates through the log files in the directory of the current log file. It checks the modification time of each log file and removes those that are older than the specified retention period.\n\n    Args:\n        None\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If there is an error removing a log file.\n    \"\"\"\n    now = datetime.now()\n    for filename in os.listdir(os.path.dirname(self.path)):\n        if (\n            filename.startswith(os.path.basename(self.path))\n            and len(filename.split(\".\")) &gt; 1\n        ):\n            file_path = os.path.join(os.path.dirname(self.path), filename)\n            file_time = datetime.fromtimestamp(os.path.getmtime(file_path))\n            if now - file_time &gt; self.retention_days:\n                os.remove(file_path)\n</code></pre>"},{"location":"common_functions/logging/#dsg_lib.common_functions.logging_config.SafeFileSink.parse_duration","title":"<code>parse_duration(duration_str)</code>  <code>staticmethod</code>","text":"<p>Parses a duration string and returns a timedelta object.</p> <p>Parameters:</p> Name Type Description Default <code>duration_str</code> <code>str</code> <p>The duration string (e.g., '7 days').</p> required <p>Returns:</p> Name Type Description <code>timedelta</code> <p>The duration as a timedelta object.</p> Source code in <code>dsg_lib/common_functions/logging_config.py</code> <pre><code>@staticmethod\ndef parse_duration(duration_str):  # pragma: no cover\n    \"\"\"\n    Parses a duration string and returns a timedelta object.\n\n    Args:\n        duration_str (str): The duration string (e.g., '7 days').\n\n    Returns:\n        timedelta: The duration as a timedelta object.\n    \"\"\"\n    duration_str = duration_str.lower()\n    if \"day\" in duration_str:\n        return timedelta(days=int(duration_str.split()[0]))\n    elif \"hour\" in duration_str:\n        return timedelta(hours=int(duration_str.split()[0]))\n    elif \"minute\" in duration_str:\n        return timedelta(minutes=int(duration_str.split()[0]))\n    else:\n        return timedelta(days=0)\n</code></pre>"},{"location":"common_functions/logging/#dsg_lib.common_functions.logging_config.SafeFileSink.parse_size","title":"<code>parse_size(size_str)</code>  <code>staticmethod</code>","text":"<p>Parses a size string and returns the size in bytes.</p> <p>Parameters:</p> Name Type Description Default <code>size_str</code> <code>str</code> <p>The size string (e.g., '100MB').</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The size in bytes.</p> Source code in <code>dsg_lib/common_functions/logging_config.py</code> <pre><code>@staticmethod\ndef parse_size(size_str):  # pragma: no cover\n    \"\"\"\n    Parses a size string and returns the size in bytes.\n\n    Args:\n        size_str (str): The size string (e.g., '100MB').\n\n    Returns:\n        int: The size in bytes.\n    \"\"\"\n    size_str = size_str.upper()\n    if size_str.endswith(\"MB\"):\n        return int(size_str[:-2]) * 1024 * 1024\n    elif size_str.endswith(\"GB\"):\n        return int(size_str[:-2]) * 1024 * 1024 * 1024\n    elif size_str.endswith(\"KB\"):\n        return int(size_str[:-2]) * 1024\n    else:\n        return int(size_str)\n</code></pre>"},{"location":"common_functions/logging/#dsg_lib.common_functions.logging_config.SafeFileSink.rotate_logs","title":"<code>rotate_logs()</code>","text":"<p>Rotates the log file if it exceeds the specified rotation size.</p> <p>This method checks the size of the current log file. If the file size exceeds the specified rotation size, it renames the current log file by appending a timestamp to its name. Optionally, it compresses the rotated log file using the specified compression method and removes the original uncompressed file.</p> <p>Returns:</p> Type Description <p>None</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If there is an error renaming or compressing the log file.</p> Source code in <code>dsg_lib/common_functions/logging_config.py</code> <pre><code>def rotate_logs(self):  # pragma: no cover\n    \"\"\"\n    Rotates the log file if it exceeds the specified rotation size.\n\n    This method checks the size of the current log file. If the file size exceeds the specified rotation size, it renames the current log file by appending a timestamp to its name. Optionally, it compresses the rotated log file using the specified compression method and removes the original uncompressed file.\n\n    Args:\n        None\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If there is an error renaming or compressing the log file.\n    \"\"\"\n    if os.path.getsize(self.path) &gt;= self.rotation_size:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        rotated_path = f\"{self.path}.{timestamp}\"\n        os.rename(self.path, rotated_path)\n        if self.compression:\n            shutil.make_archive(\n                rotated_path,\n                self.compression,\n                root_dir=os.path.dirname(rotated_path),\n                base_dir=os.path.basename(rotated_path),\n            )\n            os.remove(rotated_path)\n</code></pre>"},{"location":"common_functions/logging/#dsg_lib.common_functions.logging_config.SafeFileSink.write_message","title":"<code>write_message(message)</code>","text":"<p>Writes a log message to the log file.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The log message to be written.</p> required <p>This method opens the log file in append mode and writes the message to it.</p> Source code in <code>dsg_lib/common_functions/logging_config.py</code> <pre><code>def write_message(self, message):  # pragma: no cover\n    \"\"\"\n    Writes a log message to the log file.\n\n    Args:\n        message (str): The log message to be written.\n\n    This method opens the log file in append mode and writes the message to it.\n    \"\"\"\n    with open(self.path, \"a\") as f:\n        f.write(message)\n</code></pre>"},{"location":"common_functions/logging/#dsg_lib.common_functions.logging_config.config_log","title":"<code>config_log(logging_directory='log', log_name='log', logging_level='INFO', log_rotation='100 MB', log_retention='30 days', log_backtrace=False, log_format=None, log_serializer=False, log_diagnose=False, app_name=None, append_app_name=False, enqueue=True, intercept_standard_logging=True, log_propagate=False, compression='zip')</code>","text":"<p>Configures the logging settings for the application.</p> <p>This function sets up the logging configuration, including the log directory, log file name, logging level, log rotation, retention policies, and other optional settings.</p> <p>Parameters:</p> Name Type Description Default <code>logging_directory</code> <code>str</code> <p>The directory where log files will be stored. Defaults to \"log\".</p> <code>'log'</code> <code>log_name</code> <code>str</code> <p>The base name of the log file. Defaults to \"log\".</p> <code>'log'</code> <code>logging_level</code> <code>str</code> <p>The logging level (e.g., \"INFO\", \"DEBUG\"). Defaults to \"INFO\".</p> <code>'INFO'</code> <code>log_rotation</code> <code>str</code> <p>The size threshold for log rotation (e.g., \"100 MB\"). Defaults to \"100 MB\".</p> <code>'100 MB'</code> <code>log_retention</code> <code>str</code> <p>The duration to retain old log files (e.g., \"30 days\"). Defaults to \"30 days\".</p> <code>'30 days'</code> <code>log_backtrace</code> <code>bool</code> <p>Whether to include backtrace information in logs. Defaults to False.</p> <code>False</code> <code>log_format</code> <code>str</code> <p>The format string for log messages. Defaults to a predefined format if not provided.</p> <code>None</code> <code>log_serializer</code> <code>bool</code> <p>Whether to serialize log messages. Defaults to False.</p> <code>False</code> <code>log_diagnose</code> <code>bool</code> <p>Whether to include diagnostic information in logs. Defaults to False.</p> <code>False</code> <code>app_name</code> <code>str</code> <p>The name of the application. Defaults to None.</p> <code>None</code> <code>append_app_name</code> <code>bool</code> <p>Whether to append the application name to the log file name. Defaults to False.</p> <code>False</code> <code>enqueue</code> <code>bool</code> <p>Whether to enqueue log messages for asynchronous processing. Defaults to True.</p> <code>True</code> <code>intercept_standard_logging</code> <code>bool</code> <p>Whether to intercept standard logging calls. Defaults to True.</p> <code>True</code> <code>log_propagate</code> <code>bool</code> <p>Whether to propagate log messages to ancestor loggers. Defaults to False.</p> <code>False</code> <code>compression</code> <code>str</code> <p>The compression method for rotated log files (e.g., \"zip\"). Defaults to 'zip'.</p> <code>'zip'</code> <p>Returns:</p> Type Description <p>None</p> Example <p>config_log(     logging_directory='logs',     log_name='app_log',     logging_level='DEBUG',     log_rotation='50 MB',     log_retention='7 days',     log_backtrace=True,     log_format='{time} - {level} - {message}',     log_serializer=True,     log_diagnose=True,     app_name='MyApp',     append_app_name=True,     enqueue=False,     intercept_standard_logging=False,     log_propagate=False,     compression='gz' )</p> <p>This will configure the logging settings with the specified parameters, setting up a log file at 'logs/app_log' with rotation at 50 MB, retention for 7 days, and other specified options.</p> Source code in <code>dsg_lib/common_functions/logging_config.py</code> <pre><code>def config_log(\n    logging_directory: str = \"log\",\n    log_name: str = \"log\",\n    logging_level: str = \"INFO\",\n    log_rotation: str = \"100 MB\",\n    log_retention: str = \"30 days\",\n    log_backtrace: bool = False,\n    log_format: str = None,\n    log_serializer: bool = False,\n    log_diagnose: bool = False,\n    app_name: str = None,\n    append_app_name: bool = False,\n    enqueue: bool = True,\n    intercept_standard_logging: bool = True,\n    log_propagate: bool = False,\n    compression: str = \"zip\",\n):\n    \"\"\"\n    Configures the logging settings for the application.\n\n    This function sets up the logging configuration, including the log directory, log file name, logging level, log rotation, retention policies, and other optional settings.\n\n    Args:\n        logging_directory (str): The directory where log files will be stored. Defaults to \"log\".\n        log_name (str): The base name of the log file. Defaults to \"log\".\n        logging_level (str): The logging level (e.g., \"INFO\", \"DEBUG\"). Defaults to \"INFO\".\n        log_rotation (str): The size threshold for log rotation (e.g., \"100 MB\"). Defaults to \"100 MB\".\n        log_retention (str): The duration to retain old log files (e.g., \"30 days\"). Defaults to \"30 days\".\n        log_backtrace (bool): Whether to include backtrace information in logs. Defaults to False.\n        log_format (str, optional): The format string for log messages. Defaults to a predefined format if not provided.\n        log_serializer (bool): Whether to serialize log messages. Defaults to False.\n        log_diagnose (bool): Whether to include diagnostic information in logs. Defaults to False.\n        app_name (str, optional): The name of the application. Defaults to None.\n        append_app_name (bool): Whether to append the application name to the log file name. Defaults to False.\n        enqueue (bool): Whether to enqueue log messages for asynchronous processing. Defaults to True.\n        intercept_standard_logging (bool): Whether to intercept standard logging calls. Defaults to True.\n        log_propagate (bool): Whether to propagate log messages to ancestor loggers. Defaults to False.\n        compression (str): The compression method for rotated log files (e.g., \"zip\"). Defaults to 'zip'.\n\n    Returns:\n        None\n\n    Example:\n        config_log(\n            logging_directory='logs',\n            log_name='app_log',\n            logging_level='DEBUG',\n            log_rotation='50 MB',\n            log_retention='7 days',\n            log_backtrace=True,\n            log_format='{time} - {level} - {message}',\n            log_serializer=True,\n            log_diagnose=True,\n            app_name='MyApp',\n            append_app_name=True,\n            enqueue=False,\n            intercept_standard_logging=False,\n            log_propagate=False,\n            compression='gz'\n        )\n\n    This will configure the logging settings with the specified parameters, setting up a log file at 'logs/app_log' with rotation at 50 MB, retention for 7 days, and other specified options.\n    \"\"\"\n\n    # If the log_name ends with \".log\", remove the extension\n    if log_name.endswith(\".log\"):\n        log_name = log_name.replace(\".log\", \"\")  # pragma: no cover\n\n    # If the log_name ends with \".json\", remove the extension\n    if log_name.endswith(\".json\"):\n        log_name = log_name.replace(\".json\", \"\")  # pragma: no cover\n\n    # Set default log format if not provided\n    if log_format is None:  # pragma: no cover\n        log_format = \"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSSSSS}&lt;/green&gt; | &lt;level&gt;{level: &lt;8}&lt;/level&gt; | &lt;cyan&gt; {name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - &lt;level&gt;{message}&lt;/level&gt;\"  # pragma: no cover\n\n    if log_serializer is True:\n        log_format = \"{message}\"  # pragma: no cover\n        log_name = f\"{log_name}.json\"  # pragma: no cover\n    else:\n        log_name = f\"{log_name}.log\"  # pragma: no cover\n\n    # Validate logging level\n    log_levels: list = [\"DEBUG\", \"INFO\", \"ERROR\", \"WARNING\", \"CRITICAL\"]\n    if logging_level.upper() not in log_levels:\n        raise ValueError(\n            f\"Invalid logging level: {logging_level}. Valid levels are: {log_levels}\"\n        )\n\n    # Generate unique trace ID\n    trace_id: str = str(uuid4())\n    logger.configure(extra={\"app_name\": app_name, \"trace_id\": trace_id})\n\n    # Append app name to log format if provided\n    if app_name is not None:\n        log_format += \" | app_name: {extra[app_name]}\"\n\n    # Remove any previously added sinks\n    logger.remove()\n\n    # Append app name to log file name if required\n    if append_app_name is True and app_name is not None:\n        log_name = log_name.replace(\".\", f\"_{app_name}.\")\n\n    # Construct log file path\n    log_path = Path.cwd().joinpath(logging_directory).joinpath(log_name)\n\n    # Add loguru logger with specified configuration\n    logger.add(\n        SafeFileSink(\n            log_path,\n            rotation=log_rotation,\n            retention=log_retention,\n            compression=compression,\n        ),\n        level=logging_level.upper(),\n        format=log_format,\n        enqueue=enqueue,\n        backtrace=log_backtrace,\n        serialize=log_serializer,\n        diagnose=log_diagnose,\n    )\n\n\n    class InterceptHandler(logging.Handler):\n        \"\"\"\n        A logging handler that intercepts standard logging messages and redirects them to Loguru.\n\n        This handler captures log messages from the standard logging module and forwards them to Loguru, preserving the log level and message details.\n\n        Methods:\n            emit(record):\n                Emits a log record to Loguru.\n        \"\"\"\n\n        def emit(self, record):\n            \"\"\"\n            Emits a log record to Loguru.\n\n            This method captures the log record, determines the appropriate Loguru log level, and logs the message using Loguru. It also handles exceptions and finds the caller's frame to maintain accurate log information.\n\n            Args:\n                record (logging.LogRecord): The log record to be emitted.\n\n            Returns:\n                None\n            \"\"\"\n            # Get corresponding Loguru level if it exists\n            try:\n                level = logger.level(record.levelname).name\n            except ValueError:  # pragma: no cover\n                level = record.levelno  # pragma: no cover\n\n            # Find caller from where originated the logged message\n            frame, depth = logging.currentframe(), 2\n            while frame.f_code.co_filename == logging.__file__:  # pragma: no cover\n                frame = frame.f_back  # pragma: no cover\n                depth += 1  # pragma: no cover\n\n            # Log the message using loguru\n            logger.opt(depth=depth, exception=record.exc_info).log(\n                level, record.getMessage()\n            )  # pragma: no cover\n\n    if intercept_standard_logging:\n        # Remove all handlers from all loggers to prevent duplicates\n        for logger_name in logging.Logger.manager.loggerDict:\n            log_instance = logging.getLogger(logger_name)\n            log_instance.handlers = []\n            # Optionally, set propagate to False if you want to avoid double propagation\n            log_instance.propagate = log_propagate\n\n        # Remove handlers from root logger\n        root_logger = logging.getLogger()\n        root_logger.handlers = []\n\n        # Add InterceptHandler to all loggers (including root)\n        for logger_name in logging.Logger.manager.loggerDict:\n            logging.getLogger(logger_name).addHandler(InterceptHandler())\n        root_logger.addHandler(InterceptHandler())\n\n        # Set the root logger's level to the lowest level possible\n        root_logger.setLevel(logging.NOTSET)\n    else:\n        # If not intercepting, you may want to configure basicConfig as before\n        logging.basicConfig(level=logging_level.upper())\n</code></pre>"},{"location":"common_functions/regex/","title":"Reference","text":""},{"location":"common_functions/regex/#dsg_lib.common_functions.patterns","title":"<code>dsg_lib.common_functions.patterns</code>","text":"<p>This module contains functions for pattern searching in text using regular expressions.</p> <p>The main function in this module is <code>pattern_between_two_char</code>, which searches for all patterns between two characters in a given string. The function uses Python's built-in <code>re</code> module for regex searching and the <code>loguru</code> module for logging.</p> <p>Functions:</p> Name Description <code>pattern_between_two_char</code> <p>str, left_characters: str,</p> <code>right_characters</code> <p>str) -&gt; dict: Searches for all patterns between two characters (left and right) in a given string using regular expressions.</p> Example <pre><code>from dsg_lib.common_functions import patterns\n\ntext = \"Hello, my name is 'John Doe' and I live in 'New York'.\" left_char =\n\"'\" right_char = \"'\"\n\nresults = patterns.pattern_between_two_char(text, left_char, right_char)\n\nprint(results) ``` This will output: ```python {\n    'found': ['John Doe', 'New York'], 'matched_found': 2,\n    'pattern_parameters': {\n        'left_character': \"'\", 'right_character': \"'\", 'regex_pattern':\n        \"'(.+?)'\", 'text_string': \"Hello, my name is 'John Doe' and I live\n        in 'New York'.\"\n    }\n}\n</code></pre> <p>Author: Mike Ryan Date: 2024/05/16 License: MIT</p>"},{"location":"common_functions/regex/#dsg_lib.common_functions.patterns.pattern_between_two_char","title":"<code>pattern_between_two_char(text_string, left_characters, right_characters)</code>","text":"<p>Searches for all patterns between two characters (left and right) in a given string using regular expressions.</p> <p>This function takes a string and two characters as input, and returns a dictionary containing all patterns found between the two characters in the string. The dictionary also includes the number of matches found and the regex pattern used for searching.</p> <p>The function uses Python's built-in <code>re</code> module for regex searching and the <code>loguru</code> module for logging.</p> <p>Parameters:</p> Name Type Description Default <code>text_string</code> <code>str</code> <p>The string in which to search for patterns.</p> required <code>left_characters</code> <code>str</code> <p>The character(s) that appear(s) immediately to</p> required <code>the</code> <code>left of the desired pattern. right_characters (str</code> <p>The</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the following keys: - \"found\": a list of strings containing all patterns found. - \"matched_found\": the number of patterns found. - \"pattern_parameters\": a dictionary with the following keys:     - \"left_character\": the escaped left character string used to       build the regex pattern.     - \"right_character\": the escaped right character string used to       build the regex pattern.     - \"regex_pattern\": the final regex pattern used for searching.     - \"text_string\": the escaped input string used for searching.</p> Example <pre><code>from dsg_lib.common_functions import patterns\n\ntext = \"Hello, my name is 'John Doe' and I live in 'New York'.\"\nleft_char = \"'\" right_char = \"'\"\n\nresults = patterns.pattern_between_two_char(text, left_char, right_char)\n\nprint(results) ``` This will output: ```python {\n    'found': ['John Doe', 'New York'], 'matched_found': 2,\n    'pattern_parameters': {\n        'left_character': \"'\", 'right_character': \"'\", 'regex_pattern':\n        \"'(.+?)'\", 'text_string': \"Hello, my name is 'John Doe' and I\n        live in 'New York'.\"\n    }\n}\n</code></pre> Source code in <code>dsg_lib/common_functions/patterns.py</code> <pre><code>def pattern_between_two_char(\n    text_string: str, left_characters: str, right_characters: str\n) -&gt; dict:\n    \"\"\"\n    Searches for all patterns between two characters (left and right) in a given\n    string using regular expressions.\n\n    This function takes a string and two characters as input, and returns a\n    dictionary containing all patterns found between the two characters in the\n    string. The dictionary also includes the number of matches found and the\n    regex pattern used for searching.\n\n    The function uses Python's built-in `re` module for regex searching and the\n    `loguru` module for logging.\n\n    Args:\n        text_string (str): The string in which to search for patterns.\n        left_characters (str): The character(s) that appear(s) immediately to\n        the left of the desired pattern. right_characters (str): The\n        character(s) that appear(s) immediately to the right of the desired\n        pattern.\n\n    Returns:\n        dict: A dictionary with the following keys:\n            - \"found\": a list of strings containing all patterns found.\n            - \"matched_found\": the number of patterns found.\n            - \"pattern_parameters\": a dictionary with the following keys:\n                - \"left_character\": the escaped left character string used to\n                  build the regex pattern.\n                - \"right_character\": the escaped right character string used to\n                  build the regex pattern.\n                - \"regex_pattern\": the final regex pattern used for searching.\n                - \"text_string\": the escaped input string used for searching.\n\n    Example:\n        ```python\n        from dsg_lib.common_functions import patterns\n\n        text = \"Hello, my name is 'John Doe' and I live in 'New York'.\"\n        left_char = \"'\" right_char = \"'\"\n\n        results = patterns.pattern_between_two_char(text, left_char, right_char)\n\n        print(results) ``` This will output: ```python {\n            'found': ['John Doe', 'New York'], 'matched_found': 2,\n            'pattern_parameters': {\n                'left_character': \"'\", 'right_character': \"'\", 'regex_pattern':\n                \"'(.+?)'\", 'text_string': \"Hello, my name is 'John Doe' and I\n                live in 'New York'.\"\n            }\n        }\n        ```\n    \"\"\"\n\n    if not left_characters or not right_characters:\n        raise ValueError(\n            f\"Left '{left_characters}' and/or Right '{right_characters}' characters must not be None or empty\"\n        )\n\n    try:\n        # Escape input strings to safely use them in regex pattern\n        esc_text = re.escape(text_string)\n        esc_left_char = re.escape(left_characters)\n        esc_right_char = re.escape(right_characters)\n\n        # Create a regex pattern that matches all substrings between target\n        # characters\n        pattern = f\"{esc_left_char}(.+?){esc_right_char}\"\n\n        # Replace \\w with . to match any printable UTF-8 character\n        pattern = pattern.replace(r\"\\w\", r\".\")\n\n        # Search for all patterns and store them in pattern_list variable\n        pattern_list = re.findall(pattern, esc_text)\n\n        # Create a dictionary to store match details\n        results: dict = {\n            \"found\": pattern_list,\n            \"matched_found\": len(pattern_list),\n            \"pattern_parameters\": {\n                \"left_character\": esc_left_char,\n                \"right_character\": esc_right_char,\n                \"regex_pattern\": pattern,\n                \"text_string\": esc_text,\n            },\n        }\n\n        # Log matched pattern(s) found using 'debug' log level\n        if len(pattern_list) &gt; 0:\n            logger.debug(f\"Matched pattern(s): {pattern_list}\")\n\n        # Log successful function execution using 'info' log level\n        logger.info(\"Successfully executed 'pattern_between_two_char' function\")\n        return results\n\n    except ValueError as e:  # pragma: no cover\n        # capture exception and return error in case of invalid input parameters\n        results: dict = {\n            \"error\": str(e),\n            \"matched_found\": 0,\n            \"pattern_parameters\": {\n                \"left_character\": left_characters,\n                \"right_character\": right_characters,\n                \"regex_pattern\": None,\n                \"text_string\": text_string,\n            },\n        }\n        # logger of regex error using 'critical' log level\n        logger.critical(f\"Failed to generate regex pattern with error: {e}\")\n        return results\n</code></pre>"},{"location":"database/async_database_setup/","title":"Reference","text":""},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database","title":"<code>dsg_lib.async_database_functions.async_database</code>","text":"<p>async_database.py</p> <p>This module provides classes for managing asynchronous database operations using SQLAlchemy and asyncio.</p> <p>Classes:</p> Name Description <code>- DBConfig</code> <p>Initializes and manages the database configuration including the creation of the SQLAlchemy engine and MetaData instance.</p> <code>- AsyncDatabase</code> <p>Leverages a DBConfig instance to perform asynchronous database operations such as obtaining sessions, creating tables, and disconnecting from the database.</p> <p>Logging is performed using the logger from dsg_lib.common_functions.</p> Example <pre><code>from dsg_lib.async_database_functions import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n)\n\n# Define database configuration\nconfig = {\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    \"pool_recycle\": 3600,\n}\n\n# Create the configuration instance\ndb_config = database_config.DBConfig(config)\n\n# Instantiate AsyncDatabase with the given configuration\nasync_db = async_database.AsyncDatabase(db_config)\n\n# Optionally, create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n</code></pre> Author <p>Mike Ryan</p> Date Created <p>2024/05/16</p> Date Updated <p>2025/02/15 - docstring and comments updated</p> License <p>MIT</p>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase","title":"<code>AsyncDatabase</code>","text":"<p>Manages asynchronous database operations.</p> <p>This class provides methods to acquire database sessions, create tables asynchronously, and disconnect the database engine safely.</p>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase--attributes","title":"Attributes","text":"<p>db_config : DBConfig     An instance of DBConfig containing the database configuration such as the engine. Base : Base     The declarative base model used by SQLAlchemy to define database models.</p>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase--methods","title":"Methods","text":"<p>get_db_session():     Returns a context manager that yields a new asynchronous database session. create_tables():     Asynchronously creates all tables as defined in the metadata. disconnect():     Asynchronously disconnects the database engine.</p> Source code in <code>dsg_lib/async_database_functions/async_database.py</code> <pre><code>class AsyncDatabase:\n    \"\"\"\n    Manages asynchronous database operations.\n\n    This class provides methods to acquire database sessions, create tables asynchronously,\n    and disconnect the database engine safely.\n\n    Attributes\n    ----------\n    db_config : DBConfig\n        An instance of DBConfig containing the database configuration such as the engine.\n    Base : Base\n        The declarative base model used by SQLAlchemy to define database models.\n\n    Methods\n    -------\n    get_db_session():\n        Returns a context manager that yields a new asynchronous database session.\n    create_tables():\n        Asynchronously creates all tables as defined in the metadata.\n    disconnect():\n        Asynchronously disconnects the database engine.\n    \"\"\"\n\n    def __init__(self, db_config: DBConfig):\n        \"\"\"\n        Initialize AsyncDatabase with a database configuration.\n\n        Parameters\n        ----------\n        db_config : DBConfig\n            An instance of DBConfig containing the necessary database configurations.\n        \"\"\"\n        self.db_config = db_config\n        self.Base = BASE\n        logger.debug(\"AsyncDatabase initialized\")\n\n    def get_db_session(self):\n        \"\"\"\n        Obtain a new asynchronous database session.\n\n        Returns\n        -------\n        contextlib._GeneratorContextManager\n            A context manager that yields a new database session.\n        \"\"\"\n        logger.debug(\"Getting database session\")\n        return self.db_config.get_db_session()\n\n    async def create_tables(self):\n        \"\"\"\n        Asynchronously create all tables defined in the metadata.\n\n        This method binds the engine to the Base metadata and runs the table creation\n        in a synchronous manner within an asynchronous transaction.\n\n        Raises\n        ------\n        Exception\n            Propagates any exceptions encountered during table creation.\n        \"\"\"\n        logger.debug(\"Creating tables\")\n        try:\n            # Bind the engine to the Base metadata\n            self.Base.metadata.bind = self.db_config.engine\n\n            # Begin an asynchronous transaction and create tables synchronously\n            async with self.db_config.engine.begin() as conn:\n                await conn.run_sync(self.Base.metadata.create_all)\n            logger.info(\"Tables created successfully\")\n        except Exception as ex:  # pragma: no cover\n            logger.error(f\"Error creating tables: {ex}\")  # pragma: no cover\n            raise  # pragma: no cover\n\n    async def disconnect(self):  # pragma: no cover\n        \"\"\"\n        Asynchronously disconnect the database engine.\n\n        Closes all connections and disposes of the engine resources.\n\n        Raises\n        ------\n        Exception\n            Propagates any exceptions encountered during disconnection.\n        \"\"\"\n        logger.debug(\"Disconnecting from database\")\n        try:\n            await self.db_config.engine.dispose()\n            logger.info(\"Disconnected from database\")\n        except Exception as ex:  # pragma: no cover\n            logger.error(f\"Error disconnecting from database: {ex}\")  # pragma: no cover\n            raise  # pragma: no cover\n</code></pre>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase.__init__","title":"<code>__init__(db_config)</code>","text":"<p>Initialize AsyncDatabase with a database configuration.</p>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase.__init__--parameters","title":"Parameters","text":"<p>db_config : DBConfig     An instance of DBConfig containing the necessary database configurations.</p> Source code in <code>dsg_lib/async_database_functions/async_database.py</code> <pre><code>def __init__(self, db_config: DBConfig):\n    \"\"\"\n    Initialize AsyncDatabase with a database configuration.\n\n    Parameters\n    ----------\n    db_config : DBConfig\n        An instance of DBConfig containing the necessary database configurations.\n    \"\"\"\n    self.db_config = db_config\n    self.Base = BASE\n    logger.debug(\"AsyncDatabase initialized\")\n</code></pre>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase.create_tables","title":"<code>create_tables()</code>  <code>async</code>","text":"<p>Asynchronously create all tables defined in the metadata.</p> <p>This method binds the engine to the Base metadata and runs the table creation in a synchronous manner within an asynchronous transaction.</p>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase.create_tables--raises","title":"Raises","text":"<p>Exception     Propagates any exceptions encountered during table creation.</p> Source code in <code>dsg_lib/async_database_functions/async_database.py</code> <pre><code>async def create_tables(self):\n    \"\"\"\n    Asynchronously create all tables defined in the metadata.\n\n    This method binds the engine to the Base metadata and runs the table creation\n    in a synchronous manner within an asynchronous transaction.\n\n    Raises\n    ------\n    Exception\n        Propagates any exceptions encountered during table creation.\n    \"\"\"\n    logger.debug(\"Creating tables\")\n    try:\n        # Bind the engine to the Base metadata\n        self.Base.metadata.bind = self.db_config.engine\n\n        # Begin an asynchronous transaction and create tables synchronously\n        async with self.db_config.engine.begin() as conn:\n            await conn.run_sync(self.Base.metadata.create_all)\n        logger.info(\"Tables created successfully\")\n    except Exception as ex:  # pragma: no cover\n        logger.error(f\"Error creating tables: {ex}\")  # pragma: no cover\n        raise  # pragma: no cover\n</code></pre>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase.disconnect","title":"<code>disconnect()</code>  <code>async</code>","text":"<p>Asynchronously disconnect the database engine.</p> <p>Closes all connections and disposes of the engine resources.</p>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase.disconnect--raises","title":"Raises","text":"<p>Exception     Propagates any exceptions encountered during disconnection.</p> Source code in <code>dsg_lib/async_database_functions/async_database.py</code> <pre><code>async def disconnect(self):  # pragma: no cover\n    \"\"\"\n    Asynchronously disconnect the database engine.\n\n    Closes all connections and disposes of the engine resources.\n\n    Raises\n    ------\n    Exception\n        Propagates any exceptions encountered during disconnection.\n    \"\"\"\n    logger.debug(\"Disconnecting from database\")\n    try:\n        await self.db_config.engine.dispose()\n        logger.info(\"Disconnected from database\")\n    except Exception as ex:  # pragma: no cover\n        logger.error(f\"Error disconnecting from database: {ex}\")  # pragma: no cover\n        raise  # pragma: no cover\n</code></pre>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase.get_db_session","title":"<code>get_db_session()</code>","text":"<p>Obtain a new asynchronous database session.</p>"},{"location":"database/async_database_setup/#dsg_lib.async_database_functions.async_database.AsyncDatabase.get_db_session--returns","title":"Returns","text":"<p>contextlib._GeneratorContextManager     A context manager that yields a new database session.</p> Source code in <code>dsg_lib/async_database_functions/async_database.py</code> <pre><code>def get_db_session(self):\n    \"\"\"\n    Obtain a new asynchronous database session.\n\n    Returns\n    -------\n    contextlib._GeneratorContextManager\n        A context manager that yields a new database session.\n    \"\"\"\n    logger.debug(\"Getting database session\")\n    return self.db_config.get_db_session()\n</code></pre>"},{"location":"database/base_schema/","title":"Reference","text":""},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema","title":"<code>dsg_lib.async_database_functions.base_schema</code>","text":"<p>This module defines the base schema for database models in the application.</p> <p>The module uses SQLAlchemy as the ORM and provides a <code>SchemaBase</code> class that all other models should inherit from. The <code>SchemaBase</code> class includes common columns that are needed for most models like <code>pkid</code>, <code>date_created</code>, and <code>date_updated</code>.</p> <ul> <li><code>pkid</code>: A unique identifier for each record. It's a string representation of a   UUID.</li> <li><code>date_created</code>: The date and time when a particular row was inserted into the   table.     It defaults to the current UTC time when the instance is created.</li> <li><code>date_updated</code>: The date and time when a particular row was last updated.     It defaults to the current UTC time whenever the instance is updated.</li> </ul> <p>To create a new database model, import this module and extend the <code>SchemaBase</code> class.</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\n\nclass MyModel(base_schema.SchemaBaseSQLite):\n        # Define your model-specific columns here my_column =\n        base_schema.Column(base_schema.String(50))\n</code></pre></p> <p>Author: Mike Ryan Date: 2024/05/16 License: MIT</p>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseCockroachDB","title":"<code>SchemaBaseCockroachDB</code>","text":"<p>This class provides a base schema that includes common columns for most models when using a CockroachDB database. CockroachDB uses the same syntax as PostgreSQL. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBaseCockroachDB, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseCockroachDB:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using a CockroachDB database. CockroachDB uses the same syntax\n    as PostgreSQL. All other models should inherit from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBaseCockroachDB, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"(CURRENT_TIMESTAMP AT TIME ZONE 'UTC')\"),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"(CURRENT_TIMESTAMP AT TIME ZONE 'UTC')\"),\n        onupdate=text(\"(CURRENT_TIMESTAMP AT TIME ZONE 'UTC')\"),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseFirebird","title":"<code>SchemaBaseFirebird</code>","text":"<p>This class provides a base schema that includes common columns for most models when using a Firebird database. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBaseFirebird, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseFirebird:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using a Firebird database. All other models should inherit\n    from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBaseFirebird, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"CURRENT_TIMESTAMP\"),\n        comment=\"Date and time when a row was inserted, defaults to current time\",\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"CURRENT_TIMESTAMP\"),\n        onupdate=text(\"CURRENT_TIMESTAMP\"),\n        comment=\"Date and time when a row was last updated, defaults to current time on update\",\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseMSSQL","title":"<code>SchemaBaseMSSQL</code>","text":"<p>This class provides a base schema that includes common columns for most models when using a Microsoft SQL Server database. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBaseMSSQL, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseMSSQL:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using a Microsoft SQL Server database. All other models should\n    inherit from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBaseMSSQL, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"GETUTCDATE()\"),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"GETUTCDATE()\"),\n        onupdate=text(\"GETUTCDATE()\"),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseMySQL","title":"<code>SchemaBaseMySQL</code>","text":"<p>This class provides a base schema that includes common columns for most models when using a MySQL database. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBaseMySQL, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseMySQL:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using a MySQL database. All other models should inherit\n    from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBaseMySQL, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"UTC_TIMESTAMP()\"),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"UTC_TIMESTAMP()\"),\n        onupdate=text(\"UTC_TIMESTAMP()\"),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseOracle","title":"<code>SchemaBaseOracle</code>","text":"<p>This class provides a base schema that includes common columns for most models when using an Oracle database. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBaseOracle, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseOracle:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using an Oracle database. All other models should inherit\n    from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBaseOracle, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"SYS_EXTRACT_UTC(SYSTIMESTAMP)\"),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"SYS_EXTRACT_UTC(SYSTIMESTAMP)\"),\n        onupdate=text(\"SYS_EXTRACT_UTC(SYSTIMESTAMP)\"),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBasePostgres","title":"<code>SchemaBasePostgres</code>","text":"<p>This class provides a base schema that includes common columns for most models when using a PostgreSQL database. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBasePostgres, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBasePostgres:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using a PostgreSQL database. All other models should inherit\n    from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBasePostgres, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"(CURRENT_TIMESTAMP AT TIME ZONE 'UTC')\"),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"(CURRENT_TIMESTAMP AT TIME ZONE 'UTC')\"),\n        onupdate=text(\"(CURRENT_TIMESTAMP AT TIME ZONE 'UTC')\"),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseSQLite","title":"<code>SchemaBaseSQLite</code>","text":"<p>This class provides a base schema that includes common columns for most models. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBase, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseSQLite:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models. All other models should inherit from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBase, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        default=datetime.datetime.now(datetime.timezone.utc),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        default=datetime.datetime.now(datetime.timezone.utc),\n        onupdate=datetime.datetime.now(datetime.timezone.utc),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/base_schema/#dsg_lib.async_database_functions.base_schema.SchemaBaseSybase","title":"<code>SchemaBaseSybase</code>","text":"<p>This class provides a base schema that includes common columns for most models when using a Sybase database. All other models should inherit from this class.</p> <p>Attributes:</p> Name Type Description <code>pkid</code> <code>str</code> <p>A unique identifier for each record. It's a string</p> <code>date_created</code> <code>datetime</code> <p>The date and time when a particular row was</p> <code>date_updated</code> <code>datetime</code> <p>The date and time when a particular row was</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import base_schema\nfrom sqlalchemy.orm import declarative_base\n\nBASE = declarative_base()\n\nclass MyModel(base_schema.SchemaBaseSybase, BASE):\n    # Define your model-specific columns here\n    my_column = base_schema.Column(base_schema.String(50))\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/base_schema.py</code> <pre><code>class SchemaBaseSybase:\n    \"\"\"\n    This class provides a base schema that includes common columns for most\n    models when using a Sybase database. All other models should inherit\n    from this class.\n\n    Attributes:\n        pkid (str): A unique identifier for each record. It's a string\n        representation of a UUID.\n        date_created (datetime): The date and time when a particular row was\n        inserted into the table. It defaults to the current UTC time when the\n        instance is created.\n        date_updated (datetime): The date and time when a particular row was\n        last updated. It defaults to the current UTC time whenever the instance\n        is updated.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import base_schema\n    from sqlalchemy.orm import declarative_base\n\n    BASE = declarative_base()\n\n    class MyModel(base_schema.SchemaBaseSybase, BASE):\n        # Define your model-specific columns here\n        my_column = base_schema.Column(base_schema.String(50))\n    ```\n    \"\"\"\n\n    # Each instance in the table will have a unique id which is a string\n    # representation of a UUID\n    pkid = Column(\n        String(36),\n        primary_key=True,\n        index=True,\n        default=lambda: str(uuid4()),\n        comment=uuid_comment,\n    )\n\n    # The date and time when a particular row was inserted into the table. It\n    # defaults to the current UTC time when the instance is created.\n    date_created = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"GETUTCDATE()\"),\n        comment=date_created_comment,\n    )\n\n    # The date and time when a particular row was last updated. It defaults to\n    # the current UTC time whenever the instance is updated.\n    date_updated = Column(\n        DateTime,\n        index=True,\n        server_default=text(\"GETUTCDATE()\"),\n        onupdate=text(\"GETUTCDATE()\"),\n        comment=date_updated_comment,\n    )\n</code></pre>"},{"location":"database/database_configuration/","title":"Reference","text":""},{"location":"database/database_configuration/#supported-databases","title":"Supported Databases","text":"Database Library (with link) Supported Database Versions (via library) PostgreSQL asyncpg <code>&gt;=0.21.0</code>SQLAlchemy <code>&gt;=2.0.10,&lt;2.0.99</code> asyncpg supports PostgreSQL 9.2+ SQLite aiosqlite <code>&gt;=0.17.0</code>SQLAlchemy <code>&gt;=2.0.10,&lt;2.0.99</code> SQLite 3.x Oracle oracledb <code>&gt;=2.4.1,&lt;2.5.0</code>SQLAlchemy <code>&gt;=2.0.10,&lt;2.0.99</code> Oracle Database 11.2+ MS SQL Server aioodbc <code>&gt;=0.4.1</code>SQLAlchemy <code>&gt;=2.0.10,&lt;2.0.99</code> SQL Server 2008+ (via ODBC driver) CockroachDB sqlalchemy-cockroachdb <code>&lt;2.0.2</code>SQLAlchemy <code>&gt;=2.0.10,&lt;2.0.99</code>asyncpg <code>&gt;=0.21.0</code> CockroachDB v19.2+ MySQL asyncmy <code>&gt;=0.2.10</code>SQLAlchemy <code>&gt;=2.0.10,&lt;2.0.99</code> MySQL 5.7+, MariaDB 10.2+"},{"location":"database/database_configuration/#dsg_lib.async_database_functions.database_config","title":"<code>dsg_lib.async_database_functions.database_config</code>","text":"<p>This module provides classes and functions for managing asynchronous database operations using SQLAlchemy and asyncio.</p> <p>The main classes are DBConfig, which manages the database configuration and creates a SQLAlchemy engine and a MetaData instance, and AsyncDatabase, which uses an instance of DBConfig to perform asynchronous database operations.</p> <p>The module also provides a function, import_sqlalchemy, which tries to import SQLAlchemy and its components, and raises an ImportError if SQLAlchemy is not installed or if the installed version is not compatible.</p> <p>The module uses the logger from the <code>dsg_lib</code> for logging, and the <code>time</code> module for working with times. It also uses the <code>contextlib</code> module for creating context managers, and the <code>typing</code> module for type hinting.</p> <p>The <code>BASE</code> variable is a base class for declarative database models. It is created using the <code>declarative_base</code> function from <code>sqlalchemy.orm</code>.</p> <p>This module is part of the <code>dsg_lib</code> package, which provides utilities for working with databases in Python.</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import database_config\n\n# Define your database configuration config = {\n    \"database_uri\": \"postgresql+asyncpg://user:password@localhost/dbname\",\n    \"echo\": True, \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n    \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n}\n\n# Create a DBConfig instance db_config = database_config.DBConfig(config)\n\n# Use the DBConfig instance to get a database session async with\ndb_config.get_db_session() as session:\n    # Perform your database operations here pass\n</code></pre></p> <p>Author: Mike Ryan Date: 2024/05/16 License: MIT</p>"},{"location":"database/database_configuration/#dsg_lib.async_database_functions.database_config.DBConfig","title":"<code>DBConfig</code>","text":"<p>A class used to manage the database configuration and create a SQLAlchemy engine.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>dict</code> <p>A dictionary containing the database configuration</p> <code>parameters.</code> <code>engine (Engine</code> <p>The SQLAlchemy engine created with the</p> <code>database</code> <code>URI from the config. metadata (MetaData</code> <p>The SQLAlchemy</p> <p>Create Engine Support Functions by Database Type Confirmed by testing [SQLITE, PostrgeSQL] To Be Tested [MySQL, Oracle, MSSQL] and should be considered experimental ------- Option          SQLite  PostgreSQL  MySQL Oracle  MSSQL echo                Yes         Yes         Yes     Yes Yes future              Yes         Yes         Yes     Yes     Yes pool_pre_ping       Yes         Yes         Yes     Yes     Yes pool_size No          Yes         Yes     Yes     Yes max_overflow        No Yes         Yes     Yes     Yes pool_recycle        Yes         Yes Yes     Yes     Yes pool_timeout        No          Yes         Yes     Yes Yes</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import database_config\n\n# Define your database configuration config = {\n    \"database_uri\": \"postgresql+asyncpg://user:password@localhost/dbname\",\n    \"echo\": True, \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n    \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n}\n\n# Create a DBConfig instance db_config = database_config.DBConfig(config)\n\n# Use the DBConfig instance to get a database session async with\ndb_config.get_db_session() as session:\n    # Perform your database operations here pass\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_config.py</code> <pre><code>class DBConfig:\n    \"\"\"\n    A class used to manage the database configuration and create a SQLAlchemy\n    engine.\n\n    Attributes:\n        config (dict): A dictionary containing the database configuration\n        parameters. engine (Engine): The SQLAlchemy engine created with the\n        database URI from the config. metadata (MetaData): The SQLAlchemy\n        MetaData instance.\n\n\n    Create Engine Support Functions by Database Type Confirmed by testing\n    [SQLITE, PostrgeSQL] To Be Tested [MySQL, Oracle, MSSQL] and should be\n    considered experimental ------- Option          SQLite  PostgreSQL  MySQL\n    Oracle  MSSQL echo                Yes         Yes         Yes     Yes\n    Yes future              Yes         Yes         Yes     Yes     Yes\n    pool_pre_ping       Yes         Yes         Yes     Yes     Yes pool_size\n    No          Yes         Yes     Yes     Yes max_overflow        No\n    Yes         Yes     Yes     Yes pool_recycle        Yes         Yes\n    Yes     Yes     Yes pool_timeout        No          Yes         Yes     Yes\n    Yes\n\n    Example:\n    ```python\n\n    from dsg_lib.async_database_functions import database_config\n\n    # Define your database configuration config = {\n        \"database_uri\": \"postgresql+asyncpg://user:password@localhost/dbname\",\n        \"echo\": True, \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n        \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n    }\n\n    # Create a DBConfig instance db_config = database_config.DBConfig(config)\n\n    # Use the DBConfig instance to get a database session async with\n    db_config.get_db_session() as session:\n        # Perform your database operations here pass\n    ```\n\n    \"\"\"\n\n    SUPPORTED_PARAMETERS = {\n        \"sqlite\": {\"echo\", \"future\", \"pool_recycle\"},\n        \"postgresql\": {\n            \"echo\",\n            \"future\",\n            \"pool_pre_ping\",\n            \"pool_size\",\n            \"max_overflow\",\n            \"pool_recycle\",\n            \"pool_timeout\",\n        },\n        \"cockroachdb\": {  # experimental support for CockroachDB\n            \"echo\",\n            \"future\",\n            \"pool_pre_ping\",\n            \"pool_size\",\n            \"max_overflow\",\n            \"pool_recycle\",\n            \"pool_timeout\",\n        },\n        \"mysql\": {  # experimental support for MySQL\n            \"echo\",\n            \"future\",\n            \"pool_pre_ping\",\n            \"pool_size\",\n            \"max_overflow\",\n            \"pool_recycle\",\n            \"pool_timeout\",\n        },\n        \"mssql\": {  # experimental support for Microsoft SQL Server\n            \"echo\",\n            \"future\",\n            \"pool_pre_ping\",\n            \"pool_size\",\n            \"max_overflow\",\n            \"pool_recycle\",\n            \"pool_timeout\",\n        },\n        \"oracle\": {  # experimental support for Oracle\n            \"echo\",\n            \"future\",\n            \"pool_pre_ping\",\n            \"pool_size\",\n            \"max_overflow\",\n            \"pool_recycle\",\n            \"pool_timeout\",\n        },\n        # Add other engines here...\n    }\n\n    def __init__(self, config: Dict):\n        \"\"\"\n        Initializes the DBConfig instance with the given database configuration.\n\n        The configuration should be a dictionary with the following keys: -\n        \"database_uri\": The URI for the database. - \"echo\": If True, the engine\n        will log all statements as well as a `repr()` of their parameter lists\n        to the engines logger, which defaults to sys.stdout. - \"future\": If\n        True, use the future version of SQLAlchemy, which supports asyncio. -\n        \"pool_pre_ping\": If True, the pool will test the connection for liveness\n        upon each checkout. - \"pool_size\": The size of the connection pool to be\n        maintained. - \"max_overflow\": The number of connections that can be\n        opened above the `pool_size` setting, when all other connections are in\n        use. - \"pool_recycle\": The number of seconds after which a connection is\n        automatically recycled. This is required for MySQL, which removes\n        connections after 8 hours idle by default. - \"pool_timeout\": The number\n        of seconds to wait before giving up on getting a connection from the\n        pool.\n\n        Args:\n            config (Dict): A dictionary containing the database configuration\n            parameters.\n\n        Raises:\n            Exception: If there are unsupported parameters for the database\n            engine type.\n\n        Example:\n        ```python\n\n        from dsg_lib.async_database_functions import database_config\n\n        # Define your database configuration config = {\n            \"database_uri\":\n            \"postgresql+asyncpg://user:password@localhost/dbname\", \"echo\": True,\n            \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n            \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n        }\n\n        # Create a DBConfig instance db_config =\n        database_config.DBConfig(config)\n        ```\n        \"\"\"\n        self.config = config\n        engine_type = self.config[\"database_uri\"].split(\"+\")[0]\n        supported_parameters = self.SUPPORTED_PARAMETERS.get(engine_type, set())\n        unsupported_parameters = (\n            set(config.keys()) - supported_parameters - {\"database_uri\"}\n        )\n        if unsupported_parameters:\n            error_message = (\n                f\"Unsupported parameters for {engine_type}: {unsupported_parameters}\"\n            )\n            logger.error(error_message)\n            raise Exception(error_message)\n\n        engine_parameters = {\n            param: self.config.get(param)\n            for param in supported_parameters\n            if self.config.get(param) is not None\n        }\n        self.engine = create_async_engine(\n            self.config[\"database_uri\"], **engine_parameters\n        )\n        self.metadata = MetaData()\n\n    @asynccontextmanager\n    async def get_db_session(self):\n        \"\"\"\n        This method returns a context manager that provides a new database\n        session.\n\n        The session is created using the SQLAlchemy engine from the DBConfig\n        instance, and it does not expire on commit. The session is of type\n        AsyncSession.\n\n        This method should be used with the `async with` statement.\n\n        Yields:\n            AsyncSession: A new SQLAlchemy asynchronous session.\n\n        Raises:\n            SQLAlchemyError: If a database error occurs.\n\n        Example:\n        ```python\n\n        from dsg_lib.async_database_functions import database_config\n\n        # Define your database configuration config = {\n            \"database_uri\":\n            \"postgresql+asyncpg://user:password@localhost/dbname\", \"echo\": True,\n            \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n            \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n        }\n\n        # Create a DBConfig instance db_config =\n        database_config.DBConfig(config)\n\n        # Use the DBConfig instance to get a database session async with\n        db_config.get_db_session() as session:\n            # Perform your database operations here pass\n        ```\n        \"\"\"\n        logger.debug(\"Creating new database session\")\n        try:\n            # Create a new database session\n            async with sessionmaker(\n                self.engine, expire_on_commit=False, class_=AsyncSession\n            )() as session:\n                # Yield the session to the context manager\n                yield session\n        except SQLAlchemyError as e:  # pragma: no cover\n            # Log the error and raise it\n            logger.error(f\"Database error occurred: {str(e)}\")  # pragma: no cover\n            raise  # pragma: no cover\n        finally:  # pragma: no cover\n            # Log the end of the database session\n            logger.debug(\"Database session ended\")  # pragma: no cover\n</code></pre>"},{"location":"database/database_configuration/#dsg_lib.async_database_functions.database_config.DBConfig.__init__","title":"<code>__init__(config)</code>","text":"<p>Initializes the DBConfig instance with the given database configuration.</p> <p>The configuration should be a dictionary with the following keys: - \"database_uri\": The URI for the database. - \"echo\": If True, the engine will log all statements as well as a <code>repr()</code> of their parameter lists to the engines logger, which defaults to sys.stdout. - \"future\": If True, use the future version of SQLAlchemy, which supports asyncio. - \"pool_pre_ping\": If True, the pool will test the connection for liveness upon each checkout. - \"pool_size\": The size of the connection pool to be maintained. - \"max_overflow\": The number of connections that can be opened above the <code>pool_size</code> setting, when all other connections are in use. - \"pool_recycle\": The number of seconds after which a connection is automatically recycled. This is required for MySQL, which removes connections after 8 hours idle by default. - \"pool_timeout\": The number of seconds to wait before giving up on getting a connection from the pool.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict</code> <p>A dictionary containing the database configuration</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If there are unsupported parameters for the database</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import database_config\n\n# Define your database configuration config = {\n    \"database_uri\":\n    \"postgresql+asyncpg://user:password@localhost/dbname\", \"echo\": True,\n    \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n    \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n}\n\n# Create a DBConfig instance db_config =\ndatabase_config.DBConfig(config)\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_config.py</code> <pre><code>def __init__(self, config: Dict):\n    \"\"\"\n    Initializes the DBConfig instance with the given database configuration.\n\n    The configuration should be a dictionary with the following keys: -\n    \"database_uri\": The URI for the database. - \"echo\": If True, the engine\n    will log all statements as well as a `repr()` of their parameter lists\n    to the engines logger, which defaults to sys.stdout. - \"future\": If\n    True, use the future version of SQLAlchemy, which supports asyncio. -\n    \"pool_pre_ping\": If True, the pool will test the connection for liveness\n    upon each checkout. - \"pool_size\": The size of the connection pool to be\n    maintained. - \"max_overflow\": The number of connections that can be\n    opened above the `pool_size` setting, when all other connections are in\n    use. - \"pool_recycle\": The number of seconds after which a connection is\n    automatically recycled. This is required for MySQL, which removes\n    connections after 8 hours idle by default. - \"pool_timeout\": The number\n    of seconds to wait before giving up on getting a connection from the\n    pool.\n\n    Args:\n        config (Dict): A dictionary containing the database configuration\n        parameters.\n\n    Raises:\n        Exception: If there are unsupported parameters for the database\n        engine type.\n\n    Example:\n    ```python\n\n    from dsg_lib.async_database_functions import database_config\n\n    # Define your database configuration config = {\n        \"database_uri\":\n        \"postgresql+asyncpg://user:password@localhost/dbname\", \"echo\": True,\n        \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n        \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n    }\n\n    # Create a DBConfig instance db_config =\n    database_config.DBConfig(config)\n    ```\n    \"\"\"\n    self.config = config\n    engine_type = self.config[\"database_uri\"].split(\"+\")[0]\n    supported_parameters = self.SUPPORTED_PARAMETERS.get(engine_type, set())\n    unsupported_parameters = (\n        set(config.keys()) - supported_parameters - {\"database_uri\"}\n    )\n    if unsupported_parameters:\n        error_message = (\n            f\"Unsupported parameters for {engine_type}: {unsupported_parameters}\"\n        )\n        logger.error(error_message)\n        raise Exception(error_message)\n\n    engine_parameters = {\n        param: self.config.get(param)\n        for param in supported_parameters\n        if self.config.get(param) is not None\n    }\n    self.engine = create_async_engine(\n        self.config[\"database_uri\"], **engine_parameters\n    )\n    self.metadata = MetaData()\n</code></pre>"},{"location":"database/database_configuration/#dsg_lib.async_database_functions.database_config.DBConfig.get_db_session","title":"<code>get_db_session()</code>  <code>async</code>","text":"<p>This method returns a context manager that provides a new database session.</p> <p>The session is created using the SQLAlchemy engine from the DBConfig instance, and it does not expire on commit. The session is of type AsyncSession.</p> <p>This method should be used with the <code>async with</code> statement.</p> <p>Yields:</p> Name Type Description <code>AsyncSession</code> <p>A new SQLAlchemy asynchronous session.</p> <p>Raises:</p> Type Description <code>SQLAlchemyError</code> <p>If a database error occurs.</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import database_config\n\n# Define your database configuration config = {\n    \"database_uri\":\n    \"postgresql+asyncpg://user:password@localhost/dbname\", \"echo\": True,\n    \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n    \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n}\n\n# Create a DBConfig instance db_config =\ndatabase_config.DBConfig(config)\n\n# Use the DBConfig instance to get a database session async with\ndb_config.get_db_session() as session:\n    # Perform your database operations here pass\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_config.py</code> <pre><code>@asynccontextmanager\nasync def get_db_session(self):\n    \"\"\"\n    This method returns a context manager that provides a new database\n    session.\n\n    The session is created using the SQLAlchemy engine from the DBConfig\n    instance, and it does not expire on commit. The session is of type\n    AsyncSession.\n\n    This method should be used with the `async with` statement.\n\n    Yields:\n        AsyncSession: A new SQLAlchemy asynchronous session.\n\n    Raises:\n        SQLAlchemyError: If a database error occurs.\n\n    Example:\n    ```python\n\n    from dsg_lib.async_database_functions import database_config\n\n    # Define your database configuration config = {\n        \"database_uri\":\n        \"postgresql+asyncpg://user:password@localhost/dbname\", \"echo\": True,\n        \"future\": True, \"pool_pre_ping\": True, \"pool_size\": 5,\n        \"max_overflow\": 10, \"pool_recycle\": 3600, \"pool_timeout\": 30,\n    }\n\n    # Create a DBConfig instance db_config =\n    database_config.DBConfig(config)\n\n    # Use the DBConfig instance to get a database session async with\n    db_config.get_db_session() as session:\n        # Perform your database operations here pass\n    ```\n    \"\"\"\n    logger.debug(\"Creating new database session\")\n    try:\n        # Create a new database session\n        async with sessionmaker(\n            self.engine, expire_on_commit=False, class_=AsyncSession\n        )() as session:\n            # Yield the session to the context manager\n            yield session\n    except SQLAlchemyError as e:  # pragma: no cover\n        # Log the error and raise it\n        logger.error(f\"Database error occurred: {str(e)}\")  # pragma: no cover\n        raise  # pragma: no cover\n    finally:  # pragma: no cover\n        # Log the end of the database session\n        logger.debug(\"Database session ended\")  # pragma: no cover\n</code></pre>"},{"location":"database/database_operations/","title":"Reference","text":""},{"location":"database/database_operations/#configuration-matrix","title":"Configuration Matrix","text":"<p>Create Engine Support Functions by Database Type Confirmed by testing [SQLITE, PostgreSQL] To Be Tested [MySQL, Oracle, MSSQL] and should be considered experimental.</p> Option SQLite PostgreSQL MySQL Oracle MSSQL echo Yes Yes Yes Yes Yes future Yes Yes Yes Yes Yes pool_pre_ping Yes Yes Yes Yes Yes pool_size No Yes Yes Yes Yes max_overflow No Yes Yes Yes Yes pool_recycle Yes Yes Yes Yes Yes pool_timeout No Yes Yes Yes Yes"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations","title":"<code>dsg_lib.async_database_functions.database_operations</code>","text":"<p>This module provides the <code>DatabaseOperations</code> class for performing CRUD operations on a database using SQLAlchemy's asynchronous session.</p> <p>The <code>DatabaseOperations</code> class includes the following methods:</p> <pre><code>- `execute_one`: Executes a single non-read SQL query asynchronously.\n- `execute_many`: Executes multiple non-read SQL queries asynchronously within a single transaction.\n- 'read_one_record': Retrieves a single record from the database based on the provided query.\n- `read_query`: Executes a fetch query on the database and returns a list of records that match the query.\n- `read_multi_query`: Executes multiple fetch queries on the database and returns a dictionary of results for each query.\n- `count_query`: Counts the number of records that match a given query.\n- `get_column_details`: Gets the details of the columns in a table.\n- `get_primary_keys`: Gets the primary keys of a table.\n- `get_table_names`: Gets the names of all tables in the database.\n\nDeprecated Methods:\n- `create_one`: [Deprecated] Use `execute_one` with an INSERT query instead.\n- `create_many`: [Deprecated] Use `execute_many` with INSERT queries instead.\n- `update_one`: [Deprecated] Use `execute_one` with an UPDATE query instead.\n- `update_many`: [Deprecated] Use `execute_many` with UPDATE queries instead.\n- `delete_one`: [Deprecated] Use `execute_one` with a DELETE query instead.\n- `delete_many`: [Deprecated] Use `execute_many` with DELETE queries instead.\n</code></pre> <p>Each method is designed to handle errors correctly and provide a simple interface for performing database operations.</p> <p>This module also imports the necessary SQLAlchemy and loguru modules, and the <code>AsyncDatabase</code> class from the local <code>async_database</code> module.</p> <p>Author: Mike Ryan Date: 2024/11/29 License: MIT</p>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations","title":"<code>DatabaseOperations</code>","text":"<p>This class provides methods for performing CRUD operations on a database using SQLAlchemy's asynchronous session.</p> <p>The methods include:</p> <ul> <li><code>execute_one</code>: Executes a single non-read SQL query asynchronously.</li> <li><code>execute_many</code>: Executes multiple non-read SQL queries asynchronously within a single transaction.</li> <li><code>read_one_record</code>: Retrieves a single record from the database based on the provided query.</li> <li><code>read_query</code>: Executes a fetch query on the database and returns a list of records that match the query.</li> <li><code>read_multi_query</code>: Executes multiple fetch queries on the database and returns a dictionary of results for each query.</li> <li><code>count_query</code>: Counts the number of records that match a given query.</li> <li><code>get_column_details</code>: Gets the details of the columns in a table.</li> <li><code>get_primary_keys</code>: Gets the primary keys of a table.</li> <li><code>get_table_names</code>: Gets the names of all tables in the database.</li> </ul> <p>Deprecated Methods: - <code>create_one</code>: [Deprecated] Use <code>execute_one</code> with an INSERT query instead. - <code>create_many</code>: [Deprecated] Use <code>execute_many</code> with INSERT queries instead. - <code>update_one</code>: [Deprecated] Use <code>execute_one</code> with an UPDATE query instead. - <code>delete_one</code>: [Deprecated] Use <code>execute_one</code> with a DELETE query instead. - <code>delete_many</code>: [Deprecated] Use <code>execute_many</code> with DELETE queries instead.</p> <p>Examples: <pre><code>from sqlalchemy import insert, select\nfrom dsg_lib.async_database_functions import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n)\n\n# Create a DBConfig instance\nconfig = {\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    \"pool_recycle\": 3600,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n\n# create one record\nquery = insert(User).values(name='John Doe')\nresult = await db_ops.execute_one(query)\n\n# read one record\nquery = select(User).where(User.name == 'John Doe')\nrecord = await db_ops.read_query(query)\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>class DatabaseOperations:\n    \"\"\"\n    This class provides methods for performing CRUD operations on a database using SQLAlchemy's asynchronous session.\n\n    The methods include:\n\n    - `execute_one`: Executes a single non-read SQL query asynchronously.\n    - `execute_many`: Executes multiple non-read SQL queries asynchronously within a single transaction.\n    - `read_one_record`: Retrieves a single record from the database based on the provided query.\n    - `read_query`: Executes a fetch query on the database and returns a list of records that match the query.\n    - `read_multi_query`: Executes multiple fetch queries on the database and returns a dictionary of results for each query.\n    - `count_query`: Counts the number of records that match a given query.\n    - `get_column_details`: Gets the details of the columns in a table.\n    - `get_primary_keys`: Gets the primary keys of a table.\n    - `get_table_names`: Gets the names of all tables in the database.\n\n    Deprecated Methods:\n    - `create_one`: [Deprecated] Use `execute_one` with an INSERT query instead.\n    - `create_many`: [Deprecated] Use `execute_many` with INSERT queries instead.\n    - `update_one`: [Deprecated] Use `execute_one` with an UPDATE query instead.\n    - `delete_one`: [Deprecated] Use `execute_one` with a DELETE query instead.\n    - `delete_many`: [Deprecated] Use `execute_many` with DELETE queries instead.\n\n    Examples:\n    ```python\n    from sqlalchemy import insert, select\n    from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n    )\n\n    # Create a DBConfig instance\n    config = {\n        \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n        \"echo\": False,\n        \"future\": True,\n        \"pool_recycle\": 3600,\n    }\n    # create database configuration\n    db_config = database_config.DBConfig(config)\n    # Create an AsyncDatabase instance\n    async_db = async_database.AsyncDatabase(db_config)\n    # Create a DatabaseOperations instance\n    db_ops = database_operations.DatabaseOperations(async_db)\n\n    # create one record\n    query = insert(User).values(name='John Doe')\n    result = await db_ops.execute_one(query)\n\n    # read one record\n    query = select(User).where(User.name == 'John Doe')\n    record = await db_ops.read_query(query)\n    ```\n    \"\"\"\n\n    def __init__(self, async_db: AsyncDatabase):\n        \"\"\"\n        Initializes a new instance of the DatabaseOperations class.\n\n        Args:\n            async_db (module_name.AsyncDatabase): An instance of the\n            AsyncDatabase class for performing asynchronous database operations.\n\n        Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n\n        db_config = database_config.DBConfig(config)\n\n        async_db = async_database.AsyncDatabase(db_config)\n\n        db_ops = database_operations.DatabaseOperations(async_db)\n\n        ```\n        \"\"\"\n        # Log the start of the initialization\n        logger.debug(\"Initializing DatabaseOperations instance\")\n\n        # Store the AsyncDatabase instance in the async_db attribute This\n        # instance will be used for performing asynchronous database operations\n        self.async_db = async_db\n\n        # Log the successful initialization\n        logger.debug(\"DatabaseOperations instance initialized successfully\")\n\n    async def get_columns_details(self, table):\n        \"\"\"\n        Retrieves the details of the columns of a given table.\n\n        This asynchronous method accepts a table object and returns a\n        dictionary. Each key in the dictionary is a column name from the table,\n        and the corresponding value is another dictionary containing details\n        about that column, such as type, if it's nullable, if it's a primary\n        key, if it's unique, its autoincrement status, and its default value.\n\n        Args:\n            table (Table): An instance of the SQLAlchemy Table class\n            representing the database table for which column details are\n            required.\n\n        Returns:\n            dict: A dictionary where each key is a column name, and each value\n            is a dictionary with the column's details.\n\n        Raises:\n            Exception: If any error occurs during the database operation.\n\n        Example:\n        ```python\n        from sqlalchemy import Table, MetaData, Column,\n        Integer, String from dsg_lib.async_database_functions import module_name metadata = MetaData()\n        my_table = Table('my_table', metadata,\n                        Column('id', Integer, primary_key=True), Column('name',\n                        String))\n\n        from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # get columns details\n        columns = await db_ops.get_columns_details(my_table)\n        ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug(\n            f\"Starting get_columns_details operation for table: {table.__name__}\"\n        )\n\n        try:\n            # Log the start of the column retrieval\n            logger.debug(f\"Getting columns for table: {table.__name__}\")\n\n            # Retrieve the details of the columns and store them in a dictionary\n            # The keys are the column names and the values are dictionaries\n            # containing the column details\n            columns = {\n                c.name: {\n                    \"type\": str(c.type),\n                    \"nullable\": c.nullable,\n                    \"primary_key\": c.primary_key,\n                    \"unique\": c.unique,\n                    \"autoincrement\": c.autoincrement,\n                    \"default\": (\n                        str(c.default.arg)\n                        if c.default is not None and not callable(c.default.arg)\n                        else None\n                    ),\n                }\n                for c in table.__table__.columns\n            }\n\n            # Log the successful column retrieval\n            logger.debug(f\"Successfully retrieved columns for table: {table.__name__}\")\n\n            return columns\n        except Exception as ex:  # pragma: no cover\n            # Handle any exceptions that occur during the column retrieval\n            logger.error(\n                f\"An error occurred while getting columns for table: {table.__name__}\"\n            )  # pragma: no cover\n            return handle_exceptions(ex)  # pragma: no cover\n\n    async def get_primary_keys(self, table):\n        \"\"\"\n        Retrieves the primary keys of a given table.\n\n        This asynchronous method accepts a table object and returns a list\n        containing the names of its primary keys. It is useful for understanding\n        the structure of the table and for operations that require knowledge of\n        the primary keys.\n\n        Args:\n            table (Table): An instance of the SQLAlchemy Table class\n            representing the database table for which primary keys are required.\n\n        Returns:\n            list: A list containing the names of the primary keys of the table.\n\n        Raises:\n            Exception: If any error occurs during the database operation.\n\n        Example:\n            ```python\n            from sqlalchemy import Table, MetaData, Column, Integer,\n                String from dsg_lib.async_database_functions import module_name metadata = MetaData()\n                my_table = Table('my_table', metadata,\n                                Column('id', Integer, primary_key=True),\n                                Column('name', String, primary_key=True))\n            from dsg_lib.async_database_functions import (\n                async_database,\n                base_schema,\n                database_config,\n                database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n\n            # get primary keys\n            primary_keys = await db_ops.get_primary_keys(my_table)\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug(f\"Starting get_primary_keys operation for table: {table.__name__}\")\n\n        try:\n            # Log the start of the primary key retrieval\n            logger.debug(f\"Getting primary keys for table: {table.__name__}\")\n\n            # Retrieve the primary keys and store them in a list\n            primary_keys = table.__table__.primary_key.columns.keys()\n\n            # Log the successful primary key retrieval\n            logger.debug(f\"Primary keys retrieved successfully: {primary_keys}\")\n\n            return primary_keys\n\n        except Exception as ex:  # pragma: no cover\n            # Handle any exceptions that occur during the primary key retrieval\n            logger.error(f\"Exception occurred: {ex}\")  # pragma: no cover\n            return handle_exceptions(ex)  # pragma: no cover\n\n    async def get_table_names(self):\n        \"\"\"\n        Retrieves the names of all tables in the database.\n\n        This asynchronous method returns a list containing the names of all\n        tables in the database. It is useful for database introspection,\n        allowing the user to know which tables are available in the current\n        database context.\n\n        Returns:\n            list: A list containing the names of all tables in the database.\n\n        Raises:\n            Exception: If any error occurs during the database operation.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # get table names\n            table_names = await db_ops.get_table_names()\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug(\"Starting get_table_names operation\")\n\n        try:\n            # Log the start of the table name retrieval\n            logger.debug(\"Retrieving table names\")\n\n            # Retrieve the table names and store them in a list The keys of the\n            # metadata.tables dictionary are the table names\n            table_names = list(self.async_db.Base.metadata.tables.keys())\n\n            # Log the successful table name retrieval\n            logger.debug(f\"Table names retrieved successfully: {table_names}\")\n\n            return table_names\n\n        except Exception as ex:  # pragma: no cover\n            # Handle any exceptions that occur during the table name retrieval\n            logger.error(f\"Exception occurred: {ex}\")  # pragma: no cover\n            return handle_exceptions(ex)  # pragma: no cover\n\n    # ---------------------------\n    # Internal helpers (refactor)\n    # ---------------------------\n    def _classify_statement(self, query: ClauseElement) -&gt; str:\n        \"\"\"Classify a SQLAlchemy Core statement into a simple kind.\n\n        Returns one of: 'select' | 'insert' | 'update' | 'delete' | 'other'.\n        \"\"\"\n        select_type = getattr(sqlalchemy.sql.selectable, \"Select\", None)\n        insert_type = getattr(sqlalchemy.sql.dml, \"Insert\", None)\n        update_type = getattr(sqlalchemy.sql.dml, \"Update\", None)\n        delete_type = getattr(sqlalchemy.sql.dml, \"Delete\", None)\n\n        if select_type is not None and isinstance(query, select_type):\n            return \"select\"\n        if insert_type is not None and isinstance(query, insert_type):\n            return \"insert\"\n        if update_type is not None and isinstance(query, update_type):\n            return \"update\"\n        if delete_type is not None and isinstance(query, delete_type):\n            return \"delete\"\n        return \"other\"\n\n    def _shape_rows_from_result(self, result) -&gt; List[Any]:\n        \"\"\"Shape rows from a SQLAlchemy Result similar to read_query behavior.\n\n        - Single column \u2192 list of scalars\n        - Multi column \u2192 list of dicts via row._mapping\n        - ORM/objects \u2192 list of objects\n        \"\"\"\n        try:\n            # Keys available: determine single vs multi column quickly\n            if self._has_keys(result):\n                if self._is_single_column(result):\n                    return result.scalars().all()\n                return [self._shape_row(row) for row in result.fetchall()]\n\n            # Fallback when keys() is not available\n            return [self._shape_row(row) for row in result.fetchall()]\n        except Exception as _row_ex:  # pragma: no cover - defensive\n            logger.debug(f\"_shape_rows_from_result failed: {_row_ex}\")\n            return []\n\n    def _has_keys(self, result) -&gt; bool:\n        \"\"\"Return True if result exposes keys() callable.\"\"\"\n        return hasattr(result, \"keys\") and callable(result.keys)\n\n    def _is_single_column(self, result) -&gt; bool:\n        \"\"\"Return True if result has exactly one column.\"\"\"\n        try:\n            keys = result.keys()\n            return len(keys) == 1\n        except Exception:  # pragma: no cover - defensive\n            return False\n\n    def _shape_row(self, row: Any) -&gt; Any:\n        \"\"\"Shape a single row to a scalar/dict/object according to mapping presence.\"\"\"\n        if hasattr(row, \"_mapping\"):\n            mapping = row._mapping\n            return list(mapping.values())[0] if len(mapping) == 1 else dict(mapping)\n        # For ORM/objects or plain rows, return as-is\n        return row\n\n    def _collect_rows(self, result) -&gt; List[Any]:\n        \"\"\"Collect and shape rows from a SQLAlchemy Result if it returns rows.\"\"\"\n        try:\n            if getattr(result, \"returns_rows\", False):\n                return self._shape_rows_from_result(result)\n        except Exception as _row_ex:  # pragma: no cover - defensive\n            logger.debug(f\"Unable to materialize returned rows: {_row_ex}\")\n        return []\n\n    def _assemble_dml_metadata(self, query: ClauseElement, result, rows: List[Any]) -&gt; Dict[str, Any]:\n        \"\"\"Build a metadata dictionary for DML results, including rowcount, inserted PK, rows.\"\"\"\n        meta: Dict[str, Any] = {\"rowcount\": getattr(result, \"rowcount\", None)}\n        insert_type = getattr(sqlalchemy.sql.dml, \"Insert\", None)\n        try:\n            if insert_type is not None and isinstance(query, insert_type):\n                meta[\"inserted_primary_key\"] = getattr(result, \"inserted_primary_key\", None)\n        except Exception:  # pragma: no cover - driver specific\n            meta[\"inserted_primary_key\"] = None\n        if rows:\n            meta[\"rows\"] = rows\n        return meta\n\n    async def count_query(self, query):\n        \"\"\"\n        Executes a count query on the database and returns the number of records\n        that match the query.\n\n        This asynchronous method accepts a SQLAlchemy `Select` query object and\n        returns the count of records that match the query. This is particularly\n        useful for getting the total number of records that satisfy certain\n        conditions without actually fetching the records themselves.\n\n        Parameters:\n            query (Select): A SQLAlchemy `Select` query object specifying the\n            conditions to count records for.\n\n        Returns:\n            int: The number of records that match the query.\n\n        Raises:\n            Exception: If any error occurs during the execution of the query.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # count query\n            count = await db_ops.count_query(select(User).where(User.age &gt; 30))\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug(\"Starting count_query operation\")\n\n        try:\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the query being executed\n                logger.debug(f\"Executing count query: {query}\")\n\n                # Execute the count query and retrieve the count\n                result = await session.execute(\n                    select(func.count()).select_from(query.subquery())\n                )\n                count = result.scalar()\n\n                # Log the successful query execution\n                logger.debug(f\"Count query executed successfully. Result: {count}\")\n\n                return count\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the query execution\n            logger.error(f\"Exception occurred: {ex}\")\n            return handle_exceptions(ex)\n\n    async def read_one_record(self, query):\n        \"\"\"\n        Retrieves a single record from the database based on the provided query.\n\n        This asynchronous method accepts a SQL query object and returns the\n        first record that matches the query. If no record matches the query, it\n        returns None. This method is useful for fetching specific data\n        when the expected result is a single record.\n\n        Parameters:\n            query (Select): An instance of the SQLAlchemy Select class,\n            representing the query to be executed.\n\n        Returns:\n            Result: The first record that matches the query or None if no record matches.\n\n        Raises:\n            Exception: If any error occurs during the database operation.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # read one record\n            record = await db_ops.read_one_record(select(User).where(User.name == 'John Doe'))\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug(f\"Starting read_one_record operation for {query}\")\n\n        try:\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the start of the record retrieval\n                logger.debug(f\"Getting record with query: {query}\")\n\n                # Execute the query and retrieve the first record\n                result = await session.execute(query)\n                record = result.scalar_one()\n\n                # Log the successful record retrieval\n                logger.debug(f\"Record retrieved successfully: {record}\")\n\n                return record\n\n        except NoResultFound:\n            # No record was found\n            logger.debug(\"No record found\")\n            return None\n\n        except Exception as ex:  # pragma: no cover\n            # Handle any exceptions that occur during the record retrieval\n            logger.error(f\"Exception occurred: {ex}\")  # pragma: no cover\n            return handle_exceptions(ex)  # pragma: no cover\n\n    async def read_query(self, query):\n        # Log the start of the operation\n        logger.debug(\"Starting read_query operation\")\n\n        try:\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the query being executed\n                logger.debug(f\"Executing fetch query: {query}\")\n\n                # Execute the fetch query and retrieve the records\n                result = await session.execute(query)\n\n                # Use result.keys() to determine number of columns in result\n                if hasattr(result, \"keys\") and callable(result.keys):\n                    keys = result.keys()\n                    if len(keys) == 1:\n                        # Use scalars() for single-column queries\n                        records = result.scalars().all()\n                    else:\n                        rows = result.fetchall()\n                        records = []\n                        for row in rows:\n                            if hasattr(row, \"_mapping\"):\n                                mapping = row._mapping\n                                if len(mapping) == 1:# pragma: no cover\n                                    records.append(list(mapping.values())[0])# pragma: no cover\n                                else:\n                                    records.append(dict(mapping))\n                            elif hasattr(row, \"__dict__\"):# pragma: no cover\n                                records.append(row)# pragma: no cover\n                            else:# pragma: no cover\n                                records.append(row)# pragma: no cover\n                else:\n                    # Fallback to previous logic if keys() is not available\n                    rows = result.fetchall()\n                    records = []\n                    for row in rows:\n                        if hasattr(row, \"_mapping\"):\n                            mapping = row._mapping\n                            if len(mapping) == 1:\n                                records.append(list(mapping.values())[0])\n                            else:# pragma: no cover\n                                records.append(dict(mapping))# pragma: no cover\n                        elif hasattr(row, \"__dict__\"):\n                            records.append(row)\n                        else:\n                            records.append(row)# pragma: no cover\n                logger.debug(f\"read_query result: {records}\")\n                return records\n\n        except Exception as ex:\n            logger.error(f\"Exception occurred: {ex}\")\n            return handle_exceptions(ex)\n\n    async def read_multi_query(self, queries: Dict[str, str]):\n        \"\"\"\n        Executes multiple fetch queries asynchronously and returns a dictionary of results for each query.\n\n        This asynchronous method accepts a dictionary where each key is a query name (str)\n        and each value is a SQLAlchemy `Select` query object. It executes each query within a single\n        database session and collects the results. The results are returned as a dictionary mapping\n        each query name to a list of records that match that query.\n\n        The function automatically determines the structure of each result set:\n        - If the query returns a single column, the result will be a list of scalar values.\n        - If the query returns multiple columns, the result will be a list of dictionaries mapping column names to values.\n        - If the result row is an ORM object, it will be returned as-is.\n\n        Args:\n            queries (Dict[str, Select]): A dictionary mapping query names to SQLAlchemy `Select` query objects.\n\n        Returns:\n            Dict[str, List[Any]]: A dictionary where each key is a query name and each value is a list of records\n            (scalars, dictionaries, or ORM objects) that match the corresponding query.\n\n        Raises:\n            Exception: If any error occurs during the execution of any query, the function logs the error and\n            returns a dictionary with error details using `handle_exceptions`.\n\n        Example:\n            ```python\n            from sqlalchemy import select\n            queries = {\n                \"adults\": select(User).where(User.age &gt;= 18),\n                \"minors\": select(User).where(User.age &lt; 18),\n            }\n            results = await db_ops.read_multi_query(queries)\n            # results[\"adults\"] and results[\"minors\"] will contain lists of records\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug(\"Starting read_multi_query operation\")\n\n        try:\n            results = {}\n            async with self.async_db.get_db_session() as session:\n                for query_name, query in queries.items():\n                    logger.debug(f\"Executing fetch query: {query}\")\n                    result = await session.execute(query)\n                    if hasattr(result, \"keys\") and callable(result.keys):\n                        keys = result.keys()\n                        if len(keys) == 1:\n                            data = result.scalars().all()\n                        else:\n                            rows = result.fetchall()\n                            data = []\n                            for row in rows:\n                                if hasattr(row, \"_mapping\"):\n                                    mapping = row._mapping\n                                    if len(mapping) == 1: # pragma: no cover\n                                        data.append(list(mapping.values())[0]) # pragma: no cover\n                                    else:\n                                        data.append(dict(mapping))\n                                elif hasattr(row, \"__dict__\"): # pragma: no cover\n                                    data.append(row) # pragma: no cover\n                                else:# pragma: no cover\n                                    data.append(row)# pragma: no cover\n                    else:\n                        rows = result.fetchall()\n                        data = []\n                        for row in rows:\n                            if hasattr(row, \"_mapping\"):\n                                mapping = row._mapping\n                                if len(mapping) == 1:# pragma: no cover\n                                    data.append(list(mapping.values())[0])# pragma: no cover\n                                else:\n                                    data.append(dict(mapping))\n                            elif hasattr(row, \"__dict__\"):\n                                data.append(row)\n                            else:\n                                data.append(row)# pragma: no cover\n                    results[query_name] = data\n            return results\n\n        except Exception as ex:\n            logger.error(f\"Exception occurred: {ex}\")\n            return handle_exceptions(ex)\n\n    async def execute_one(\n        self,\n        query: ClauseElement,\n        values: Optional[Dict[str, Any]] = None,\n        return_metadata: bool = False,\n    ) -&gt; Union[str, Dict[str, Any]]:\n        \"\"\"\n                Executes a single SQL statement asynchronously and returns a meaningful result.\n\n                Supports both read (SELECT) and write (INSERT/UPDATE/DELETE) statements.\n                - For SELECT, returns the fetched records (list of scalars, dicts, or ORM objects).\n                - For INSERT/UPDATE/DELETE, returns a dict with metadata including rowcount,\n                    inserted_primary_key (if available), and any returned rows when the statement\n                    includes RETURNING.\n\n        Args:\n            query (ClauseElement): An SQLAlchemy query object representing the SQL statement to execute.\n            values (Optional[Dict[str, Any]]): A dictionary of parameter values to bind to the query.\n                Defaults to None.\n            return_metadata (bool): When True, returns metadata for DML statements.\n                When False (default), returns \"complete\" for DML statements to preserve legacy behavior.\n\n                Returns:\n                        - For SELECT: List[Any] of records.\n                        - For DML (INSERT/UPDATE/DELETE) when return_metadata=True: Dict with keys:\n                                {\n                                    \"rowcount\": int | None,\n                                    \"inserted_primary_key\": List[Any] | None,\n                                    \"rows\": List[Any] (if statement returns rows)\n                                }\n                        - For DML when return_metadata=False: the string \"complete\".\n                        - On error: Dict[str, str] from handle_exceptions.\n\n        Example:\n            ```python\n            from sqlalchemy import insert\n\n            query = insert(User).values(name='John Doe')\n            result = await db_ops.execute_one(query)\n            ```\n        \"\"\"\n        logger.debug(\"Starting execute_one operation\")\n        try:\n            kind = self._classify_statement(query)\n            # SELECT \u2192 delegate to read_query for consistent shaping\n            if kind == \"select\":\n                logger.debug(\"Detected SELECT; delegating to read_query\")\n                return await self.read_query(query)\n\n            async with self.async_db.get_db_session() as session:\n                logger.debug(f\"Executing query: {query}\")\n                result = await session.execute(query, params=values)\n\n                rows = self._collect_rows(result)\n                meta = self._assemble_dml_metadata(query, result, rows)\n\n                # Commit for non-SELECT statements (DML/DDL). Safe even if no-op.\n                try:\n                    await session.commit()\n                except Exception:  # pragma: no cover\n                    pass\n\n                logger.debug(f\"Query executed successfully with metadata: {meta}\")\n                return meta if return_metadata else \"complete\"\n        except Exception as ex:\n            logger.error(f\"Exception occurred: {ex}\")\n            return handle_exceptions(ex)\n\n    async def execute_many(\n        self,\n        queries: List[Tuple[ClauseElement, Optional[Dict[str, Any]]]],\n        return_results: bool = False,\n    ) -&gt; Union[str, List[Any], Dict[str, str]]:\n        \"\"\"\n                Executes multiple SQL statements asynchronously within a single transaction.\n\n                        Supports a mix of SELECT and DML statements.\n                        When return_results=True, returns a list of per-statement results in the order provided.\n                        Each item is either:\n                        - For SELECT: a list of records.\n                        - For DML: a dict containing rowcount, inserted_primary_key (if available),\n                            and any returned rows (when using RETURNING).\n\n        Args:\n            queries (List[Tuple[ClauseElement, Optional[Dict[str, Any]]]]): A list of tuples, each containing\n                a query and an optional dictionary of parameter values. Each tuple should be of the form\n                `(query, values)` where:\n                    - `query` is an SQLAlchemy query object.\n                    - `values` is a dictionary of parameters to bind to the query (or None).\n\n        Returns:\n            - On success and return_results=True: List[Any | Dict[str, Any]] with one entry per input query.\n            - On success and return_results=False: the string \"complete\" (legacy behavior).\n            - On error: Dict[str, str] from handle_exceptions.\n\n        Example:\n            ```python\n            from sqlalchemy import insert\n\n            queries = [\n                (insert(User), {'name': 'User1'}),\n                (insert(User), {'name': 'User2'}),\n                (insert(User), {'name': 'User3'}),\n            ]\n            result = await db_ops.execute_many(queries)\n            ```\n        \"\"\"\n        logger.debug(\"Starting execute_many operation\")\n        try:\n            results: List[Any] = []\n            async with self.async_db.get_db_session() as session:\n                for query, values in queries:\n                    logger.debug(f\"Executing query: {query}\")\n                    kind = self._classify_statement(query)\n\n                    if kind == \"select\":\n                        data = await self.read_query(query)\n                        if return_results:\n                            results.append(data)\n                        continue\n\n                    res = await session.execute(query, params=values)\n                    rows = self._collect_rows(res)\n                    meta = self._assemble_dml_metadata(query, res, rows)\n                    if return_results:\n                        results.append(meta)\n\n                # Commit once for the whole batch (safe even if some were SELECT)\n                try:\n                    await session.commit()\n                except Exception:  # pragma: no cover\n                    pass\n\n            logger.debug(\"All queries executed successfully with results collected\")\n            return results if return_results else \"complete\"\n        except Exception as ex:\n            logger.error(f\"Exception occurred: {ex}\")\n            return handle_exceptions(ex)\n\n    @deprecated(\"Use `execute_one` with an INSERT query instead.\")\n    async def create_one(self, record):\n        \"\"\"\n        This method is deprecated. Use `execute_one` with an INSERT query instead.\n\n        Adds a single record to the database.\n\n        This asynchronous method accepts a record object and adds it to the\n        database. If the operation is successful, it returns the added record.\n        The method is useful for inserting a new row into a database table.\n\n        Parameters:\n            record (Base): An instance of the SQLAlchemy declarative base class\n            representing the record to be added to the database.\n\n        Returns:\n            Base: The instance of the record that was added to the database.\n\n        Raises:\n            Exception: If any error occurs during the database operation.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # create one record\n            record = await db_ops.create_one(User(name='John Doe'))\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug(\"Starting create_one operation\")\n\n        try:\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the record being added\n                logger.debug(f\"Adding record to session: {record.__dict__}\")\n\n                # Add the record to the session and commit the changes\n                session.add(record)\n                await session.commit()\n\n                # Log the successful record addition\n                logger.debug(f\"Record added successfully: {record}\")\n\n                return record\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the record addition\n            logger.error(f\"Exception occurred: {ex}\")\n            return handle_exceptions(ex)\n\n    @deprecated(\"Use `execute_one` with an INSERT query instead.\")\n    async def create_many(self, records):\n        \"\"\"\n        This method is deprecated. Use `execute_many` with INSERT queries instead.\n\n        Adds multiple records to the database.\n\n        This asynchronous method accepts a list of record objects and adds them\n        to the database. If the operation is successful, it returns the added\n        records. This method is useful for bulk inserting multiple rows into a\n        database table efficiently.\n\n        Parameters:\n            records (list[Base]): A list of instances of the SQLAlchemy\n            declarative base class, each representing a record to be added to\n            the database.\n\n        Returns:\n            list[Base]: A list of instances of the records that were added to\n            the database.\n\n        Raises:\n            Exception: If any error occurs during the database operation.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # create many records\n            records = await db_ops.create_many([User(name='John Doe'), User(name='Jane Doe')])\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug(\"Starting create_many operation\")\n\n        try:\n            # Start a timer to measure the operation time\n            t0 = time.time()\n\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the number of records being added\n                logger.debug(f\"Adding {len(records)} records to session\")\n\n                # Add the records to the session and commit the changes\n                session.add_all(records)\n                await session.commit()\n\n                # Log the added records\n                records_data = [record.__dict__ for record in records]\n                logger.debug(f\"Records added to session: {records_data}\")\n\n                # Calculate the operation time and log the successful record\n                # addition\n                num_records = len(records)\n                t1 = time.time() - t0\n                logger.debug(\n                    f\"Record operations were successful. {num_records} records were created in {t1:.4f} seconds.\"\n                )\n\n                return records\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the record addition\n            logger.error(f\"Exception occurred: {ex}\")\n            return handle_exceptions(ex)\n\n    @deprecated(\"Use `execute_one` with a UPDATE query instead.\")\n    async def update_one(self, table, record_id: str, new_values: dict):\n        \"\"\"\n        This method is deprecated. Use `execute_one` with an UPDATE query instead.\n\n        Updates a single record in the database identified by its ID.\n\n        This asynchronous method takes a SQLAlchemy `Table` object, a record ID,\n        and a dictionary of new values to update the record. It updates the\n        specified record in the given table with the new values. The method does\n        not allow updating certain fields, such as 'id' or 'date_created'.\n\n        Parameters:\n            table (Table): The SQLAlchemy `Table` object representing the table\n            in the database. record_id (str): The ID of the record to be\n            updated. new_values (dict): A dictionary containing the fields to\n            update and their new values.\n\n        Returns:\n            Base: The updated record if successful; otherwise, an error\n            dictionary.\n\n        Raises:\n            Exception: If any error occurs during the update operation.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # update one record\n            record = await db_ops.update_one(User, 1, {'name': 'John Smith'})\n            ```\n        \"\"\"\n        non_updatable_fields = [\"id\", \"date_created\"]\n\n        # Log the start of the operation\n        logger.debug(\n            f\"Starting update_one operation for record_id: {record_id} in table: {table.__name__}\"\n        )\n\n        try:\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the record being fetched\n                logger.debug(f\"Fetching record with id: {record_id}\")\n\n                # Fetch the record\n                record = await session.get(table, record_id)\n                if not record:\n                    # Log the error if no record is found\n                    logger.error(f\"No record found with pkid: {record_id}\")\n                    return {\n                        \"error\": \"Record not found\",\n                        \"details\": f\"No record found with pkid {record_id}\",\n                    }\n\n                # Log the record being updated\n                logger.debug(f\"Updating record with new values: {new_values}\")\n\n                # Update the record with the new values\n                for key, value in new_values.items():\n                    if key not in non_updatable_fields:\n                        setattr(record, key, value)\n                await session.commit()\n\n                # Log the successful record update\n                logger.debug(f\"Record updated successfully: {record.pkid}\")\n                return record\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the record update\n            logger.error(f\"Exception occurred: {ex}\")\n            return handle_exceptions(ex)\n\n    @deprecated(\"Use `execute_many` with a DELETE query instead.\")\n    async def delete_one(self, table, record_id: str):\n        \"\"\"\n        This method is deprecated. Use `execute_one` with a DELETE query instead.\n\n        Deletes a single record from the database based on the provided table\n        and record ID.\n\n        This asynchronous method accepts a SQLAlchemy `Table` object and a\n        record ID. It attempts to delete the record with the given ID from the\n        specified table. If the record is successfully deleted, it returns a\n        success message. If no record with the given ID is found, it returns an\n        error message.\n\n        Args:\n            table (Table): An instance of the SQLAlchemy `Table` class\n            representing the database table from which the record will be\n            deleted. record_id (str): The ID of the record to be deleted.\n\n        Returns:\n            dict: A dictionary containing a success message if the record was\n            deleted successfully, or an error message if the record was not\n            found or an exception occurred.\n\n        Raises:\n            Exception: If any error occurs during the delete operation.\n\n        Example:\n            ```python\n            from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n            )\n            # Create a DBConfig instance\n            config = {\n                # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n                \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n                \"echo\": False,\n                \"future\": True,\n                # \"pool_pre_ping\": True,\n                # \"pool_size\": 10,\n                # \"max_overflow\": 10,\n                \"pool_recycle\": 3600,\n                # \"pool_timeout\": 30,\n            }\n            # create database configuration\n            db_config = database_config.DBConfig(config)\n            # Create an AsyncDatabase instance\n            async_db = async_database.AsyncDatabase(db_config)\n            # Create a DatabaseOperations instance\n            db_ops = database_operations.DatabaseOperations(async_db)\n            # delete one record\n            result = await db_ops.delete_one(User, 1)\n            ```\n        \"\"\"\n        # Log the start of the operation\n        logger.debug(\n            f\"Starting delete_one operation for record_id: {record_id} in table: {table.__name__}\"\n        )\n\n        try:\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the record being fetched\n                logger.debug(f\"Fetching record with id: {record_id}\")\n\n                # Fetch the record\n                record = await session.get(table, record_id)\n\n                # If the record doesn't exist, return an error\n                if not record:\n                    logger.error(f\"No record found with pkid: {record_id}\")\n                    return {\n                        \"error\": \"Record not found\",\n                        \"details\": f\"No record found with pkid {record_id}\",\n                    }\n\n                # Log the record being deleted\n                logger.debug(f\"Deleting record with id: {record_id}\")\n\n                # Delete the record\n                await session.delete(record)\n\n                # Log the successful record deletion from the session\n                logger.debug(f\"Record deleted from session: {record}\")\n\n                # Log the start of the commit\n                logger.debug(\n                    f\"Committing changes to delete record with id: {record_id}\"\n                )\n\n                # Commit the changes\n                await session.commit()\n\n                # Log the successful record deletion\n                logger.debug(f\"Record deleted successfully: {record_id}\")\n\n                return {\"success\": \"Record deleted successfully\"}\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the record deletion\n            logger.error(f\"Exception occurred: {ex}\")\n            return handle_exceptions(ex)\n\n    @deprecated(\"User 'execute_many' with a DELETE query instead.\")\n    async def delete_many(\n        self,\n        table: Type[DeclarativeMeta],\n        id_column_name: str = \"pkid\",\n        id_values: List[int] = None,\n    ) -&gt; int:\n        \"\"\"\n        This method is deprecated. Use `execute_many` with a DELETE query instead.\n\n        Deletes multiple records from the specified table in the database.\n\n        This method takes a table, an optional id column name, and a list of id values. It deletes the records in the table where the id column matches any of the id values in the list.\n\n        Args:\n            table (Type[DeclarativeMeta]): The table from which to delete records.\n            id_column_name (str, optional): The name of the id column in the table. Defaults to \"pkid\".\n            id_values (List[int], optional): A list of id values for the records to delete. Defaults to [].\n\n        Returns:\n            int: The number of records deleted from the table.\n\n        Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            \"pool_recycle\": 3600,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # Delete multiple records\n        deleted_count = await db_ops.delete_many(User, 'id', [1, 2, 3])\n        print(f\"Deleted {deleted_count} records.\")\n        ```\n        \"\"\"\n        if id_values is None:  # pragma: no cover\n            id_values = []\n        try:\n            # Start a timer to measure the operation time\n            t0 = time.time()\n\n            # Start a new database session\n            async with self.async_db.get_db_session() as session:\n                # Log the number of records being deleted\n                logger.debug(f\"Deleting {len(id_values)} records from session\")\n\n                # Create delete statement\n                stmt = delete(table).where(\n                    getattr(table, id_column_name).in_(id_values)\n                )\n\n                # Execute the delete statement and fetch result\n                result = await session.execute(stmt)\n\n                # Commit the changes\n                await session.commit()\n\n                # Get the count of deleted records\n                deleted_count = result.rowcount\n\n                # Log the deleted records\n                logger.debug(f\"Records deleted from session: {deleted_count}\")\n\n                # Calculate the operation time and log the successful record deletion\n                t1 = time.time() - t0\n                logger.debug(\n                    f\"Record operations were successful. {deleted_count} records were deleted in {t1:.4f} seconds.\"\n                )\n\n                return deleted_count\n\n        except Exception as ex:\n            # Handle any exceptions that occur during the record deletion\n            logger.error(f\"Exception occurred: {ex}\")\n            return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.__init__","title":"<code>__init__(async_db)</code>","text":"<p>Initializes a new instance of the DatabaseOperations class.</p> <p>Parameters:</p> Name Type Description Default <code>async_db</code> <code>AsyncDatabase</code> <p>An instance of the</p> required <p>Example: <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n\ndb_config = database_config.DBConfig(config)\n\nasync_db = async_database.AsyncDatabase(db_config)\n\ndb_ops = database_operations.DatabaseOperations(async_db)\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>def __init__(self, async_db: AsyncDatabase):\n    \"\"\"\n    Initializes a new instance of the DatabaseOperations class.\n\n    Args:\n        async_db (module_name.AsyncDatabase): An instance of the\n        AsyncDatabase class for performing asynchronous database operations.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n    )\n\n    config = {\n        # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n        \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n        \"echo\": False,\n        \"future\": True,\n        # \"pool_pre_ping\": True,\n        # \"pool_size\": 10,\n        # \"max_overflow\": 10,\n        \"pool_recycle\": 3600,\n        # \"pool_timeout\": 30,\n    }\n\n    db_config = database_config.DBConfig(config)\n\n    async_db = async_database.AsyncDatabase(db_config)\n\n    db_ops = database_operations.DatabaseOperations(async_db)\n\n    ```\n    \"\"\"\n    # Log the start of the initialization\n    logger.debug(\"Initializing DatabaseOperations instance\")\n\n    # Store the AsyncDatabase instance in the async_db attribute This\n    # instance will be used for performing asynchronous database operations\n    self.async_db = async_db\n\n    # Log the successful initialization\n    logger.debug(\"DatabaseOperations instance initialized successfully\")\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.count_query","title":"<code>count_query(query)</code>  <code>async</code>","text":"<p>Executes a count query on the database and returns the number of records that match the query.</p> <p>This asynchronous method accepts a SQLAlchemy <code>Select</code> query object and returns the count of records that match the query. This is particularly useful for getting the total number of records that satisfy certain conditions without actually fetching the records themselves.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Select</code> <p>A SQLAlchemy <code>Select</code> query object specifying the</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The number of records that match the query.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the execution of the query.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# count query\ncount = await db_ops.count_query(select(User).where(User.age &gt; 30))\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def count_query(self, query):\n    \"\"\"\n    Executes a count query on the database and returns the number of records\n    that match the query.\n\n    This asynchronous method accepts a SQLAlchemy `Select` query object and\n    returns the count of records that match the query. This is particularly\n    useful for getting the total number of records that satisfy certain\n    conditions without actually fetching the records themselves.\n\n    Parameters:\n        query (Select): A SQLAlchemy `Select` query object specifying the\n        conditions to count records for.\n\n    Returns:\n        int: The number of records that match the query.\n\n    Raises:\n        Exception: If any error occurs during the execution of the query.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # count query\n        count = await db_ops.count_query(select(User).where(User.age &gt; 30))\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug(\"Starting count_query operation\")\n\n    try:\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the query being executed\n            logger.debug(f\"Executing count query: {query}\")\n\n            # Execute the count query and retrieve the count\n            result = await session.execute(\n                select(func.count()).select_from(query.subquery())\n            )\n            count = result.scalar()\n\n            # Log the successful query execution\n            logger.debug(f\"Count query executed successfully. Result: {count}\")\n\n            return count\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the query execution\n        logger.error(f\"Exception occurred: {ex}\")\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.create_many","title":"<code>create_many(records)</code>  <code>async</code>","text":"<p>This method is deprecated. Use <code>execute_many</code> with INSERT queries instead.</p> <p>Adds multiple records to the database.</p> <p>This asynchronous method accepts a list of record objects and adds them to the database. If the operation is successful, it returns the added records. This method is useful for bulk inserting multiple rows into a database table efficiently.</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>list[Base]</code> <p>A list of instances of the SQLAlchemy</p> required <p>Returns:</p> Type Description <p>list[Base]: A list of instances of the records that were added to</p> <p>the database.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the database operation.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# create many records\nrecords = await db_ops.create_many([User(name='John Doe'), User(name='Jane Doe')])\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>@deprecated(\"Use `execute_one` with an INSERT query instead.\")\nasync def create_many(self, records):\n    \"\"\"\n    This method is deprecated. Use `execute_many` with INSERT queries instead.\n\n    Adds multiple records to the database.\n\n    This asynchronous method accepts a list of record objects and adds them\n    to the database. If the operation is successful, it returns the added\n    records. This method is useful for bulk inserting multiple rows into a\n    database table efficiently.\n\n    Parameters:\n        records (list[Base]): A list of instances of the SQLAlchemy\n        declarative base class, each representing a record to be added to\n        the database.\n\n    Returns:\n        list[Base]: A list of instances of the records that were added to\n        the database.\n\n    Raises:\n        Exception: If any error occurs during the database operation.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # create many records\n        records = await db_ops.create_many([User(name='John Doe'), User(name='Jane Doe')])\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug(\"Starting create_many operation\")\n\n    try:\n        # Start a timer to measure the operation time\n        t0 = time.time()\n\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the number of records being added\n            logger.debug(f\"Adding {len(records)} records to session\")\n\n            # Add the records to the session and commit the changes\n            session.add_all(records)\n            await session.commit()\n\n            # Log the added records\n            records_data = [record.__dict__ for record in records]\n            logger.debug(f\"Records added to session: {records_data}\")\n\n            # Calculate the operation time and log the successful record\n            # addition\n            num_records = len(records)\n            t1 = time.time() - t0\n            logger.debug(\n                f\"Record operations were successful. {num_records} records were created in {t1:.4f} seconds.\"\n            )\n\n            return records\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the record addition\n        logger.error(f\"Exception occurred: {ex}\")\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.create_one","title":"<code>create_one(record)</code>  <code>async</code>","text":"<p>This method is deprecated. Use <code>execute_one</code> with an INSERT query instead.</p> <p>Adds a single record to the database.</p> <p>This asynchronous method accepts a record object and adds it to the database. If the operation is successful, it returns the added record. The method is useful for inserting a new row into a database table.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Base</code> <p>An instance of the SQLAlchemy declarative base class</p> required <p>Returns:</p> Name Type Description <code>Base</code> <p>The instance of the record that was added to the database.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the database operation.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# create one record\nrecord = await db_ops.create_one(User(name='John Doe'))\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>@deprecated(\"Use `execute_one` with an INSERT query instead.\")\nasync def create_one(self, record):\n    \"\"\"\n    This method is deprecated. Use `execute_one` with an INSERT query instead.\n\n    Adds a single record to the database.\n\n    This asynchronous method accepts a record object and adds it to the\n    database. If the operation is successful, it returns the added record.\n    The method is useful for inserting a new row into a database table.\n\n    Parameters:\n        record (Base): An instance of the SQLAlchemy declarative base class\n        representing the record to be added to the database.\n\n    Returns:\n        Base: The instance of the record that was added to the database.\n\n    Raises:\n        Exception: If any error occurs during the database operation.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # create one record\n        record = await db_ops.create_one(User(name='John Doe'))\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug(\"Starting create_one operation\")\n\n    try:\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the record being added\n            logger.debug(f\"Adding record to session: {record.__dict__}\")\n\n            # Add the record to the session and commit the changes\n            session.add(record)\n            await session.commit()\n\n            # Log the successful record addition\n            logger.debug(f\"Record added successfully: {record}\")\n\n            return record\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the record addition\n        logger.error(f\"Exception occurred: {ex}\")\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.delete_many","title":"<code>delete_many(table, id_column_name='pkid', id_values=None)</code>  <code>async</code>","text":"<p>This method is deprecated. Use <code>execute_many</code> with a DELETE query instead.</p> <p>Deletes multiple records from the specified table in the database.</p> <p>This method takes a table, an optional id column name, and a list of id values. It deletes the records in the table where the id column matches any of the id values in the list.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Type[DeclarativeMeta]</code> <p>The table from which to delete records.</p> required <code>id_column_name</code> <code>str</code> <p>The name of the id column in the table. Defaults to \"pkid\".</p> <code>'pkid'</code> <code>id_values</code> <code>List[int]</code> <p>A list of id values for the records to delete. Defaults to [].</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of records deleted from the table.</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    \"pool_recycle\": 3600,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# Delete multiple records\ndeleted_count = await db_ops.delete_many(User, 'id', [1, 2, 3])\nprint(f\"Deleted {deleted_count} records.\")\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>@deprecated(\"User 'execute_many' with a DELETE query instead.\")\nasync def delete_many(\n    self,\n    table: Type[DeclarativeMeta],\n    id_column_name: str = \"pkid\",\n    id_values: List[int] = None,\n) -&gt; int:\n    \"\"\"\n    This method is deprecated. Use `execute_many` with a DELETE query instead.\n\n    Deletes multiple records from the specified table in the database.\n\n    This method takes a table, an optional id column name, and a list of id values. It deletes the records in the table where the id column matches any of the id values in the list.\n\n    Args:\n        table (Type[DeclarativeMeta]): The table from which to delete records.\n        id_column_name (str, optional): The name of the id column in the table. Defaults to \"pkid\".\n        id_values (List[int], optional): A list of id values for the records to delete. Defaults to [].\n\n    Returns:\n        int: The number of records deleted from the table.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n    )\n    # Create a DBConfig instance\n    config = {\n        \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n        \"echo\": False,\n        \"future\": True,\n        \"pool_recycle\": 3600,\n    }\n    # create database configuration\n    db_config = database_config.DBConfig(config)\n    # Create an AsyncDatabase instance\n    async_db = async_database.AsyncDatabase(db_config)\n    # Create a DatabaseOperations instance\n    db_ops = database_operations.DatabaseOperations(async_db)\n    # Delete multiple records\n    deleted_count = await db_ops.delete_many(User, 'id', [1, 2, 3])\n    print(f\"Deleted {deleted_count} records.\")\n    ```\n    \"\"\"\n    if id_values is None:  # pragma: no cover\n        id_values = []\n    try:\n        # Start a timer to measure the operation time\n        t0 = time.time()\n\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the number of records being deleted\n            logger.debug(f\"Deleting {len(id_values)} records from session\")\n\n            # Create delete statement\n            stmt = delete(table).where(\n                getattr(table, id_column_name).in_(id_values)\n            )\n\n            # Execute the delete statement and fetch result\n            result = await session.execute(stmt)\n\n            # Commit the changes\n            await session.commit()\n\n            # Get the count of deleted records\n            deleted_count = result.rowcount\n\n            # Log the deleted records\n            logger.debug(f\"Records deleted from session: {deleted_count}\")\n\n            # Calculate the operation time and log the successful record deletion\n            t1 = time.time() - t0\n            logger.debug(\n                f\"Record operations were successful. {deleted_count} records were deleted in {t1:.4f} seconds.\"\n            )\n\n            return deleted_count\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the record deletion\n        logger.error(f\"Exception occurred: {ex}\")\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.delete_one","title":"<code>delete_one(table, record_id)</code>  <code>async</code>","text":"<p>This method is deprecated. Use <code>execute_one</code> with a DELETE query instead.</p> <p>Deletes a single record from the database based on the provided table and record ID.</p> <p>This asynchronous method accepts a SQLAlchemy <code>Table</code> object and a record ID. It attempts to delete the record with the given ID from the specified table. If the record is successfully deleted, it returns a success message. If no record with the given ID is found, it returns an error message.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>An instance of the SQLAlchemy <code>Table</code> class</p> required <code>deleted.</code> <code>record_id (str</code> <p>The ID of the record to be deleted.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing a success message if the record was</p> <p>deleted successfully, or an error message if the record was not</p> <p>found or an exception occurred.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the delete operation.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# delete one record\nresult = await db_ops.delete_one(User, 1)\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>@deprecated(\"Use `execute_many` with a DELETE query instead.\")\nasync def delete_one(self, table, record_id: str):\n    \"\"\"\n    This method is deprecated. Use `execute_one` with a DELETE query instead.\n\n    Deletes a single record from the database based on the provided table\n    and record ID.\n\n    This asynchronous method accepts a SQLAlchemy `Table` object and a\n    record ID. It attempts to delete the record with the given ID from the\n    specified table. If the record is successfully deleted, it returns a\n    success message. If no record with the given ID is found, it returns an\n    error message.\n\n    Args:\n        table (Table): An instance of the SQLAlchemy `Table` class\n        representing the database table from which the record will be\n        deleted. record_id (str): The ID of the record to be deleted.\n\n    Returns:\n        dict: A dictionary containing a success message if the record was\n        deleted successfully, or an error message if the record was not\n        found or an exception occurred.\n\n    Raises:\n        Exception: If any error occurs during the delete operation.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # delete one record\n        result = await db_ops.delete_one(User, 1)\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug(\n        f\"Starting delete_one operation for record_id: {record_id} in table: {table.__name__}\"\n    )\n\n    try:\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the record being fetched\n            logger.debug(f\"Fetching record with id: {record_id}\")\n\n            # Fetch the record\n            record = await session.get(table, record_id)\n\n            # If the record doesn't exist, return an error\n            if not record:\n                logger.error(f\"No record found with pkid: {record_id}\")\n                return {\n                    \"error\": \"Record not found\",\n                    \"details\": f\"No record found with pkid {record_id}\",\n                }\n\n            # Log the record being deleted\n            logger.debug(f\"Deleting record with id: {record_id}\")\n\n            # Delete the record\n            await session.delete(record)\n\n            # Log the successful record deletion from the session\n            logger.debug(f\"Record deleted from session: {record}\")\n\n            # Log the start of the commit\n            logger.debug(\n                f\"Committing changes to delete record with id: {record_id}\"\n            )\n\n            # Commit the changes\n            await session.commit()\n\n            # Log the successful record deletion\n            logger.debug(f\"Record deleted successfully: {record_id}\")\n\n            return {\"success\": \"Record deleted successfully\"}\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the record deletion\n        logger.error(f\"Exception occurred: {ex}\")\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.execute_many","title":"<code>execute_many(queries, return_results=False)</code>  <code>async</code>","text":"<pre><code>    Executes multiple SQL statements asynchronously within a single transaction.\n\n            Supports a mix of SELECT and DML statements.\n            When return_results=True, returns a list of per-statement results in the order provided.\n            Each item is either:\n            - For SELECT: a list of records.\n            - For DML: a dict containing rowcount, inserted_primary_key (if available),\n                and any returned rows (when using RETURNING).\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>List[Tuple[ClauseElement, Optional[Dict[str, Any]]]]</code> <p>A list of tuples, each containing a query and an optional dictionary of parameter values. Each tuple should be of the form <code>(query, values)</code> where:     - <code>query</code> is an SQLAlchemy query object.     - <code>values</code> is a dictionary of parameters to bind to the query (or None).</p> required <p>Returns:</p> Type Description <code>Union[str, List[Any], Dict[str, str]]</code> <ul> <li>On success and return_results=True: List[Any | Dict[str, Any]] with one entry per input query.</li> </ul> <code>Union[str, List[Any], Dict[str, str]]</code> <ul> <li>On success and return_results=False: the string \"complete\" (legacy behavior).</li> </ul> <code>Union[str, List[Any], Dict[str, str]]</code> <ul> <li>On error: Dict[str, str] from handle_exceptions.</li> </ul> Example <pre><code>from sqlalchemy import insert\n\nqueries = [\n    (insert(User), {'name': 'User1'}),\n    (insert(User), {'name': 'User2'}),\n    (insert(User), {'name': 'User3'}),\n]\nresult = await db_ops.execute_many(queries)\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def execute_many(\n    self,\n    queries: List[Tuple[ClauseElement, Optional[Dict[str, Any]]]],\n    return_results: bool = False,\n) -&gt; Union[str, List[Any], Dict[str, str]]:\n    \"\"\"\n            Executes multiple SQL statements asynchronously within a single transaction.\n\n                    Supports a mix of SELECT and DML statements.\n                    When return_results=True, returns a list of per-statement results in the order provided.\n                    Each item is either:\n                    - For SELECT: a list of records.\n                    - For DML: a dict containing rowcount, inserted_primary_key (if available),\n                        and any returned rows (when using RETURNING).\n\n    Args:\n        queries (List[Tuple[ClauseElement, Optional[Dict[str, Any]]]]): A list of tuples, each containing\n            a query and an optional dictionary of parameter values. Each tuple should be of the form\n            `(query, values)` where:\n                - `query` is an SQLAlchemy query object.\n                - `values` is a dictionary of parameters to bind to the query (or None).\n\n    Returns:\n        - On success and return_results=True: List[Any | Dict[str, Any]] with one entry per input query.\n        - On success and return_results=False: the string \"complete\" (legacy behavior).\n        - On error: Dict[str, str] from handle_exceptions.\n\n    Example:\n        ```python\n        from sqlalchemy import insert\n\n        queries = [\n            (insert(User), {'name': 'User1'}),\n            (insert(User), {'name': 'User2'}),\n            (insert(User), {'name': 'User3'}),\n        ]\n        result = await db_ops.execute_many(queries)\n        ```\n    \"\"\"\n    logger.debug(\"Starting execute_many operation\")\n    try:\n        results: List[Any] = []\n        async with self.async_db.get_db_session() as session:\n            for query, values in queries:\n                logger.debug(f\"Executing query: {query}\")\n                kind = self._classify_statement(query)\n\n                if kind == \"select\":\n                    data = await self.read_query(query)\n                    if return_results:\n                        results.append(data)\n                    continue\n\n                res = await session.execute(query, params=values)\n                rows = self._collect_rows(res)\n                meta = self._assemble_dml_metadata(query, res, rows)\n                if return_results:\n                    results.append(meta)\n\n            # Commit once for the whole batch (safe even if some were SELECT)\n            try:\n                await session.commit()\n            except Exception:  # pragma: no cover\n                pass\n\n        logger.debug(\"All queries executed successfully with results collected\")\n        return results if return_results else \"complete\"\n    except Exception as ex:\n        logger.error(f\"Exception occurred: {ex}\")\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.execute_one","title":"<code>execute_one(query, values=None, return_metadata=False)</code>  <code>async</code>","text":"<pre><code>    Executes a single SQL statement asynchronously and returns a meaningful result.\n\n    Supports both read (SELECT) and write (INSERT/UPDATE/DELETE) statements.\n    - For SELECT, returns the fetched records (list of scalars, dicts, or ORM objects).\n    - For INSERT/UPDATE/DELETE, returns a dict with metadata including rowcount,\n        inserted_primary_key (if available), and any returned rows when the statement\n        includes RETURNING.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>ClauseElement</code> <p>An SQLAlchemy query object representing the SQL statement to execute.</p> required <code>values</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary of parameter values to bind to the query. Defaults to None.</p> <code>None</code> <code>return_metadata</code> <code>bool</code> <p>When True, returns metadata for DML statements. When False (default), returns \"complete\" for DML statements to preserve legacy behavior.</p> <p>Returns:         - For SELECT: List[Any] of records.         - For DML (INSERT/UPDATE/DELETE) when return_metadata=True: Dict with keys:                 {                     \"rowcount\": int | None,                     \"inserted_primary_key\": List[Any] | None,                     \"rows\": List[Any] (if statement returns rows)                 }         - For DML when return_metadata=False: the string \"complete\".         - On error: Dict[str, str] from handle_exceptions.</p> <code>False</code> Example <pre><code>from sqlalchemy import insert\n\nquery = insert(User).values(name='John Doe')\nresult = await db_ops.execute_one(query)\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def execute_one(\n    self,\n    query: ClauseElement,\n    values: Optional[Dict[str, Any]] = None,\n    return_metadata: bool = False,\n) -&gt; Union[str, Dict[str, Any]]:\n    \"\"\"\n            Executes a single SQL statement asynchronously and returns a meaningful result.\n\n            Supports both read (SELECT) and write (INSERT/UPDATE/DELETE) statements.\n            - For SELECT, returns the fetched records (list of scalars, dicts, or ORM objects).\n            - For INSERT/UPDATE/DELETE, returns a dict with metadata including rowcount,\n                inserted_primary_key (if available), and any returned rows when the statement\n                includes RETURNING.\n\n    Args:\n        query (ClauseElement): An SQLAlchemy query object representing the SQL statement to execute.\n        values (Optional[Dict[str, Any]]): A dictionary of parameter values to bind to the query.\n            Defaults to None.\n        return_metadata (bool): When True, returns metadata for DML statements.\n            When False (default), returns \"complete\" for DML statements to preserve legacy behavior.\n\n            Returns:\n                    - For SELECT: List[Any] of records.\n                    - For DML (INSERT/UPDATE/DELETE) when return_metadata=True: Dict with keys:\n                            {\n                                \"rowcount\": int | None,\n                                \"inserted_primary_key\": List[Any] | None,\n                                \"rows\": List[Any] (if statement returns rows)\n                            }\n                    - For DML when return_metadata=False: the string \"complete\".\n                    - On error: Dict[str, str] from handle_exceptions.\n\n    Example:\n        ```python\n        from sqlalchemy import insert\n\n        query = insert(User).values(name='John Doe')\n        result = await db_ops.execute_one(query)\n        ```\n    \"\"\"\n    logger.debug(\"Starting execute_one operation\")\n    try:\n        kind = self._classify_statement(query)\n        # SELECT \u2192 delegate to read_query for consistent shaping\n        if kind == \"select\":\n            logger.debug(\"Detected SELECT; delegating to read_query\")\n            return await self.read_query(query)\n\n        async with self.async_db.get_db_session() as session:\n            logger.debug(f\"Executing query: {query}\")\n            result = await session.execute(query, params=values)\n\n            rows = self._collect_rows(result)\n            meta = self._assemble_dml_metadata(query, result, rows)\n\n            # Commit for non-SELECT statements (DML/DDL). Safe even if no-op.\n            try:\n                await session.commit()\n            except Exception:  # pragma: no cover\n                pass\n\n            logger.debug(f\"Query executed successfully with metadata: {meta}\")\n            return meta if return_metadata else \"complete\"\n    except Exception as ex:\n        logger.error(f\"Exception occurred: {ex}\")\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.get_columns_details","title":"<code>get_columns_details(table)</code>  <code>async</code>","text":"<p>Retrieves the details of the columns of a given table.</p> <p>This asynchronous method accepts a table object and returns a dictionary. Each key in the dictionary is a column name from the table, and the corresponding value is another dictionary containing details about that column, such as type, if it's nullable, if it's a primary key, if it's unique, its autoincrement status, and its default value.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>An instance of the SQLAlchemy Table class</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where each key is a column name, and each value</p> <p>is a dictionary with the column's details.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the database operation.</p> <p>Example: <pre><code>from sqlalchemy import Table, MetaData, Column,\nInteger, String from dsg_lib.async_database_functions import module_name metadata = MetaData()\nmy_table = Table('my_table', metadata,\n                Column('id', Integer, primary_key=True), Column('name',\n                String))\n\nfrom dsg_lib.async_database_functions import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# get columns details\ncolumns = await db_ops.get_columns_details(my_table)\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def get_columns_details(self, table):\n    \"\"\"\n    Retrieves the details of the columns of a given table.\n\n    This asynchronous method accepts a table object and returns a\n    dictionary. Each key in the dictionary is a column name from the table,\n    and the corresponding value is another dictionary containing details\n    about that column, such as type, if it's nullable, if it's a primary\n    key, if it's unique, its autoincrement status, and its default value.\n\n    Args:\n        table (Table): An instance of the SQLAlchemy Table class\n        representing the database table for which column details are\n        required.\n\n    Returns:\n        dict: A dictionary where each key is a column name, and each value\n        is a dictionary with the column's details.\n\n    Raises:\n        Exception: If any error occurs during the database operation.\n\n    Example:\n    ```python\n    from sqlalchemy import Table, MetaData, Column,\n    Integer, String from dsg_lib.async_database_functions import module_name metadata = MetaData()\n    my_table = Table('my_table', metadata,\n                    Column('id', Integer, primary_key=True), Column('name',\n                    String))\n\n    from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n    )\n    # Create a DBConfig instance\n    config = {\n        # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n        \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n        \"echo\": False,\n        \"future\": True,\n        # \"pool_pre_ping\": True,\n        # \"pool_size\": 10,\n        # \"max_overflow\": 10,\n        \"pool_recycle\": 3600,\n        # \"pool_timeout\": 30,\n    }\n    # create database configuration\n    db_config = database_config.DBConfig(config)\n    # Create an AsyncDatabase instance\n    async_db = async_database.AsyncDatabase(db_config)\n    # Create a DatabaseOperations instance\n    db_ops = database_operations.DatabaseOperations(async_db)\n    # get columns details\n    columns = await db_ops.get_columns_details(my_table)\n    ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug(\n        f\"Starting get_columns_details operation for table: {table.__name__}\"\n    )\n\n    try:\n        # Log the start of the column retrieval\n        logger.debug(f\"Getting columns for table: {table.__name__}\")\n\n        # Retrieve the details of the columns and store them in a dictionary\n        # The keys are the column names and the values are dictionaries\n        # containing the column details\n        columns = {\n            c.name: {\n                \"type\": str(c.type),\n                \"nullable\": c.nullable,\n                \"primary_key\": c.primary_key,\n                \"unique\": c.unique,\n                \"autoincrement\": c.autoincrement,\n                \"default\": (\n                    str(c.default.arg)\n                    if c.default is not None and not callable(c.default.arg)\n                    else None\n                ),\n            }\n            for c in table.__table__.columns\n        }\n\n        # Log the successful column retrieval\n        logger.debug(f\"Successfully retrieved columns for table: {table.__name__}\")\n\n        return columns\n    except Exception as ex:  # pragma: no cover\n        # Handle any exceptions that occur during the column retrieval\n        logger.error(\n            f\"An error occurred while getting columns for table: {table.__name__}\"\n        )  # pragma: no cover\n        return handle_exceptions(ex)  # pragma: no cover\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.get_primary_keys","title":"<code>get_primary_keys(table)</code>  <code>async</code>","text":"<p>Retrieves the primary keys of a given table.</p> <p>This asynchronous method accepts a table object and returns a list containing the names of its primary keys. It is useful for understanding the structure of the table and for operations that require knowledge of the primary keys.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>An instance of the SQLAlchemy Table class</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list containing the names of the primary keys of the table.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the database operation.</p> Example <pre><code>from sqlalchemy import Table, MetaData, Column, Integer,\n    String from dsg_lib.async_database_functions import module_name metadata = MetaData()\n    my_table = Table('my_table', metadata,\n                    Column('id', Integer, primary_key=True),\n                    Column('name', String, primary_key=True))\nfrom dsg_lib.async_database_functions import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n\n# get primary keys\nprimary_keys = await db_ops.get_primary_keys(my_table)\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def get_primary_keys(self, table):\n    \"\"\"\n    Retrieves the primary keys of a given table.\n\n    This asynchronous method accepts a table object and returns a list\n    containing the names of its primary keys. It is useful for understanding\n    the structure of the table and for operations that require knowledge of\n    the primary keys.\n\n    Args:\n        table (Table): An instance of the SQLAlchemy Table class\n        representing the database table for which primary keys are required.\n\n    Returns:\n        list: A list containing the names of the primary keys of the table.\n\n    Raises:\n        Exception: If any error occurs during the database operation.\n\n    Example:\n        ```python\n        from sqlalchemy import Table, MetaData, Column, Integer,\n            String from dsg_lib.async_database_functions import module_name metadata = MetaData()\n            my_table = Table('my_table', metadata,\n                            Column('id', Integer, primary_key=True),\n                            Column('name', String, primary_key=True))\n        from dsg_lib.async_database_functions import (\n            async_database,\n            base_schema,\n            database_config,\n            database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n\n        # get primary keys\n        primary_keys = await db_ops.get_primary_keys(my_table)\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug(f\"Starting get_primary_keys operation for table: {table.__name__}\")\n\n    try:\n        # Log the start of the primary key retrieval\n        logger.debug(f\"Getting primary keys for table: {table.__name__}\")\n\n        # Retrieve the primary keys and store them in a list\n        primary_keys = table.__table__.primary_key.columns.keys()\n\n        # Log the successful primary key retrieval\n        logger.debug(f\"Primary keys retrieved successfully: {primary_keys}\")\n\n        return primary_keys\n\n    except Exception as ex:  # pragma: no cover\n        # Handle any exceptions that occur during the primary key retrieval\n        logger.error(f\"Exception occurred: {ex}\")  # pragma: no cover\n        return handle_exceptions(ex)  # pragma: no cover\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.get_table_names","title":"<code>get_table_names()</code>  <code>async</code>","text":"<p>Retrieves the names of all tables in the database.</p> <p>This asynchronous method returns a list containing the names of all tables in the database. It is useful for database introspection, allowing the user to know which tables are available in the current database context.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list containing the names of all tables in the database.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the database operation.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# get table names\ntable_names = await db_ops.get_table_names()\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def get_table_names(self):\n    \"\"\"\n    Retrieves the names of all tables in the database.\n\n    This asynchronous method returns a list containing the names of all\n    tables in the database. It is useful for database introspection,\n    allowing the user to know which tables are available in the current\n    database context.\n\n    Returns:\n        list: A list containing the names of all tables in the database.\n\n    Raises:\n        Exception: If any error occurs during the database operation.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # get table names\n        table_names = await db_ops.get_table_names()\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug(\"Starting get_table_names operation\")\n\n    try:\n        # Log the start of the table name retrieval\n        logger.debug(\"Retrieving table names\")\n\n        # Retrieve the table names and store them in a list The keys of the\n        # metadata.tables dictionary are the table names\n        table_names = list(self.async_db.Base.metadata.tables.keys())\n\n        # Log the successful table name retrieval\n        logger.debug(f\"Table names retrieved successfully: {table_names}\")\n\n        return table_names\n\n    except Exception as ex:  # pragma: no cover\n        # Handle any exceptions that occur during the table name retrieval\n        logger.error(f\"Exception occurred: {ex}\")  # pragma: no cover\n        return handle_exceptions(ex)  # pragma: no cover\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.read_multi_query","title":"<code>read_multi_query(queries)</code>  <code>async</code>","text":"<p>Executes multiple fetch queries asynchronously and returns a dictionary of results for each query.</p> <p>This asynchronous method accepts a dictionary where each key is a query name (str) and each value is a SQLAlchemy <code>Select</code> query object. It executes each query within a single database session and collects the results. The results are returned as a dictionary mapping each query name to a list of records that match that query.</p> <p>The function automatically determines the structure of each result set: - If the query returns a single column, the result will be a list of scalar values. - If the query returns multiple columns, the result will be a list of dictionaries mapping column names to values. - If the result row is an ORM object, it will be returned as-is.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Dict[str, Select]</code> <p>A dictionary mapping query names to SQLAlchemy <code>Select</code> query objects.</p> required <p>Returns:</p> Type Description <p>Dict[str, List[Any]]: A dictionary where each key is a query name and each value is a list of records</p> <p>(scalars, dictionaries, or ORM objects) that match the corresponding query.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the execution of any query, the function logs the error and</p> Example <pre><code>from sqlalchemy import select\nqueries = {\n    \"adults\": select(User).where(User.age &gt;= 18),\n    \"minors\": select(User).where(User.age &lt; 18),\n}\nresults = await db_ops.read_multi_query(queries)\n# results[\"adults\"] and results[\"minors\"] will contain lists of records\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def read_multi_query(self, queries: Dict[str, str]):\n    \"\"\"\n    Executes multiple fetch queries asynchronously and returns a dictionary of results for each query.\n\n    This asynchronous method accepts a dictionary where each key is a query name (str)\n    and each value is a SQLAlchemy `Select` query object. It executes each query within a single\n    database session and collects the results. The results are returned as a dictionary mapping\n    each query name to a list of records that match that query.\n\n    The function automatically determines the structure of each result set:\n    - If the query returns a single column, the result will be a list of scalar values.\n    - If the query returns multiple columns, the result will be a list of dictionaries mapping column names to values.\n    - If the result row is an ORM object, it will be returned as-is.\n\n    Args:\n        queries (Dict[str, Select]): A dictionary mapping query names to SQLAlchemy `Select` query objects.\n\n    Returns:\n        Dict[str, List[Any]]: A dictionary where each key is a query name and each value is a list of records\n        (scalars, dictionaries, or ORM objects) that match the corresponding query.\n\n    Raises:\n        Exception: If any error occurs during the execution of any query, the function logs the error and\n        returns a dictionary with error details using `handle_exceptions`.\n\n    Example:\n        ```python\n        from sqlalchemy import select\n        queries = {\n            \"adults\": select(User).where(User.age &gt;= 18),\n            \"minors\": select(User).where(User.age &lt; 18),\n        }\n        results = await db_ops.read_multi_query(queries)\n        # results[\"adults\"] and results[\"minors\"] will contain lists of records\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug(\"Starting read_multi_query operation\")\n\n    try:\n        results = {}\n        async with self.async_db.get_db_session() as session:\n            for query_name, query in queries.items():\n                logger.debug(f\"Executing fetch query: {query}\")\n                result = await session.execute(query)\n                if hasattr(result, \"keys\") and callable(result.keys):\n                    keys = result.keys()\n                    if len(keys) == 1:\n                        data = result.scalars().all()\n                    else:\n                        rows = result.fetchall()\n                        data = []\n                        for row in rows:\n                            if hasattr(row, \"_mapping\"):\n                                mapping = row._mapping\n                                if len(mapping) == 1: # pragma: no cover\n                                    data.append(list(mapping.values())[0]) # pragma: no cover\n                                else:\n                                    data.append(dict(mapping))\n                            elif hasattr(row, \"__dict__\"): # pragma: no cover\n                                data.append(row) # pragma: no cover\n                            else:# pragma: no cover\n                                data.append(row)# pragma: no cover\n                else:\n                    rows = result.fetchall()\n                    data = []\n                    for row in rows:\n                        if hasattr(row, \"_mapping\"):\n                            mapping = row._mapping\n                            if len(mapping) == 1:# pragma: no cover\n                                data.append(list(mapping.values())[0])# pragma: no cover\n                            else:\n                                data.append(dict(mapping))\n                        elif hasattr(row, \"__dict__\"):\n                            data.append(row)\n                        else:\n                            data.append(row)# pragma: no cover\n                results[query_name] = data\n        return results\n\n    except Exception as ex:\n        logger.error(f\"Exception occurred: {ex}\")\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.read_one_record","title":"<code>read_one_record(query)</code>  <code>async</code>","text":"<p>Retrieves a single record from the database based on the provided query.</p> <p>This asynchronous method accepts a SQL query object and returns the first record that matches the query. If no record matches the query, it returns None. This method is useful for fetching specific data when the expected result is a single record.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Select</code> <p>An instance of the SQLAlchemy Select class,</p> required <p>Returns:</p> Name Type Description <code>Result</code> <p>The first record that matches the query or None if no record matches.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the database operation.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# read one record\nrecord = await db_ops.read_one_record(select(User).where(User.name == 'John Doe'))\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>async def read_one_record(self, query):\n    \"\"\"\n    Retrieves a single record from the database based on the provided query.\n\n    This asynchronous method accepts a SQL query object and returns the\n    first record that matches the query. If no record matches the query, it\n    returns None. This method is useful for fetching specific data\n    when the expected result is a single record.\n\n    Parameters:\n        query (Select): An instance of the SQLAlchemy Select class,\n        representing the query to be executed.\n\n    Returns:\n        Result: The first record that matches the query or None if no record matches.\n\n    Raises:\n        Exception: If any error occurs during the database operation.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # read one record\n        record = await db_ops.read_one_record(select(User).where(User.name == 'John Doe'))\n        ```\n    \"\"\"\n    # Log the start of the operation\n    logger.debug(f\"Starting read_one_record operation for {query}\")\n\n    try:\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the start of the record retrieval\n            logger.debug(f\"Getting record with query: {query}\")\n\n            # Execute the query and retrieve the first record\n            result = await session.execute(query)\n            record = result.scalar_one()\n\n            # Log the successful record retrieval\n            logger.debug(f\"Record retrieved successfully: {record}\")\n\n            return record\n\n    except NoResultFound:\n        # No record was found\n        logger.debug(\"No record found\")\n        return None\n\n    except Exception as ex:  # pragma: no cover\n        # Handle any exceptions that occur during the record retrieval\n        logger.error(f\"Exception occurred: {ex}\")  # pragma: no cover\n        return handle_exceptions(ex)  # pragma: no cover\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.DatabaseOperations.update_one","title":"<code>update_one(table, record_id, new_values)</code>  <code>async</code>","text":"<p>This method is deprecated. Use <code>execute_one</code> with an UPDATE query instead.</p> <p>Updates a single record in the database identified by its ID.</p> <p>This asynchronous method takes a SQLAlchemy <code>Table</code> object, a record ID, and a dictionary of new values to update the record. It updates the specified record in the given table with the new values. The method does not allow updating certain fields, such as 'id' or 'date_created'.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The SQLAlchemy <code>Table</code> object representing the table</p> required <code>in</code> <code>the database. record_id (str</code> <p>The ID of the record to be</p> required <code>updated.</code> <code>new_values (dict</code> <p>A dictionary containing the fields to</p> required <p>Returns:</p> Name Type Description <code>Base</code> <p>The updated record if successful; otherwise, an error</p> <p>dictionary.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If any error occurs during the update operation.</p> Example <pre><code>from dsg_lib.async_database_functions import (\nasync_database,\nbase_schema,\ndatabase_config,\ndatabase_operations,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n# update one record\nrecord = await db_ops.update_one(User, 1, {'name': 'John Smith'})\n</code></pre> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>@deprecated(\"Use `execute_one` with a UPDATE query instead.\")\nasync def update_one(self, table, record_id: str, new_values: dict):\n    \"\"\"\n    This method is deprecated. Use `execute_one` with an UPDATE query instead.\n\n    Updates a single record in the database identified by its ID.\n\n    This asynchronous method takes a SQLAlchemy `Table` object, a record ID,\n    and a dictionary of new values to update the record. It updates the\n    specified record in the given table with the new values. The method does\n    not allow updating certain fields, such as 'id' or 'date_created'.\n\n    Parameters:\n        table (Table): The SQLAlchemy `Table` object representing the table\n        in the database. record_id (str): The ID of the record to be\n        updated. new_values (dict): A dictionary containing the fields to\n        update and their new values.\n\n    Returns:\n        Base: The updated record if successful; otherwise, an error\n        dictionary.\n\n    Raises:\n        Exception: If any error occurs during the update operation.\n\n    Example:\n        ```python\n        from dsg_lib.async_database_functions import (\n        async_database,\n        base_schema,\n        database_config,\n        database_operations,\n        )\n        # Create a DBConfig instance\n        config = {\n            # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n            \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n            \"echo\": False,\n            \"future\": True,\n            # \"pool_pre_ping\": True,\n            # \"pool_size\": 10,\n            # \"max_overflow\": 10,\n            \"pool_recycle\": 3600,\n            # \"pool_timeout\": 30,\n        }\n        # create database configuration\n        db_config = database_config.DBConfig(config)\n        # Create an AsyncDatabase instance\n        async_db = async_database.AsyncDatabase(db_config)\n        # Create a DatabaseOperations instance\n        db_ops = database_operations.DatabaseOperations(async_db)\n        # update one record\n        record = await db_ops.update_one(User, 1, {'name': 'John Smith'})\n        ```\n    \"\"\"\n    non_updatable_fields = [\"id\", \"date_created\"]\n\n    # Log the start of the operation\n    logger.debug(\n        f\"Starting update_one operation for record_id: {record_id} in table: {table.__name__}\"\n    )\n\n    try:\n        # Start a new database session\n        async with self.async_db.get_db_session() as session:\n            # Log the record being fetched\n            logger.debug(f\"Fetching record with id: {record_id}\")\n\n            # Fetch the record\n            record = await session.get(table, record_id)\n            if not record:\n                # Log the error if no record is found\n                logger.error(f\"No record found with pkid: {record_id}\")\n                return {\n                    \"error\": \"Record not found\",\n                    \"details\": f\"No record found with pkid {record_id}\",\n                }\n\n            # Log the record being updated\n            logger.debug(f\"Updating record with new values: {new_values}\")\n\n            # Update the record with the new values\n            for key, value in new_values.items():\n                if key not in non_updatable_fields:\n                    setattr(record, key, value)\n            await session.commit()\n\n            # Log the successful record update\n            logger.debug(f\"Record updated successfully: {record.pkid}\")\n            return record\n\n    except Exception as ex:\n        # Handle any exceptions that occur during the record update\n        logger.error(f\"Exception occurred: {ex}\")\n        return handle_exceptions(ex)\n</code></pre>"},{"location":"database/database_operations/#dsg_lib.async_database_functions.database_operations.handle_exceptions","title":"<code>handle_exceptions(ex)</code>","text":"<p>Handles exceptions for database operations.</p> <p>This function checks the type of the exception, logs an appropriate error message, and returns a dictionary containing the error details.</p> <p>Parameters:</p> Name Type Description Default <code>ex</code> <code>Exception</code> <p>The exception to handle.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, str]</code> <p>A dictionary containing the error details. The dictionary has two</p> <code>keys</code> <code>Dict[str, str]</code> <p>'error' and 'details'.</p> <p>Example: <pre><code>from dsg_lib.async_database_functions import database_operations\n\ntry:\n    # Some database operation that might raise an exception pass\nexcept Exception as ex:\n    error_details = database_operations.handle_exceptions(ex)\n    print(error_details)\n</code></pre></p> Source code in <code>dsg_lib/async_database_functions/database_operations.py</code> <pre><code>def handle_exceptions(ex: Exception) -&gt; Dict[str, str]:\n    \"\"\"\n    Handles exceptions for database operations.\n\n    This function checks the type of the exception, logs an appropriate error\n    message, and returns a dictionary containing the error details.\n\n    Args:\n        ex (Exception): The exception to handle.\n\n    Returns:\n        dict: A dictionary containing the error details. The dictionary has two\n        keys: 'error' and 'details'.\n\n    Example:\n    ```python\n    from dsg_lib.async_database_functions import database_operations\n\n    try:\n        # Some database operation that might raise an exception pass\n    except Exception as ex:\n        error_details = database_operations.handle_exceptions(ex)\n        print(error_details)\n    ```\n    \"\"\"\n    # Extract the error message before the SQL statement\n    error_only = str(ex).split(\"[SQL:\")[0]\n\n    # Check the type of the exception\n    if isinstance(ex, IntegrityError):\n        # Log the error and return the error details\n        logger.error(f\"IntegrityError occurred: {ex}\")\n        return {\"error\": \"IntegrityError\", \"details\": error_only}\n    elif isinstance(ex, SQLAlchemyError):\n        # Log the error and return the error details\n        logger.error(f\"SQLAlchemyError occurred: {ex}\")\n        return {\"error\": \"SQLAlchemyError\", \"details\": error_only}\n    else:\n        # Log the error and return the error details\n        logger.error(f\"Exception occurred: {ex}\")\n        return {\"error\": \"General Exception\", \"details\": str(ex)}\n</code></pre>"},{"location":"database/database_queries_tested/","title":"Tested Queries","text":""},{"location":"database/database_queries_tested/#database-queries-covered-by-tests","title":"Database queries covered by tests","text":"<p>This page lists, function-by-function, which query patterns are exercised by the automated test suite. It\u2019s meant to give users of the library a clear picture of what\u2019s verified in CI, especially for the enhanced execute_one and execute_many APIs.</p> <p>Context - Runtime/driver in tests: SQLite (aiosqlite), in-memory shared cache - ORM: SQLAlchemy Async (Core and ORM) - Result shaping rules validated by tests:     - SELECT with one column \u2192 list of scalars     - SELECT with multiple columns \u2192 list of dicts (column name \u2192 value)     - SELECT with ORM entity \u2192 list of ORM objects     - For DML with RETURNING (backend-dependent): rows appear in metadata when present</p> <p>Note: Some branches are validated via lightweight mocks to assert fallback behavior when drivers return different row shapes.</p>"},{"location":"database/database_queries_tested/#execute_onequery-valuesnone-return_metadatafalse","title":"execute_one(query, values=None, return_metadata=False)","text":"<p>Tested SELECT paths - ORM SELECT(User) returns a list of ORM objects. - Text SELECT single column returns scalars in metadata when return_metadata=True. - Text SELECT multiple columns returns list of dicts in metadata when return_metadata=True. - Row-shaping fallbacks via mocks:     - keys-present, mapping with one key \u2192 rows as scalars     - keys-present, objects with dict \u2192 rows contain objects     - keys-present, plain objects \u2192 rows contain objects     - no-keys path, mapping with one key \u2192 rows as scalars     - no-keys path, mapping with multiple keys \u2192 rows as dicts     - no-keys path, dict and plain object branches</p> <p>Tested DML paths - INSERT (ORM/Core) with default behavior returns \"complete\" (backward compatible). - UPDATE and DELETE with return_metadata=True return metadata including rowcount (driver differences tolerated as 0/1 in CI). - Text INSERT duplicate (violates unique) triggers handled IntegrityError \u2192 error dict. - Rows in metadata when statement returns rows: behavior implemented; RETURNING scenarios are backend-dependent (not asserted under SQLite).</p> <p>Error handling - General exception path via mocked session \u2192 error dict.</p> <p>Transaction - Commit occurs for DML; explicit verification is implicit through state checks in subsequent SELECTs.</p>"},{"location":"database/database_queries_tested/#execute_manyqueries-return_resultsfalse","title":"execute_many(queries, return_results=False)","text":"<p>Tested mixed-batch behavior - Batch with multiple statements in order: INSERT, INSERT, SELECT (ORM scalar), text COUNT(), UPDATE, text DELETE. - With return_results=True:     - INSERT entries return metadata dicts with rowcount and (if provided by driver) inserted_primary_key.     - ORM SELECT entries delegate to read_query and return shaped lists (e.g., list of scalars for single-column selects).     - Text SELECT COUNT() returns metadata with rows as a list of scalars (tolerant of 0/1/2 in CI to account for timing/visibility).     - UPDATE and DELETE entries return metadata dicts with rowcount (tolerant 0/1 in CI).</p> <p>Tested SELECT (text) shaping inside execute_many - Text SELECT with multiple columns returns rows as list of dicts in metadata.</p> <p>Row-shaping fallbacks via mocks - keys-present branch:     - mapping with one key \u2192 rows as scalars     - objects with dict \u2192 rows contain objects     - plain objects \u2192 rows contain objects - no-keys branch:     - mapping with one key \u2192 rows as scalars     - mapping with multiple keys \u2192 rows as dicts     - dict and plain object branches</p> <p>Error handling - Bad SQL in batch returns an error dict.</p> <p>Legacy return - When return_results=False (default), returns the string \"complete\" (backward compatible).</p>"},{"location":"database/database_queries_tested/#read_queryquery","title":"read_query(query)","text":"<p>Entity and scalar columns - ORM SELECT(User) \u2192 list of ORM objects. - Scalar column DISTINCT \u2192 list of unique scalars. - Multi-column SELECT DISTINCT (e.g., color, name) \u2192 validated both as tuples and as dicts depending on shape.</p> <p>Mapping and fallback behavior - keys-present, single key \u2192 list of scalars via scalars().all(). - keys-present, multi key \u2192 list of dicts via row._mapping. - no-keys fallback:     - _mapping with one key \u2192 list of scalars     - row.dict present \u2192 returns row objects     - plain row objects \u2192 returns row objects</p> <p>Error handling - SQLAlchemyError and General Exception paths validated via mocks.</p>"},{"location":"database/database_queries_tested/#read_multi_queryname-query","title":"read_multi_query({name: query, ...})","text":"<p>Multiple queries in a dict - Returns a dict mapping each name to its shaped result list.</p> <p>Mapping and fallback behavior - keys-present, single key \u2192 scalars list - keys-present, multi key \u2192 list of dicts - no-keys fallback:     - _mapping with one key \u2192 scalars list     - row.dict \u2192 list of objects     - plain row objects \u2192 list of objects</p> <p>Error handling - SQLAlchemyError and General Exception paths validated via mocks.</p>"},{"location":"database/database_queries_tested/#count_queryselect","title":"count_query(select(...))","text":"<p>Behavior - After seeding many rows, count_query returns the expected count.</p> <p>Error handling - SQLAlchemyError and General Exception paths validated via mocks.</p>"},{"location":"database/database_queries_tested/#read_one_recordselect","title":"read_one_record(select(...))","text":"<p>Behavior - Returns a single ORM object for a matching row; returns None when no match.</p> <p>Error handling - General Exception path validated via mock.</p>"},{"location":"database/database_queries_tested/#get_table_names","title":"get_table_names()","text":"<p>Behavior - Returns a list of table names from Base metadata. Test asserts the list exists (other test modules may add tables to the shared Base).</p> <p>Error handling - Exception path validated via patched Base.metadata.tables.keys.</p>"},{"location":"database/database_queries_tested/#get_columns_detailstable","title":"get_columns_details(Table)","text":"<p>Behavior - Returns a dict of column metadata (type, nullable, primary_key, unique, autoincrement, default) for the provided table.</p> <p>Error handling - Exception path validated via a FakeTable raising on column access.</p>"},{"location":"database/database_queries_tested/#get_primary_keystable","title":"get_primary_keys(Table)","text":"<p>Behavior - Returns a list of primary key column names for the table.</p> <p>Error handling - Exception path validated via a FakeTable raising from primary_key.columns.</p>"},{"location":"database/database_queries_tested/#deprecated-helpers-covered-for-backward-compatibility","title":"Deprecated helpers (covered for backward compatibility)","text":"<p>create_one(record) - Inserts a single record; returns the ORM object. Error paths validated (SQLAlchemyError, IntegrityError, General Exception). Emits DeprecationWarning.</p> <p>create_many(records) - Bulk insert list of records; returns list of ORM objects. Error paths validated. Emits DeprecationWarning.</p> <p>update_one(table, record_id, new_values) - Updates a single record by PK; returns updated ORM object. Error paths validated (not found, IntegrityError, SQLAlchemyError, General Exception). Emits DeprecationWarning.</p> <p>delete_one(table, record_id) - Deletes a single record by PK; success dict on delete; error paths validated (not found, SQLAlchemyError, General Exception). Emits DeprecationWarning.</p> <p>delete_many(table, id_column_name, id_values) - Deletes many by IDs; returns number deleted or error dict when invalid column. Emits DeprecationWarning.</p>"},{"location":"database/database_queries_tested/#backend-notes-and-caveats","title":"Backend notes and caveats","text":"<ul> <li>These tests run under SQLite + aiosqlite. Some behaviors (rowcount, RETURNING) vary by driver/backends (e.g., PostgreSQL). Tests that check rowcount are tolerant (0 or 1) to accommodate these differences in CI.</li> <li>RETURNING is supported by the library APIs (rows appear in metadata), but assertions for RETURNING-specific rows are not enforced under SQLite. You can add PostgreSQL-backed tests to validate this further.</li> </ul>"},{"location":"database/database_queries_tested/#at-a-glance-whats-most-extensive","title":"At-a-glance (what\u2019s most extensive)","text":"<ul> <li>execute_one: SELECT via ORM and text (single/multi column), DML metadata, robust row-shaping with and without keys, error paths.</li> <li>execute_many: Mixed batches with INSERT/SELECT/COUNT/UPDATE/DELETE, text SELECT shaping, extensive row-shaping (keys/no-keys), error paths, legacy and opt-in result modes.</li> </ul> <p>This reflects the test suite as of 2025-08-10.</p>"},{"location":"examples/cal_example/","title":"cal_example Example","text":""},{"location":"examples/cal_example/#overview","title":"Overview","text":"<p>This module demonstrates the usage of the <code>calendar_functions</code> module from the <code>dsg_lib.common_functions</code> package. It provides examples of how to work with months, both by their numeric representation and their names.</p> <p>The module includes two main functions:</p> <ol> <li><code>calendar_check_number</code>:</li> <li>Iterates through a predefined list of month numbers (<code>month_list</code>) and uses the <code>get_month</code> function from <code>calendar_functions</code> to retrieve the corresponding month name.</li> <li>It then prints the result for each number in the list.</li> <li> <p>Example:</p> <ul> <li>Input: <code>1</code></li> <li>Output: <code>\"January\"</code></li> <li>Input: <code>13</code> (invalid)</li> <li>Output: Depends on the implementation of <code>get_month</code> (e.g., <code>\"Invalid Month\"</code>).</li> </ul> </li> <li> <p><code>calendar_check_name</code>:</p> </li> <li>Iterates through a predefined list of month names (<code>month_names</code>) and uses the <code>get_month_number</code> function from <code>calendar_functions</code> to retrieve the corresponding numeric representation of the month.</li> <li>It then prints the result for each name in the list.</li> <li>Example:<ul> <li>Input: <code>\"january\"</code></li> <li>Output: <code>1</code></li> <li>Input: <code>\"bob\"</code> (invalid)</li> <li>Output: Depends on the implementation of <code>get_month_number</code> (e.g., <code>\"Invalid Month Name\"</code>).</li> </ul> </li> </ol>"},{"location":"examples/cal_example/#features","title":"Features","text":"<ul> <li>Validation of Inputs:   The module demonstrates how to handle invalid inputs, such as:</li> <li>Numbers outside the valid range of months (1-12).</li> <li> <p>Invalid month names that do not correspond to any recognized month.</p> </li> <li> <p>Testing and Debugging:   This module can be used to test and validate the robustness of the <code>calendar_functions</code> module by providing a variety of valid and invalid inputs.</p> </li> </ul>"},{"location":"examples/cal_example/#usage","title":"Usage","text":"<ul> <li>Run the script directly to see the output of the two functions.</li> <li>Modify the <code>month_list</code> or <code>month_names</code> variables to test with different inputs.</li> </ul>"},{"location":"examples/cal_example/#dependencies","title":"Dependencies","text":"<ul> <li><code>dsg_lib.common_functions.calendar_functions</code>:</li> <li>This module must be available and contain the following functions:<ol> <li><code>get_month</code>: Accepts a numeric month (e.g., <code>1</code>) and returns the corresponding month name (e.g., <code>\"January\"</code>).</li> <li><code>get_month_number</code>: Accepts a month name (e.g., <code>\"january\"</code>) and returns the corresponding numeric representation (e.g., <code>1</code>).</li> </ol> </li> </ul>"},{"location":"examples/cal_example/#example-output","title":"Example Output","text":""},{"location":"examples/cal_example/#for-calendar_check_number","title":"For <code>calendar_check_number</code>:","text":"<p>If <code>month_list = [0, 1, 2, 3, 13]</code>, the output might be: <pre><code>Invalid Month\nJanuary\nFebruary\nMarch\nInvalid Month\n</code></pre></p>"},{"location":"examples/cal_example/#for-calendar_check_name","title":"For <code>calendar_check_name</code>:","text":"<p>If <code>month_names = [\"january\", \"february\", \"bob\"]</code>, the output might be: <pre><code>1\n2\nInvalid Month Name\n</code></pre></p>"},{"location":"examples/cal_example/#notes","title":"Notes","text":"<ul> <li>Ensure that the <code>calendar_functions</code> module is correctly implemented and imported.</li> <li>The behavior for invalid inputs depends on the implementation of <code>get_month</code> and <code>get_month_number</code>.</li> </ul>"},{"location":"examples/cal_example/#license","title":"License","text":"<p>This module is licensed under the MIT License.</p> <pre><code>from dsg_lib.common_functions import calendar_functions\nfrom typing import List, Any\n\n# List of month numbers to test, including invalid values (0, 13)\nmonth_list: List[int] = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n\n# List of month names to test, including an invalid value (\"bob\")\nmonth_names: List[str] = [\n    \"january\",\n    \"february\",\n    \"march\",\n    \"april\",\n    \"may\",\n    \"june\",\n    \"july\",\n    \"august\",\n    \"september\",\n    \"october\",\n    \"november\",\n    \"december\",\n    \"bob\",\n]\n\ndef calendar_check_number() -&gt; None:\n    \"\"\"\n    Example: Demonstrates converting month numbers to month names.\n    Iterates through `month_list` and prints the result of get_month for each.\n    \"\"\"\n    for i in month_list:\n        month = calendar_functions.get_month(month=i)\n        print(month)\n\ndef calendar_check_name() -&gt; None:\n    \"\"\"\n    Example: Demonstrates converting month names to month numbers.\n    Iterates through `month_names` and prints the result of get_month_number for each.\n    \"\"\"\n    for i in month_names:\n        month = calendar_functions.get_month_number(month_name=i)\n        print(month)\n\ndef calendar_check_float_and_invalid_types() -&gt; None:\n    \"\"\"\n    Example: Tests get_month with float values and various invalid types.\n    Shows how the function handles non-integer and unexpected input types.\n    \"\"\"\n    print(\"\\nTesting get_month with float and invalid types:\")\n    test_values: List[Any] = [1.0, 12.0, 5.5, \"3\", None, [1], {\"month\": 2}]\n    for val in test_values:\n        print(f\"Input: {val!r} -&gt; Output: {calendar_functions.get_month(month=val)}\")\n\ndef calendar_check_name_variants() -&gt; None:\n    \"\"\"\n    Example: Tests get_month_number with name variants and invalid types.\n    Includes extra spaces, different cases, abbreviations, and non-string types.\n    \"\"\"\n    print(\"\\nTesting get_month_number with name variants and invalid types:\")\n    test_names: List[Any] = [\n        \" January \", \"FEBRUARY\", \"mar\", \"Apr\", \"may\", \"JUNE\", \"July\", \"august\", \"Sept\", \"oct\", \"nov\", \"december\",\n        5, None, [\"March\"], {\"month\": \"April\"}\n    ]\n    for name in test_names:\n        print(f\"Input: {name!r} -&gt; Output: {calendar_functions.get_month_number(month_name=name)}\")\n\nif __name__ == \"__main__\":\n    # Run all example checks to demonstrate library usage and edge case handling\n    calendar_check_number()\n    calendar_check_name()\n    calendar_check_float_and_invalid_types()\n    calendar_check_name_variants()\n</code></pre>"},{"location":"examples/csv_example/","title":"csv_example Example","text":""},{"location":"examples/csv_example/#csv-example-module","title":"CSV Example Module","text":"<p>This module provides examples of how to work with CSV files using the <code>dsg_lib</code> library. It includes functions for saving data to a CSV file, opening and reading data from a CSV file, appending data to an existing CSV file, deleting a CSV file, and creating sample files for testing purposes. The module is designed to demonstrate the usage of the <code>file_functions</code> and <code>logging_config</code> utilities provided by <code>dsg_lib</code>.</p>"},{"location":"examples/csv_example/#functions","title":"Functions","text":""},{"location":"examples/csv_example/#save_some_dataexample_list-list","title":"<code>save_some_data(example_list: list)</code>","text":"<p>Saves a list of data to a CSV file. The function uses the <code>save_csv</code> utility from <code>dsg_lib</code> to write the data to a file. The file is saved with a specified delimiter and quote character.</p> <ul> <li>Parameters:</li> <li><code>example_list</code> (list): A list of lists containing the data to be saved.</li> <li>Notes:</li> <li>The file is saved in the <code>/data</code> directory with the name <code>your-file-name.csv</code>.</li> <li>The delimiter used is <code>|</code>, and the quote character is <code>\"</code>.</li> <li>Refer to the <code>save_csv</code> documentation for additional options.</li> </ul>"},{"location":"examples/csv_example/#open_some_datathe_file_name-str-dict","title":"<code>open_some_data(the_file_name: str) -&gt; dict</code>","text":"<p>Opens a CSV file and returns its contents as a dictionary. This function assumes the CSV file has a header row and uses the <code>open_csv</code> utility from <code>dsg_lib</code>.</p> <ul> <li>Parameters:</li> <li><code>the_file_name</code> (str): The name of the CSV file to open.</li> <li>Returns:</li> <li><code>dict</code>: A dictionary representation of the CSV file's contents.</li> <li>Notes:</li> <li>Additional options such as delimiter, quote level, and space handling can be configured.</li> <li>Refer to the Python CSV documentation for more details: Python CSV Documentation.</li> </ul>"},{"location":"examples/csv_example/#append_some_datarows-list","title":"<code>append_some_data(rows: list)</code>","text":"<p>Appends rows to an existing CSV file. The function uses the <code>append_csv</code> utility from <code>dsg_lib</code>.</p> <ul> <li>Parameters:</li> <li><code>rows</code> (list): A list of lists containing the rows to append. The header must match the existing file.</li> </ul>"},{"location":"examples/csv_example/#delete_example_filefile_name-str","title":"<code>delete_example_file(file_name: str)</code>","text":"<p>Deletes a CSV file. The function uses the <code>delete_file</code> utility from <code>dsg_lib</code>.</p> <ul> <li>Parameters:</li> <li><code>file_name</code> (str): The name of the file to delete.</li> </ul>"},{"location":"examples/csv_example/#sample_files","title":"<code>sample_files()</code>","text":"<p>Creates sample files for testing purposes. This function uses the <code>create_sample_files</code> utility from <code>dsg_lib</code>.</p> <ul> <li>Notes:</li> <li>The sample file is named <code>test_sample</code> and contains 1000 rows of data.</li> </ul>"},{"location":"examples/csv_example/#example-usage","title":"Example Usage","text":"<pre><code>if __name__ == \"__main__\":\n    # Save example data to a CSV file\n    save_some_data(example_list)\n\n    # Open and read data from a CSV file\n    opened_file = open_some_data(\"your-file-name.csv\")\n    print(\"Opened CSV data:\", opened_file)\n\n    # Append data to an existing CSV file\n    rows_to_append = [\n        [\"thing_one\", \"thing_two\"],  # header row (must match)\n        [\"i\", \"j\"],\n        [\"k\", \"l\"],\n    ]\n    append_some_data(rows_to_append)\n\n    # Delete the CSV file\n    delete_example_file(\"your-file-name.csv\")\n\n    # Create sample files for testing\n    sample_files()\n</code></pre>"},{"location":"examples/csv_example/#logging","title":"Logging","text":"<p>The module configures logging using the <code>config_log</code> utility from <code>dsg_lib</code>. The logging level is set to <code>DEBUG</code> to provide detailed information during execution.</p>"},{"location":"examples/csv_example/#license","title":"License","text":"<p>This module is licensed under the MIT License.</p> <pre><code>from typing import List, Dict, Any\nfrom dsg_lib.common_functions.file_functions import create_sample_files, open_csv, save_csv\nfrom dsg_lib.common_functions.logging_config import config_log\n\nconfig_log(logging_level=\"DEBUG\")\n\nexample_list = [\n    [\"thing_one\", \"thing_two\"],\n    [\"a\", \"b\"],\n    [\"c\", \"d\"],\n    [\"e\", \"f\"],\n    [\"g\", \"h\"],\n]\n\n\ndef save_some_data(example_list: List[List[str]]) -&gt; None:\n    \"\"\"\n    Save a list of lists to a CSV file using dsg_lib's save_csv.\n\n    Args:\n        example_list (List[List[str]]): Data to save, including header as first row.\n    \"\"\"\n    # Save data to CSV with custom delimiter and quote character\n    save_csv(\n        file_name=\"your-file-name.csv\",\n        data=example_list,\n        root_folder=\"/data\",\n        delimiter=\"|\",\n        quotechar='\"',\n    )\n\n\ndef open_some_data(the_file_name: str) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Open a CSV file and return its contents as a list of dictionaries.\n\n    Args:\n        the_file_name (str): Name of the CSV file to open.\n\n    Returns:\n        List[Dict[str, Any]]: List of rows as dictionaries.\n    \"\"\"\n    result = open_csv(file_name=the_file_name)\n    return result\n\n\ndef append_some_data(rows: List[List[str]]) -&gt; None:\n    \"\"\"\n    Append rows to an existing CSV file.\n\n    Args:\n        rows (List[List[str]]): Rows to append, header must match existing file.\n    \"\"\"\n    from dsg_lib.common_functions.file_functions import append_csv\n    append_csv(\n        file_name=\"your-file-name.csv\",\n        data=rows,\n        root_folder=\"/data\",\n        delimiter=\"|\",\n        quotechar='\"',\n    )\n\n\ndef delete_example_file(file_name: str) -&gt; None:\n    \"\"\"\n    Delete a CSV file using dsg_lib's delete_file.\n\n    Args:\n        file_name (str): Name of the file to delete.\n    \"\"\"\n    from dsg_lib.common_functions.file_functions import delete_file\n    delete_file(file_name)\n\n\ndef sample_files() -&gt; None:\n    \"\"\"\n    Create sample files for testing.\n    \"\"\"\n    filename = \"test_sample\"\n    samplesize = 1000\n    create_sample_files(filename, samplesize)\n\n\nif __name__ == \"__main__\":\n    # Example: Save data to CSV\n    save_some_data(example_list)\n\n    # Example: Open and read data from CSV\n    opened_file = open_some_data(\"your-file-name.csv\")\n    print(\"Opened CSV data:\", opened_file)\n\n    # Example: Append data to CSV (header must match)\n    rows_to_append = [\n        [\"thing_one\", \"thing_two\"],  # header row (must match)\n        [\"i\", \"j\"],\n        [\"k\", \"l\"],\n    ]\n    append_some_data(rows_to_append)\n\n    # Example: Delete the CSV file\n    delete_example_file(\"your-file-name.csv\")\n\n    # Example: Create sample files\n    sample_files()\n</code></pre>"},{"location":"examples/csv_example_with_timer/","title":"csv_example_with_timer Example","text":""},{"location":"examples/csv_example_with_timer/#csv-example-with-timer","title":"CSV Example with Timer","text":"<p>This module demonstrates how to generate and save CSV files at regular intervals using Python. It includes functionality to create sample data, save it to a CSV file, and repeat the process indefinitely with a specified delay.</p>"},{"location":"examples/csv_example_with_timer/#features","title":"Features","text":"<ul> <li> <p>Dynamic Data Generation: The <code>create_sample_list</code> function generates a list of lists   with a customizable number of rows. Each row contains sample data with predefined headers.</p> </li> <li> <p>Automated File Saving: The <code>save_data_with_timer</code> function saves the generated data   to a CSV file every 5 seconds. Each file is uniquely named with a timestamp to avoid   overwriting.</p> </li> <li> <p>Customizable CSV Format: The CSV files are saved with a pipe (<code>|</code>) as the delimiter   and double quotes (<code>\"</code>) as the quote character.</p> </li> <li> <p>Logging Support: The module uses a logging configuration to provide debug-level   logging for better traceability.</p> </li> </ul>"},{"location":"examples/csv_example_with_timer/#use-case","title":"Use Case","text":"<p>This module is ideal for scenarios where continuous data generation and saving are required, such as testing, simulations, or data pipeline prototyping.</p>"},{"location":"examples/csv_example_with_timer/#directory-structure","title":"Directory Structure","text":"<p>The generated CSV files are saved in the following directory: <pre><code>/workspaces/devsetgo_lib/data/move/source\n</code></pre></p>"},{"location":"examples/csv_example_with_timer/#how-to-run","title":"How to Run","text":"<p>To execute the script, simply run it as a standalone program: <pre><code>python csv_example_with_timer.py\n</code></pre></p> <p>The script will continuously generate and save CSV files until manually stopped.</p>"},{"location":"examples/csv_example_with_timer/#dependencies","title":"Dependencies","text":"<ul> <li><code>dsg_lib.common_functions.file_functions.save_csv</code>: A utility function to save data to a CSV file.</li> <li><code>dsg_lib.common_functions.logging_config.config_log</code>: A utility function to configure logging.</li> </ul>"},{"location":"examples/csv_example_with_timer/#license","title":"License","text":"<p>This module is licensed under the MIT License.</p> <pre><code>import random\nimport time\nfrom datetime import datetime\n\nfrom dsg_lib.common_functions.file_functions import save_csv\nfrom dsg_lib.common_functions.logging_config import config_log\n\nconfig_log(logging_level=\"DEBUG\")\n\nexample_list = [\n    [\"thing_one\", \"thing_two\"],\n    [\"a\", \"b\"],\n    [\"c\", \"d\"],\n    [\"e\", \"f\"],\n    [\"g\", \"h\"],\n]\n\n\ndef create_sample_list(qty=10):\n    \"\"\"\n    Create a sample list of lists with specified quantity.\n    \"\"\"\n    headers = [\"thing_one\", \"thing_two\", \"thing_three\", \"thing_four\", \"thing_five\"]\n    sample_list = [headers]\n    for i in range(qty):\n        sample_list.append(\n            [f\"item_{i+1}\", f\"item_{i+2}\", f\"item_{i+3}\", f\"item_{i+4}\", f\"item_{i+5}\"]\n        )\n    return sample_list\n\n\ndef save_data_with_timer():\n    \"\"\"\n    Saves a new CSV file every 5 seconds with a unique timestamped name.\n\n    This function generates a sample list of data with a random number of rows\n    (between 10 and 100,000) using the `create_sample_list` function. It then\n    saves this data to a CSV file in the specified directory. The file name\n    includes a timestamp to ensure uniqueness. The CSV file is saved with a\n    pipe (`|`) as the delimiter and double quotes (`\"`) as the quote character.\n\n    The process repeats indefinitely, with a 5-second delay between each file\n    creation. This function is useful for testing or simulating scenarios where\n    data is continuously generated and saved to disk.\n\n    The saved files are stored in the `/workspaces/devsetgo_lib/data/move/source`\n    directory.\n    \"\"\"\n    while True:\n        example_list = create_sample_list(qty=random.randint(10, 100000))\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        file_name = f\"data_{timestamp}.csv\"\n        save_csv(\n            file_name=file_name,\n            data=example_list,\n            root_folder=\"/workspaces/devsetgo_lib/data/move/source\",\n            delimiter=\"|\",\n            quotechar='\"',\n        )\n        print(f\"Saved file: {file_name}\")\n        time.sleep(5)\n\n\nif __name__ == \"__main__\":\n    save_data_with_timer()\n</code></pre>"},{"location":"examples/fastapi_example/","title":"fastapi_example Example","text":""},{"location":"examples/fastapi_example/#fastapi-example-module","title":"FastAPI Example Module","text":"<p>This module demonstrates the use of FastAPI in conjunction with the DevSetGo Toolkit to create a fully functional API. It includes examples of database operations, user management, and system health endpoints. The module is designed to showcase best practices for building scalable and maintainable FastAPI applications.</p>"},{"location":"examples/fastapi_example/#features","title":"Features","text":"<ul> <li>Database Integration:</li> <li>Uses SQLAlchemy for ORM and database interactions.</li> <li>Supports SQLite (in-memory) for demonstration purposes.</li> <li> <p>Includes models for <code>User</code> and <code>Address</code> tables with relationships.</p> </li> <li> <p>API Endpoints:</p> </li> <li>CRUD operations for <code>User</code> records.</li> <li>Bulk operations for creating and deleting records.</li> <li>System health endpoints for monitoring uptime, heap dumps, and status.</li> <li> <p>Robots.txt endpoint for bot management.</p> </li> <li> <p>Logging:</p> </li> <li>Configured using <code>loguru</code> for structured and detailed logging.</li> <li> <p>Logs API requests, database operations, and system events.</p> </li> <li> <p>Asynchronous Operations:</p> </li> <li>Fully asynchronous database operations using <code>asyncpg</code> and <code>aiosqlite</code>.</li> <li> <p>Asynchronous lifespan management for startup and shutdown events.</p> </li> <li> <p>Configuration:</p> </li> <li>Modular configuration for database, logging, and API behavior.</li> <li>Bot management configuration for controlling access to the API.</li> </ul>"},{"location":"examples/fastapi_example/#usage","title":"Usage","text":"<ol> <li> <p>Run the Application:    Use the following command to start the FastAPI application:    <pre><code>uvicorn fastapi_example:app --host 127.0.0.1 --port 5001\n</code></pre></p> </li> <li> <p>Access the API:</p> </li> <li>OpenAPI Documentation: http://127.0.0.1:5001/docs</li> <li> <p>ReDoc Documentation: http://127.0.0.1:5001/redoc</p> </li> <li> <p>Database Operations:</p> </li> <li>Use the provided endpoints to perform CRUD operations on the <code>User</code> and <code>Address</code> tables.</li> <li> <p>Example endpoints include:</p> <ul> <li><code>/database/create-one-record</code></li> <li><code>/database/get-all</code></li> <li><code>/database/delete-one-record</code></li> </ul> </li> <li> <p>Health Monitoring:</p> </li> <li>Access system health endpoints under <code>/api/health</code>.</li> </ol>"},{"location":"examples/fastapi_example/#dependencies","title":"Dependencies","text":"<ul> <li><code>FastAPI</code>: Web framework for building APIs.</li> <li><code>SQLAlchemy</code>: ORM for database interactions.</li> <li><code>loguru</code>: Logging library for structured logs.</li> <li><code>tqdm</code>: Progress bar for bulk operations.</li> <li><code>pydantic</code>: Data validation and settings management.</li> <li><code>DevSetGo Toolkit</code>: Custom library for database and common utility functions.</li> </ul>"},{"location":"examples/fastapi_example/#license","title":"License","text":"<p>This module is licensed under the MIT License.</p> <pre><code>import datetime\nimport secrets\nimport time\nfrom contextlib import asynccontextmanager\n\nfrom fastapi import Body, FastAPI, Query\nfrom fastapi.responses import RedirectResponse\nfrom loguru import logger\nfrom pydantic import BaseModel, EmailStr\nfrom sqlalchemy import Column, ForeignKey, Select, String, and_, delete, insert, or_, update\nfrom sqlalchemy.orm import relationship\nfrom tqdm import tqdm\n\nfrom dsg_lib.async_database_functions import (\n    async_database,\n    base_schema,\n    database_config,\n    database_operations,\n)\nfrom dsg_lib.common_functions import logging_config\nfrom dsg_lib.fastapi_functions import default_endpoints, system_health_endpoints\n\nconfig = [\n    {\"bot\": \"Bytespider\", \"allow\": False},\n    {\"bot\": \"GPTBot\", \"allow\": False},\n    {\"bot\": \"ClaudeBot\", \"allow\": True},\n    {\"bot\": \"ImagesiftBot\", \"allow\": True},\n    {\"bot\": \"CCBot\", \"allow\": False},\n    {\"bot\": \"ChatGPT-User\", \"allow\": True},\n    {\"bot\": \"omgili\", \"allow\": False},\n    {\"bot\": \"Diffbot\", \"allow\": False},\n    {\"bot\": \"Claude-Web\", \"allow\": True},\n    {\"bot\": \"PerplexityBot\", \"allow\": False},\n]\n\nlogging_config.config_log(\n    logging_level=\"INFO\",\n    log_serializer=False,\n    logging_directory=\"log\",\n    log_name=\"log.log\",\n    intercept_standard_logging=True,\n)\n# Create a DBConfig instance\nconfig = {\n    # \"database_uri\": \"postgresql+asyncpg://postgres:postgres@postgresdb/postgres\",\n    \"database_uri\": \"sqlite+aiosqlite:///:memory:?cache=shared\",\n    \"echo\": False,\n    \"future\": True,\n    # \"pool_pre_ping\": True,\n    # \"pool_size\": 10,\n    # \"max_overflow\": 10,\n    \"pool_recycle\": 3600,\n    # \"pool_timeout\": 30,\n}\n\n# create database configuration\ndb_config = database_config.DBConfig(config)\n# Create an AsyncDatabase instance\nasync_db = async_database.AsyncDatabase(db_config)\n\n# Create a DatabaseOperations instance\ndb_ops = database_operations.DatabaseOperations(async_db)\n\n\nclass User(base_schema.SchemaBaseSQLite, async_db.Base):\n    \"\"\"\n    User table storing user details like first name, last name, and email\n    \"\"\"\n\n    __tablename__ = \"users\"\n    __table_args__ = {\n        \"comment\": \"User table storing user details like first name, last name, and email\"\n    }\n\n    first_name = Column(String(50), unique=False, index=True)  # First name of the user\n    last_name = Column(String(50), unique=False, index=True)  # Last name of the user\n    email = Column(\n        String(200), unique=True, index=True, nullable=True\n    )  # Email of the user, must be unique\n    addresses = relationship(\n        \"Address\", order_by=\"Address.pkid\", back_populates=\"user\"\n    )  # Relationship to the Address class\n\n\nclass Address(base_schema.SchemaBaseSQLite, async_db.Base):\n    \"\"\"\n    Address table storing address details like street, city, and zip code\n    \"\"\"\n\n    __tablename__ = \"addresses\"\n    __table_args__ = {\n        \"comment\": \"Address table storing address details like street, city, and zip code\"\n    }\n\n    street = Column(String(200), unique=False, index=True)  # Street of the address\n    city = Column(String(200), unique=False, index=True)  # City of the address\n    zip = Column(String(50), unique=False, index=True)  # Zip code of the address\n    user_id = Column(\n        String(36), ForeignKey(\"users.pkid\")\n    )  # Foreign key to the User table\n    user = relationship(\n        \"User\", back_populates=\"addresses\"\n    )  # Relationship to the User class\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    logger.info(\"starting up\")\n    # Create the tables in the database\n    await async_db.create_tables()\n\n    create_users = True\n    if create_users:\n        await create_a_bunch_of_users(single_entry=2, many_entries=100)\n    yield\n    logger.info(\"shutting down\")\n    await async_db.disconnect()\n    logger.info(\"database disconnected\")\n    print(\"That's all folks!\")\n\n\n# Create an instance of the FastAPI class\napp = FastAPI(\n    title=\"FastAPI Example\",  # The title of the API\n    description=\"This is an example of a FastAPI application using the DevSetGo Toolkit.\",  # A brief description of the API\n    version=\"0.1.0\",  # The version of the API\n    docs_url=\"/docs\",  # The URL where the API documentation will be served\n    redoc_url=\"/redoc\",  # The URL where the ReDoc documentation will be served\n    openapi_url=\"/openapi.json\",  # The URL where the OpenAPI schema will be served\n    debug=True,  # Enable debug mode\n    middleware=[],  # A list of middleware to include in the application\n    routes=[],  # A list of routes to include in the application\n    lifespan=lifespan,  # this is the replacement for the startup and shutdown events\n)\n\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"\n    Redirect to the OpenAPI documentation.\n\n    Example:\n        GET /\n\n    Returns:\n        Redirects to /docs for interactive API documentation.\n    \"\"\"\n    logger.info(\"Redirecting to OpenAPI docs\")\n    response = RedirectResponse(url=\"/docs\")\n    return response\n\n\n# Example configuration\nconfig = {\n    \"enable_status_endpoint\": True,\n    \"enable_uptime_endpoint\": True,\n    \"enable_heapdump_endpoint\": True,\n    \"enable_robots_endpoint\": True,\n    \"user_agents\": [\n        {\"bot\": \"Bytespider\", \"allow\": False},\n        {\"bot\": \"GPTBot\", \"allow\": False},\n        {\"bot\": \"ClaudeBot\", \"allow\": True},\n        {\"bot\": \"ImagesiftBot\", \"allow\": True},\n        {\"bot\": \"CCBot\", \"allow\": False},\n        {\"bot\": \"ChatGPT-User\", \"allow\": True},\n        {\"bot\": \"omgili\", \"allow\": False},\n        {\"bot\": \"Diffbot\", \"allow\": False},\n        {\"bot\": \"Claude-Web\", \"allow\": True},\n        {\"bot\": \"PerplexityBot\", \"allow\": False},\n        {\"bot\": \"Googlebot\", \"allow\": True},\n        {\"bot\": \"Bingbot\", \"allow\": True},\n        {\"bot\": \"Baiduspider\", \"allow\": False},\n        {\"bot\": \"YandexBot\", \"allow\": False},\n        {\"bot\": \"DuckDuckBot\", \"allow\": True},\n        {\"bot\": \"Sogou\", \"allow\": False},\n        {\"bot\": \"Exabot\", \"allow\": False},\n        {\"bot\": \"facebot\", \"allow\": False},\n        {\"bot\": \"ia_archiver\", \"allow\": False},\n    ],\n}\n\n# Create and include the health router if enabled\nif (\n    config[\"enable_status_endpoint\"]\n    or config[\"enable_uptime_endpoint\"]\n    or config[\"enable_heapdump_endpoint\"]\n):\n    health_router = system_health_endpoints.create_health_router(config)\n    app.include_router(health_router, prefix=\"/api/health\", tags=[\"system-health\"])\n\n# Create and include the default router if enabled\nif config[\"enable_robots_endpoint\"]:\n    default_router = default_endpoints.create_default_router(config[\"user_agents\"])\n    app.include_router(default_router, prefix=\"\", tags=[\"default\"])\n\n\nasync def create_a_bunch_of_users(single_entry=0, many_entries=0):\n    logger.info(f\"single_entry: {single_entry}\")\n    await async_db.create_tables()\n    # Create a list to hold the user data\n\n    # Create a loop to generate user data\n\n    for _ in tqdm(range(single_entry), desc=\"executing one\"):\n        value = secrets.token_hex(16)\n        user = User(\n            first_name=f\"First{value}\",\n            last_name=f\"Last{value}\",\n            email=f\"user{value}@example.com\",\n        )\n        logger.info(f\"created_users: {user}\")\n        await db_ops.create_one(user)\n\n    users = []\n    # Create a loop to generate user data\n    for i in tqdm(range(many_entries), desc=\"executing many\"):\n        value_one = secrets.token_hex(4)\n        value_two = secrets.token_hex(8)\n        user = User(\n            first_name=f\"First{value_one}{i}{value_two}\",\n            last_name=f\"Last{value_one}{i}{value_two}\",\n            email=f\"user{value_one}{i}{value_two}@example.com\",\n        )\n        logger.info(f\"created_users: {user.first_name}\")\n        users.append(user)\n\n    # Use db_ops to add the users to the database\n    await db_ops.create_many(users)\n\n\n@app.get(\"/database/get-primary-key\", tags=[\"Database Examples\"])\nasync def table_primary_key():\n    \"\"\"\n    Get the primary key(s) of the User table.\n\n    Example:\n        GET /database/get-primary-key\n\n    Returns:\n        The primary key column(s) for the User table.\n    \"\"\"\n    logger.info(\"Getting primary key of User table\")\n    pk = await db_ops.get_primary_keys(User)\n    logger.info(f\"Primary key of User table: {pk}\")\n    return {\"pk\": pk}\n\n\n@app.get(\"/database/get-column-details\", tags=[\"Database Examples\"])\nasync def table_column_details():\n    \"\"\"\n    Get details about all columns in the User table.\n\n    Example:\n        GET /database/get-column-details\n\n    Returns:\n        Metadata for each column in the User table.\n    \"\"\"\n    logger.info(\"Getting column details of User table\")\n    columns = await db_ops.get_columns_details(User)\n    logger.info(f\"Column details of User table: {columns}\")\n    return {\"columns\": columns}\n\n\n@app.get(\"/database/get-tables\", tags=[\"Database Examples\"])\nasync def table_table_details():\n    \"\"\"\n    List all table names in the database.\n\n    Example:\n        GET /database/get-tables\n\n    Returns:\n        A list of all table names.\n    \"\"\"\n    logger.info(\"Getting table names\")\n    tables = await db_ops.get_table_names()\n    logger.info(f\"Table names: {tables}\")\n    return {\"table_names\": tables}\n\n\n@app.get(\"/database/get-count\", tags=[\"Database Examples\"])\nasync def get_count():\n    \"\"\"\n    Get the total number of User records.\n\n    Example:\n        GET /database/get-count\n\n    Returns:\n        The count of User records.\n    \"\"\"\n    logger.info(\"Getting count of users\")\n    count = await db_ops.count_query(Select(User))\n    logger.info(f\"Count of users: {count}\")\n    return {\"count\": count}\n\n\n@app.get(\"/database/get-all\", tags=[\"Database Examples\"])\nasync def get_all(offset: int = 0, limit: int = Query(100, le=100000, ge=1)):\n    \"\"\"\n    Retrieve all User records with pagination.\n\n    Example:\n        GET /database/get-all?offset=0&amp;limit=10\n\n    Returns:\n        A list of User records.\n    \"\"\"\n    logger.info(f\"Getting all users with offset {offset} and limit {limit}\")\n    records = await db_ops.read_query(Select(User).offset(offset).limit(limit))\n    logger.info(f\"Retrieved {len(records)} users\")\n    return {\"records\": records}\n\n\n@app.get(\"/database/get-one-record\", tags=[\"Database Examples\"])\nasync def read_one_record(record_id: str):\n    \"\"\"\n    Retrieve a single User record by primary key.\n\n    Example:\n        GET /database/get-one-record?record_id=some-uuid\n\n    Returns:\n        The User record with the given primary key.\n    \"\"\"\n    logger.info(f\"Reading one record with id {record_id}\")\n    record = await db_ops.read_one_record(Select(User).where(User.pkid == record_id))\n    logger.info(f\"Record with id {record_id}: {record}\")\n    return record\n\n\nclass UserBase(BaseModel):\n    first_name: str\n    last_name: str\n    email: EmailStr\n\n\nclass UserCreate(UserBase):\n    pass\n\n\n@app.post(\"/database/create-one-record\", status_code=201, tags=[\"Database Examples\"])\nasync def create_one_record(new_user: UserCreate):\n    \"\"\"\n    Create a new User record.\n\n    Example:\n        POST /database/create-one-record\n        {\n            \"first_name\": \"Alice\",\n            \"last_name\": \"Smith\",\n            \"email\": \"alice@example.com\"\n        }\n\n    Returns:\n        The created User record.\n    \"\"\"\n    logger.info(f\"Creating one record: {new_user}\")\n    user = User(**new_user.dict())\n    record = await db_ops.create_one(user)\n    logger.info(f\"Created record: {record}\")\n    return record\n\n\n@app.post(\"/database/create-many-records\", status_code=201, tags=[\"Database Examples\"])\nasync def create_many_records(number_of_users: int = Query(100, le=1000, ge=1)):\n    \"\"\"\n    Create multiple User records in bulk.\n\n    Example:\n        POST /database/create-many-records?number_of_users=10\n\n    Returns:\n        The number of users created and the process time.\n    \"\"\"\n    logger.info(f\"Creating {number_of_users} records\")\n    t0 = time.time()\n    users = []\n    # Create a loop to generate user data\n    for i in tqdm(range(number_of_users), desc=\"executing many\"):\n        value_one = secrets.token_hex(4)\n        value_two = secrets.token_hex(8)\n        user = User(\n            first_name=f\"First{value_one}{i}{value_two}\",\n            last_name=f\"Last{value_one}{i}{value_two}\",\n            email=f\"user{value_one}{i}{value_two}@example.com\",\n        )\n        logger.info(f\"Created user: {user.first_name}\")\n        users.append(user)\n\n    # Use db_ops to add the users to the database\n    await db_ops.create_many(users)\n    t1 = time.time()\n    process_time = format(t1 - t0, \".4f\")\n    logger.info(f\"Created {number_of_users} records in {process_time} seconds\")\n    return {\"number_of_users\": number_of_users, \"process_time\": process_time}\n\n\n@app.put(\"/database/update-one-record\", status_code=200, tags=[\"Database Examples\"])\nasync def update_one_record(\n    id: str = Body(\n        ...,\n        description=\"UUID to update\",\n        examples=[\"6087cce8-0bdd-48c2-ba96-7d557dae843e\"],\n    ),\n    first_name: str = Body(..., examples=[\"Agent\"]),\n    last_name: str = Body(..., examples=[\"Smith\"]),\n    email: str = Body(..., examples=[\"jim@something.com\"]),\n):\n    \"\"\"\n    Update a User record by primary key.\n\n    Example:\n        PUT /database/update-one-record\n        {\n            \"id\": \"some-uuid\",\n            \"first_name\": \"Agent\",\n            \"last_name\": \"Smith\",\n            \"email\": \"jim@something.com\"\n        }\n\n    Returns:\n        The updated User record.\n    \"\"\"\n    logger.info(f\"Updating one record with id {id}\")\n    # adding date_updated to new_values as it is not supported in sqlite \\\n    # and other database may not either.\n    new_values = {\n        \"first_name\": first_name,\n        \"last_name\": last_name,\n        \"email\": email,\n        \"date_updated\": datetime.datetime.now(datetime.timezone.utc),\n    }\n    record = await db_ops.update_one(table=User, record_id=id, new_values=new_values)\n    logger.info(f\"Updated record with id {id}\")\n    return record\n\n\n@app.delete(\"/database/delete-one-record\", status_code=200, tags=[\"Database Examples\"])\nasync def delete_one_record(record_id: str = Body(...)):\n    \"\"\"\n    Delete a User record by primary key.\n\n    Example:\n        DELETE /database/delete-one-record\n        {\n            \"record_id\": \"some-uuid\"\n        }\n\n    Returns:\n        Success message or error.\n    \"\"\"\n    logger.info(f\"Deleting one record with id {record_id}\")\n    record = await db_ops.delete_one(table=User, record_id=record_id)\n    logger.info(f\"Deleted record with id {record_id}\")\n    return record\n\n\n@app.delete(\n    \"/database/delete-many-records-aka-this-is-a-bad-idea\",\n    status_code=201,\n    tags=[\"Database Examples\"],\n)\nasync def delete_many_records(\n    id_values: list = Body(...), id_column_name: str = \"pkid\"\n):\n    \"\"\"\n    Delete multiple User records by a list of primary keys.\n\n    Example:\n        DELETE /database/delete-many-records-aka-this-is-a-bad-idea\n        {\n            \"id_values\": [\"uuid1\", \"uuid2\", \"uuid3\"]\n        }\n\n    Returns:\n        The number of records deleted.\n    \"\"\"\n    logger.info(f\"Deleting many records with ids {id_values}\")\n    record = await db_ops.delete_many(\n        table=User, id_column_name=\"pkid\", id_values=id_values\n    )\n    logger.info(f\"Deleted records with ids {id_values}\")\n    return record\n\n\n@app.get(\n    \"/database/get-list-of-records-to-paste-into-delete-many-records\",\n    tags=[\"Database Examples\"],\n)\nasync def read_list_of_records(\n    offset: int = Query(0, le=1000, ge=0), limit: int = Query(100, le=10000, ge=1)\n):\n    \"\"\"\n    Get a list of User primary keys for use in bulk delete.\n\n    Example:\n        GET /database/get-list-of-records-to-paste-into-delete-many-records?offset=0&amp;limit=10\n\n    Returns:\n        A list of User primary keys.\n    \"\"\"\n    logger.info(f\"Reading list of records with offset {offset} and limit {limit}\")\n    records = await db_ops.read_query(Select(User), offset=offset, limit=limit)\n    records_list = []\n    for record in records:\n        records_list.append(record.pkid)\n    logger.info(f\"Read list of records: {records_list}\")\n    return records_list\n\n\n@app.get(\"/database/get-list-of-distinct-records\", tags=[\"Database Examples\"])\nasync def read_list_of_distinct_records():\n    \"\"\"\n    Insert many similar User records and return distinct last names.\n\n    Example:\n        GET /database/get-list-of-distinct-records\n\n    Returns:\n        A list of distinct last names.\n    \"\"\"\n    # create many similar records to test distinct\n    queries = []\n    for i in tqdm(range(100), desc=\"executing many fake users\"):\n        value = f\"Agent {i}\"\n        queries.append(\n            (\n                insert(User),\n                {\n                    \"first_name\": value,\n                    \"last_name\": \"Smith\",\n                    \"email\": f\"{value.lower()}@abc.com\",\n                },\n            )\n        )\n\n    results = await db_ops.execute_many(queries)\n    print(results)\n\n    distinct_last_name_query = Select(User.last_name).distinct()\n    logger.info(f\"Executing query: {distinct_last_name_query}\")\n    records = await db_ops.read_query(query=distinct_last_name_query)\n\n    logger.info(f\"Read list of distinct records: {records}\")\n    return records\n\n\n@app.post(\"/database/execute-one\", tags=[\"Database Examples\"])\nasync def execute_query(query: str = Body(...)):\n    \"\"\"\n    Example of running a single SQL query (insert) using execute_one.\n\n    Example:\n        POST /database/execute-one\n        {\n            \"query\": \"insert example (not used, see code)\"\n        }\n\n    Returns:\n        The inserted User record(s) with first_name \"John\".\n    \"\"\"\n    # add a user with execute_one\n    logger.info(f\"Executing query: {query}\")\n\n    query = insert(User).values(first_name=\"John\", last_name=\"Doe\", email=\"x@abc.com\")\n    result = await db_ops.execute_one(query)\n    logger.info(f\"Executed query: {result}\")\n    query_return = await db_ops.read_query(\n        Select(User).where(User.first_name == \"John\")\n    )\n    return query_return\n\n\n@app.post(\"/database/execute-many\", tags=[\"Database Examples\"])\nasync def execute_many(query: str = Body(...)):\n    \"\"\"\n    Example of running multiple SQL queries (bulk insert) using execute_many.\n\n    Example:\n        POST /database/execute-many\n        {\n            \"query\": \"bulk insert example (not used, see code)\"\n        }\n\n    Returns:\n        All User records after bulk insert.\n    \"\"\"\n    # multiple users with execute_many\n    logger.info(f\"Executing query: {query}\")\n    queries = []\n\n    for i in range(10):\n        query = insert(User).values(\n            first_name=f\"User{i}\", last_name=\"Doe\", email=\"x@abc.com\"\n        )\n        queries.append(query)\n\n    results = await db_ops.execute_many(queries)\n    logger.info(f\"Executed query: {results}\")\n    query_return = await db_ops.read_query(Select(User))\n    return query_return\n\n\n@app.get(\"/database/get-distinct-emails\", tags=[\"Database Examples\"])\nasync def get_distinct_emails():\n    \"\"\"\n    Get a list of distinct emails from the User table.\n\n    Example:\n        GET /database/get-distinct-emails\n\n    Returns:\n        A list of unique email addresses.\n    \"\"\"\n    from sqlalchemy import select\n\n    query = select(User.email).distinct()\n    logger.info(\"Getting distinct emails\")\n    records = await db_ops.read_query(query)\n    return {\"distinct_emails\": records}\n\n\n@app.get(\"/database/get-users-by-email\", tags=[\"Database Examples\"])\nasync def get_users_by_email(email: str):\n    \"\"\"\n    Get User records by email address.\n\n    Example:\n        GET /database/get-users-by-email?email=alice@example.com\n\n    Returns:\n        A list of User records matching the email.\n    \"\"\"\n    query = Select(User).where(User.email == email)\n    logger.info(f\"Getting users with email: {email}\")\n    records = await db_ops.read_query(query)\n    return {\"users\": records}\n\n\n@app.get(\"/database/get-users-by-name\", tags=[\"Database Examples\"])\nasync def get_users_by_name(first_name: str = \"\", last_name: str = \"\"):\n    \"\"\"\n    Get User records by first and/or last name.\n\n    Example:\n        GET /database/get-users-by-name?first_name=Alice&amp;last_name=Smith\n\n    Returns:\n        A list of User records matching the name.\n    \"\"\"\n    filters = []\n    if first_name:\n        filters.append(User.first_name == first_name)\n    if last_name:\n        filters.append(User.last_name == last_name)\n    query = Select(User).where(and_(*filters)) if filters else Select(User)\n    logger.info(f\"Getting users by name: {first_name} {last_name}\")\n    records = await db_ops.read_query(query)\n    return {\"users\": records}\n\n\n@app.get(\"/database/get-users-or\", tags=[\"Database Examples\"])\nasync def get_users_or(first_name: str = \"\", last_name: str = \"\"):\n    \"\"\"\n    Get User records where first name OR last name matches.\n\n    Example:\n        GET /database/get-users-or?first_name=Alice\n\n    Returns:\n        A list of User records matching either name.\n    \"\"\"\n    filters = []\n    if first_name:\n        filters.append(User.first_name == first_name)\n    if last_name:\n        filters.append(User.last_name == last_name)\n    query = Select(User).where(or_(*filters)) if filters else Select(User)\n    logger.info(f\"Getting users by OR: {first_name} {last_name}\")\n    records = await db_ops.read_query(query)\n    return {\"users\": records}\n\n\n@app.get(\"/database/get-multi-query\", tags=[\"Database Examples\"])\nasync def get_multi_query():\n    \"\"\"\n    Run multiple queries at once and return results as a dictionary.\n\n    Example:\n        GET /database/get-multi-query\n\n    Returns:\n        A dictionary with results for each query.\n    \"\"\"\n    queries = {\n        \"all_users\": Select(User),\n        \"distinct_emails\": Select(User.email).distinct(),\n        \"first_10\": Select(User).limit(10),\n    }\n    logger.info(\"Running multi-query example\")\n    results = await db_ops.read_multi_query(queries)\n    return results\n\n\n@app.put(\"/database/update-email\", tags=[\"Database Examples\"])\nasync def update_email(record_id: str = Body(...), new_email: str = Body(...)):\n    \"\"\"\n    Update a User's email address by primary key.\n\n    Example:\n        PUT /database/update-email\n        {\n            \"record_id\": \"some-uuid\",\n            \"new_email\": \"new@email.com\"\n        }\n\n    Returns:\n        Result of the update operation.\n    \"\"\"\n    query = update(User).where(User.pkid == record_id).values(email=new_email)\n    logger.info(f\"Updating email for user {record_id} to {new_email}\")\n    result = await db_ops.execute_one(query)\n    return {\"result\": result}\n\n\n@app.delete(\"/database/delete-by-email\", tags=[\"Database Examples\"])\nasync def delete_by_email(email: str = Body(...)):\n    \"\"\"\n    Delete User records by email address.\n\n    Example:\n        DELETE /database/delete-by-email\n        {\n            \"email\": \"alice@example.com\"\n        }\n\n    Returns:\n        Result of the delete operation.\n    \"\"\"\n    query = delete(User).where(User.email == email)\n    logger.info(f\"Deleting users with email {email}\")\n    result = await db_ops.execute_one(query)\n    return {\"result\": result}\n\n\n@app.post(\"/database/insert-bulk\", tags=[\"Database Examples\"])\nasync def insert_bulk(count: int = Body(5)):\n    \"\"\"\n    Bulk insert User records using execute_many.\n\n    Example:\n        POST /database/insert-bulk\n        {\n            \"count\": 10\n        }\n\n    Returns:\n        Result of the bulk insert operation.\n    \"\"\"\n    queries = []\n    for i in range(count):\n        value = secrets.token_hex(4)\n        q = (\n            insert(User),\n            {\n                \"first_name\": f\"Bulk{value}{i}\",\n                \"last_name\": f\"User{value}{i}\",\n                \"email\": f\"bulk{value}{i}@example.com\",\n            },\n        )\n        queries.append(q)\n    logger.info(f\"Bulk inserting {count} users\")\n    result = await db_ops.execute_many(queries)\n    return {\"result\": result}\n\n\n@app.get(\"/database/error-example\", tags=[\"Database Examples\"])\nasync def error_example():\n    \"\"\"\n    Trigger an error to demonstrate error handling.\n\n    Example:\n        GET /database/error-example\n\n    Returns:\n        Error details from a failed query.\n    \"\"\"\n    # Try to select from a non-existent table\n    from sqlalchemy import text\n\n    query = text(\"SELECT * FROM non_existent_table\")\n    logger.info(\"Triggering error example\")\n    result = await db_ops.read_query(query)\n    return {\"result\": result}\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    uvicorn.run(app, host=\"127.0.0.1\", port=5001)\n</code></pre>"},{"location":"examples/file_monitor/","title":"file_monitor Example","text":""},{"location":"examples/file_monitor/#file-monitor-example","title":"File Monitor Example","text":"<p>This module demonstrates the usage of the <code>process_files_flow</code> function from the <code>dsg_lib.common_functions.file_mover</code> library. It monitors a source directory for files matching a specific pattern, processes them, and moves them to a destination directory, optionally compressing the files during the process.</p>"},{"location":"examples/file_monitor/#features","title":"Features","text":"<ul> <li>Directory Monitoring: Watches a source directory for files matching a specified pattern (e.g., <code>*.csv</code>).</li> <li>File Processing Flow: Utilizes the <code>process_files_flow</code> function to handle file movement and optional compression.</li> <li>Sample File Creation: Periodically generates sample files in the source directory for testing purposes.</li> <li>Asynchronous Execution: Leverages Python's <code>asyncio</code> for concurrent tasks, such as file creation and processing.</li> </ul>"},{"location":"examples/file_monitor/#configuration","title":"Configuration","text":"<p>The following constants can be configured to customize the behavior of the script:</p> <ul> <li><code>SOURCE_DIRECTORY</code>: Path to the directory where files are monitored.</li> <li><code>TEMPORARY_DIRECTORY</code>: Path to a temporary directory used during file processing.</li> <li><code>DESTINATION_DIRECTORY</code>: Path to the directory where processed files are moved.</li> <li><code>FILE_PATTERN</code>: File pattern to monitor (e.g., <code>*.csv</code>).</li> <li><code>COMPRESS_FILES</code>: Boolean flag to enable or disable file compression during processing.</li> <li><code>CLEAR_SOURCE</code>: Boolean flag to clear the source directory before starting.</li> </ul>"},{"location":"examples/file_monitor/#usage","title":"Usage","text":"<ol> <li>Ensure the required directories exist. The script will create them if they do not.</li> <li>Run the script to start monitoring the source directory and processing files.</li> <li>The script will also create sample files in the source directory every 10 seconds for demonstration purposes.</li> </ol>"},{"location":"examples/file_monitor/#example","title":"Example","text":"<p>To run the script:</p> <pre><code>python file_monitor.py\n</code></pre> <p>Press <code>Ctrl+C</code> to stop the script.</p>"},{"location":"examples/file_monitor/#dependencies","title":"Dependencies","text":"<ul> <li><code>os</code> and <code>pathlib</code>: For file and directory operations.</li> <li><code>asyncio</code>: For asynchronous task management.</li> <li><code>loguru</code>: For logging.</li> <li><code>dsg_lib.common_functions.file_mover</code>: For the file processing flow.</li> </ul>"},{"location":"examples/file_monitor/#notes","title":"Notes","text":"<ul> <li>The script is designed for demonstration purposes and may require adjustments for production use.</li> <li>Ensure the <code>dsg_lib</code> library is installed and accessible in your environment.</li> </ul>"},{"location":"examples/file_monitor/#error-handling","title":"Error Handling","text":"<ul> <li>The script gracefully handles <code>KeyboardInterrupt</code> to stop execution.</li> <li>The file creation task is canceled when the main function completes.</li> </ul>"},{"location":"examples/file_monitor/#license","title":"License","text":"<p>This module is licensed under the MIT License.</p> <pre><code>import asyncio\nimport os\nfrom pathlib import Path\n\nfrom loguru import logger\n\nfrom dsg_lib.common_functions.file_mover import process_files_flow\n\n# Define source, temporary, and destination directories\nSOURCE_DIRECTORY: str = \"/workspaces/devsetgo_lib/data/move/source/csv\"\nTEMPORARY_DIRECTORY: str = \"/workspaces/devsetgo_lib/data/move/temp\"\nDESTINATION_DIRECTORY: str = \"/workspaces/devsetgo_lib/data/move/destination\"\nFILE_PATTERN: str = \"*.csv\"  # File pattern to monitor (e.g., '*.txt')\nCOMPRESS_FILES: bool = True  # Set to True to compress files before moving\nCLEAR_SOURCE: bool = True  # Set to True to clear the source directory before starting\n\n# Ensure directories exist\nos.makedirs(SOURCE_DIRECTORY, exist_ok=True)\nos.makedirs(TEMPORARY_DIRECTORY, exist_ok=True)\nos.makedirs(DESTINATION_DIRECTORY, exist_ok=True)\n\n\nasync def create_sample_files() -&gt; None:\n    \"\"\"\n    Periodically create sample files in the source directory for demonstration purposes.\n\n    This coroutine creates a new sample file every 10 seconds in the source directory.\n    \"\"\"\n    while True:\n        # Count existing files to generate a unique file name\n        file_count: int = len(list(Path(SOURCE_DIRECTORY).glob('*')))\n        file_name: str = f\"sample_{file_count + 1}.txt\"\n        file_path: Path = Path(SOURCE_DIRECTORY) / file_name\n        file_path.write_text(\"This is a sample file for testing the file mover.\")\n        logger.info(f\"Created sample file: {file_path}\")\n        await asyncio.sleep(10)  # Create a new file every 10 seconds\n\n\nasync def main() -&gt; None:\n    \"\"\"\n    Main function to demonstrate the file mover library.\n\n    Starts the sample file creation task and runs the file processing flow in a separate thread.\n    Cancels the file creation task when processing is complete.\n    \"\"\"\n    # Start the sample file creation task\n    file_creator_task: asyncio.Task = asyncio.create_task(create_sample_files())\n\n    # Run the file processing flow in a separate thread (to avoid blocking the event loop)\n    loop: asyncio.AbstractEventLoop = asyncio.get_event_loop()\n    await loop.run_in_executor(\n        None,\n        process_files_flow,\n        SOURCE_DIRECTORY,\n        TEMPORARY_DIRECTORY,\n        DESTINATION_DIRECTORY,\n        FILE_PATTERN,\n        COMPRESS_FILES,\n        CLEAR_SOURCE,  # Pass the clear_source flag\n    )\n\n    # Cancel the file creator task when done\n    file_creator_task.cancel()\n    try:\n        await file_creator_task\n    except asyncio.CancelledError:\n        logger.info(\"File creation task cancelled.\")\n\n\nif __name__ == \"__main__\":\n    try:\n        asyncio.run(main())\n    except KeyboardInterrupt:\n        logger.info(\"File monitor example stopped.\")\n</code></pre>"},{"location":"examples/json_example/","title":"json_example Example","text":""},{"location":"examples/json_example/#json-example-module","title":"JSON Example Module","text":"<p>This module demonstrates how to use the <code>open_json</code> and <code>save_json</code> functions from the <code>dsg_lib.common_functions.file_functions</code> package. It provides an example JSON structure and functions to save and load JSON data to and from a file.</p>"},{"location":"examples/json_example/#features","title":"Features","text":"<ul> <li>Example JSON Data: Contains a dictionary with information about historical figures and their contributions.</li> <li>Save JSON Data: Demonstrates saving JSON data to a file using the <code>save_json</code> function.</li> <li>Open JSON Data: Demonstrates loading JSON data from a file using the <code>open_json</code> function.</li> </ul>"},{"location":"examples/json_example/#example-json-structure","title":"Example JSON Structure","text":"<p>The <code>example_json</code> dictionary includes: - A list of <code>super_cool_people</code> with details such as:   - <code>name</code>: The name of the person.   - <code>famous_for</code>: A brief description of their contributions.   - <code>birth_date</code>: Their date of birth.   - <code>death_date</code>: Their date of death. - A <code>sources</code> field indicating the source of the information.</p>"},{"location":"examples/json_example/#functions","title":"Functions","text":""},{"location":"examples/json_example/#save_some_dataexample_json-dictstr-any","title":"<code>save_some_data(example_json: Dict[str, Any])</code>","text":"<p>Saves the provided JSON data to a file named <code>your-file-name.json</code>.</p>"},{"location":"examples/json_example/#open_some_datathe_file_name-str-dictstr-any","title":"<code>open_some_data(the_file_name: str) -&gt; Dict[str, Any]</code>","text":"<p>Loads JSON data from the specified file and returns it as a dictionary.</p>"},{"location":"examples/json_example/#save_list_jsondata-list-file_name-str","title":"<code>save_list_json(data: list, file_name: str)</code>","text":"<p>Saves a list of dictionaries as JSON to the specified file.</p>"},{"location":"examples/json_example/#open_list_jsonfile_name-str-list","title":"<code>open_list_json(file_name: str) -&gt; list</code>","text":"<p>Loads a list of dictionaries from the specified JSON file.</p>"},{"location":"examples/json_example/#try_open_nonexistent_jsonfile_name-str","title":"<code>try_open_nonexistent_json(file_name: str)</code>","text":"<p>Attempts to open a non-existent JSON file and handles the error.</p>"},{"location":"examples/json_example/#usage","title":"Usage","text":"<p>Run the module directly to: 1. Save the <code>example_json</code> data to a file. 2. Load the data back from the file. 3. Save and load a list of dictionaries. 4. Attempt to open a non-existent file.</p>"},{"location":"examples/json_example/#notes","title":"Notes","text":"<ul> <li>Ensure the <code>dsg_lib</code> package is installed and accessible in your environment.</li> <li>Replace <code>\"your-file-name.json\"</code> with the desired file name when using the functions in a real-world scenario.</li> </ul>"},{"location":"examples/json_example/#example-execution","title":"Example Execution","text":"<pre><code>python json_example.py\n</code></pre>"},{"location":"examples/json_example/#license","title":"License","text":"<p>This module is licensed under the MIT License.</p> <pre><code>from typing import Any, Dict\n\nfrom dsg_lib.common_functions.file_functions import open_json, save_json\n\nexample_json: Dict[str, Any] = {\n    \"super_cool_people\": [\n        {\n            \"name\": \"Blaise Pascal\",\n            \"famous_for\": \"Blaise Pascal was a French mathematician, physicist, inventor, writer and Catholic theologian. He was a child prodigy who was educated by his father, a tax collector in Rouen. Pascal's earliest work was in the natural and applied sciences where he made important contributions to the study of fluids, and clarified the concepts of pressure and vacuum by generalising the work of Evangelista Torricelli. Pascal also wrote in defence of the scientific method.\",  # noqa: E501\n            \"birth_date\": \"Jun 19, 1623\",\n            \"death_date\": \"Aug 19, 1662\",\n        },\n        {\n            \"name\": \"Galileo Galilei\",\n            \"famous_for\": 'Galileo di Vincenzo Bonaulti de Galilei was an Italian astronomer, physicist and engineer, sometimes described as a polymath, from Pisa. Galileo has been called the \"father of observational astronomy\", the \"father of modern physics\", the \"father of the scientific method\", and the \"father of modern science\".',  # noqa: E501\n            \"birth_date\": \"Feb 15, 1564\",\n            \"death_date\": \"Jan 08, 1642\",\n        },\n        {\n            \"name\": \"Michelangelo di Lodovico Buonarroti Simoni\",\n            \"famous_for\": \"Michelangelo di Lodovico Buonarroti Simoni , known best as simply Michelangelo, was an Italian sculptor, painter, architect and poet of the High Renaissance born in the Republic of Florence, who exerted an unparalleled influence on the development of Western art.\",  # noqa: E501\n            \"birth_date\": \"Mar 06, 1475\",\n            \"death_date\": \"Feb 18, 1564\",\n        },\n    ],\n    \"sources\": \"wikipedia via Google search.\",\n}\n\ndef save_some_data(example_json: Dict[str, Any]) -&gt; None:\n    \"\"\"\n    Save the provided JSON data to a file named 'your-file-name.json'.\n\n    Args:\n        example_json (Dict[str, Any]): The JSON data to save.\n    \"\"\"\n    save_json(file_name=\"your-file-name.json\", data=example_json)\n\ndef open_some_data(the_file_name: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load JSON data from the specified file.\n\n    Args:\n        the_file_name (str): The name of the JSON file to open.\n\n    Returns:\n        Dict[str, Any]: The loaded JSON data.\n    \"\"\"\n    result: Dict[str, Any] = open_json(file_name=the_file_name)\n    return result\n\n# --- Additional Examples ---\n\nsimple_list_json: list = [\n    {\"id\": 1, \"value\": \"foo\"},\n    {\"id\": 2, \"value\": \"bar\"},\n]\n\ndef save_list_json(data: list, file_name: str) -&gt; None:\n    \"\"\"\n    Save a list of dictionaries as JSON.\n\n    Args:\n        data (list): The list of dictionaries to save.\n        file_name (str): The file name to save to.\n    \"\"\"\n    save_json(file_name=file_name, data=data)\n\ndef open_list_json(file_name: str) -&gt; list:\n    \"\"\"\n    Load a list of dictionaries from a JSON file.\n\n    Args:\n        file_name (str): The file name to load from.\n\n    Returns:\n        list: The loaded list of dictionaries.\n    \"\"\"\n    return open_json(file_name=file_name)\n\ndef try_open_nonexistent_json(file_name: str) -&gt; None:\n    \"\"\"\n    Attempt to open a non-existent JSON file and handle the error.\n\n    Args:\n        file_name (str): The file name to attempt to open.\n    \"\"\"\n    try:\n        open_json(file_name=file_name)\n    except FileNotFoundError as e:\n        print(f\"Handled error: {e}\")\n\nif __name__ == \"__main__\":\n    # Example 1: Save and load a complex dictionary\n    print(\"Saving and loading example_json...\")\n    save_some_data(example_json)\n    opened_file: Dict[str, Any] = open_some_data(\"your-file-name.json\")\n    print(\"Loaded example_json:\", opened_file)\n\n    # Example 2: Save and load a list of dictionaries\n    print(\"\\nSaving and loading a list of dictionaries...\")\n    save_list_json(simple_list_json, \"list-example.json\")\n    loaded_list = open_list_json(\"list-example.json\")\n    print(\"Loaded list-example.json:\", loaded_list)\n\n    # Example 3: Attempt to open a non-existent file\n    print(\"\\nAttempting to open a non-existent file...\")\n    try_open_nonexistent_json(\"does_not_exist.json\")\n</code></pre>"},{"location":"examples/log_example/","title":"log_example Example","text":""},{"location":"examples/log_example/#log-example-module","title":"Log Example Module","text":"<p>This module demonstrates advanced logging configurations and usage in Python. It integrates both the <code>logging</code> module and <code>loguru</code> for robust logging capabilities. The module also showcases multi-threading and multi-processing for concurrent execution, while logging messages and handling exceptions.</p>"},{"location":"examples/log_example/#features","title":"Features","text":"<ul> <li>Logging Configuration: Configures logging with options for log rotation, retention, backtrace, and serialization.</li> <li>Exception Handling: Demonstrates exception handling with logging for <code>ZeroDivisionError</code>.</li> <li>Concurrent Execution:</li> <li>Multi-threading: Executes tasks concurrently using threads.</li> <li>Multi-processing: Executes tasks concurrently using processes.</li> <li>Large Message Logging: Logs large messages repeatedly to test logging performance.</li> <li>Progress Tracking: Uses <code>tqdm</code> to display progress bars for threads and processes.</li> </ul>"},{"location":"examples/log_example/#functions","title":"Functions","text":""},{"location":"examples/log_example/#div_zerox-y","title":"<code>div_zero(x, y)</code>","text":"<p>Attempts to divide <code>x</code> by <code>y</code> and logs any <code>ZeroDivisionError</code> encountered.</p>"},{"location":"examples/log_example/#div_zero_twox-y","title":"<code>div_zero_two(x, y)</code>","text":"<p>Similar to <code>div_zero</code>, attempts to divide <code>x</code> by <code>y</code> and logs any <code>ZeroDivisionError</code> encountered.</p>"},{"location":"examples/log_example/#log_big_stringlqty100-size256","title":"<code>log_big_string(lqty=100, size=256)</code>","text":"<p>Logs a large string multiple times, demonstrating both standard logging and <code>loguru</code> logging.</p>"},{"location":"examples/log_example/#workerwqty1000-lqty100-size256","title":"<code>worker(wqty=1000, lqty=100, size=256)</code>","text":"<p>Executes the <code>log_big_string</code> function repeatedly, simulating a worker process or thread.</p>"},{"location":"examples/log_example/#mainwqty-lqty-size-workers-thread_test-process_test","title":"<code>main(wqty, lqty, size, workers, thread_test, process_test)</code>","text":"<p>Main entry point for the module. Configures and starts either multi-threading or multi-processing based on the provided arguments.</p>"},{"location":"examples/log_example/#usage","title":"Usage","text":"<p>Run the module directly to test its functionality. Example:</p> <pre><code>python log_example.py\n</code></pre> <p>You can customize the parameters for workers, logging quantity, and message size by modifying the <code>main</code> function call in the <code>__main__</code> block.</p>"},{"location":"examples/log_example/#dependencies","title":"Dependencies","text":"<ul> <li><code>logging</code></li> <li><code>loguru</code></li> <li><code>multiprocessing</code></li> <li><code>threading</code></li> <li><code>secrets</code></li> <li><code>tqdm</code></li> <li><code>dsg_lib.common_functions</code></li> </ul>"},{"location":"examples/log_example/#notes","title":"Notes","text":"<ul> <li>Ensure the <code>dsg_lib</code> library is installed and accessible.</li> <li>Adjust the logging configuration as needed for your application.</li> <li>Use the <code>process_test</code> or <code>thread_test</code> flags to toggle between multi-processing and multi-threading.</li> </ul>"},{"location":"examples/log_example/#license","title":"License","text":"<p>This module is licensed under the MIT License.</p> <pre><code># from loguru import logger\nimport logging\nimport logging as logger\nimport multiprocessing\nimport secrets\nimport threading\n\nfrom tqdm import tqdm\n\nfrom dsg_lib.common_functions import logging_config\n\n# Configure logging as before\nlogging_config.config_log(\n    logging_directory=\"log\",\n    log_name=\"log\",\n    logging_level=\"DEBUG\",\n    log_rotation=\"100 MB\",\n    log_retention=\"10 days\",\n    log_backtrace=True,\n    log_serializer=True,\n    log_diagnose=True,\n    # app_name='my_app',\n    # append_app_name=True,\n    intercept_standard_logging=True,\n    enqueue=True,\n)\n\n\ndef div_zero(x: float, y: float) -&gt; float | None:\n    \"\"\"\n    Safely divide x by y and log ZeroDivisionError if encountered.\n\n    Args:\n        x (float): Numerator.\n        y (float): Denominator.\n    Returns:\n        float | None: Quotient or None on error.\n    \"\"\"\n    try:\n        return x / y\n    except ZeroDivisionError as e:\n        logger.error(f\"{e}\")       # log via loguru\n        logging.error(f\"{e}\")      # log via standard logging\n\n\ndef div_zero_two(x: float, y: float) -&gt; float | None:\n    \"\"\"\n    Mirror of div_zero demonstrating identical error handling.\n    \"\"\"\n    try:\n        return x / y\n    except ZeroDivisionError as e:\n        logger.error(f\"{e}\")\n        logging.error(f\"{e}\")\n\n\ndef log_big_string(lqty: int = 100, size: int = 256) -&gt; None:\n    \"\"\"\n    Generate a large random string and log various messages repeatedly.\n\n    Args:\n        lqty (int): Number of log iterations.\n        size (int): Length of each random string.\n    \"\"\"\n    big_string = secrets.token_urlsafe(size)  # create URL-safe token\n    for _ in range(lqty):\n        logging.debug(f\"Lets make this a big message {big_string}\")  # standard debug\n        div_zero(1, 0)     # trigger/log ZeroDivisionError\n        div_zero_two(1, 0)\n        # loguru messages\n        logger.debug(\"This is a loguru debug message\")\n        logger.info(\"This is a loguru info message\")\n        logger.warning(\"This is a loguru warning message\")\n        logger.error(\"This is a loguru error message\")\n        logger.critical(\"This is a loguru critical message\")\n        # continued standard logging\n        logging.info(\"This is a standard logging info message\")\n        logging.warning(\"This is a standard logging warning message\")\n        logging.error(\"This is a standard logging error message\")\n        logging.critical(\"This is a standard logging critical message\")\n\n\ndef worker(wqty: int = 1000, lqty: int = 100, size: int = 256) -&gt; None:\n    \"\"\"\n    Worker routine performing log_big_string in a progress loop.\n\n    Args:\n        wqty (int): Number of outer iterations.\n        lqty (int): Messages per iteration.\n        size (int): Random string length.\n    \"\"\"\n    for _ in tqdm(range(wqty), ascii=True, leave=True):\n        log_big_string(lqty=lqty, size=size)\n\n\ndef main(\n    wqty: int = 100,\n    lqty: int = 10,\n    size: int = 256,\n    workers: int = 16,\n    thread_test: bool = False,\n    process_test: bool = False,\n) -&gt; None:\n    \"\"\"\n    Configure and launch concurrent logging workers.\n\n    Args:\n        wqty (int): Iterations per worker.\n        lqty (int): Logs per iteration.\n        size (int): Random string size.\n        workers (int): Thread/process count.\n        thread_test (bool): Run threads if True.\n        process_test (bool): Run processes if True.\n    \"\"\"\n    if process_test:\n        processes = []\n        # Create worker processes\n        for _ in tqdm(range(workers), desc=\"Multi-Processing Start\", leave=True):\n            p = multiprocessing.Process(\n                target=worker,\n                args=(\n                    wqty,\n                    lqty,\n                    size,\n                ),\n            )\n            processes.append(p)\n            p.start()\n\n        for p in tqdm((processes), desc=\"Multi-Processing Start\", leave=False):\n            p.join(timeout=60)  # Timeout after 60 seconds\n            if p.is_alive():\n                logger.error(f\"Process {p.name} is hanging. Terminating.\")\n                p.terminate()\n                p.join()\n\n    if thread_test:\n        threads = []\n        for _ in tqdm(\n            range(workers), desc=\"Threading Start\", leave=True\n        ):  # Create worker threads\n            t = threading.Thread(\n                target=worker,\n                args=(\n                    wqty,\n                    lqty,\n                    size,\n                ),\n            )\n            threads.append(t)\n            t.start()\n\n        for t in tqdm(threads, desc=\"Threading Gather\", leave=False):\n            t.join()\n\n\nif __name__ == \"__main__\":\n    from time import time\n\n    start = time()\n    main(wqty=5, lqty=50, size=64, workers=8, thread_test=False, process_test=True)\n    print(f\"Execution time: {time()-start:.2f} seconds\")\n</code></pre>"},{"location":"examples/pattern_example/","title":"pattern_example Example","text":""},{"location":"examples/pattern_example/#pattern-example-module","title":"Pattern Example Module","text":"<p>This module demonstrates the usage of the <code>pattern_between_two_char</code> function from the <code>dsg_lib.common_functions.patterns</code> package. It provides examples of how to extract patterns between specified characters in a given text block.</p>"},{"location":"examples/pattern_example/#features","title":"Features","text":"<ul> <li>ASCII_LIST: A comprehensive list of ASCII characters, which can be used for various text processing tasks.</li> <li>pattern_find: A utility function to find and pretty-print patterns between two specified characters in a text block.</li> <li>run_examples: A function that runs example use cases, including:</li> <li>Extracting patterns from a simple text block.</li> <li>Generating a large random text block and extracting patterns from it.</li> </ul>"},{"location":"examples/pattern_example/#usage","title":"Usage","text":"<p>To run the examples, execute this script directly. The output will demonstrate how patterns are extracted from text blocks.</p>"},{"location":"examples/pattern_example/#functions","title":"Functions","text":""},{"location":"examples/pattern_example/#pattern_findleft_char-str-right_char-str-text_block-str","title":"<code>pattern_find(left_char: str, right_char: str, text_block: str)</code>","text":"<p>Finds and pretty-prints patterns between the specified <code>left_char</code> and <code>right_char</code> in the provided <code>text_block</code>.</p>"},{"location":"examples/pattern_example/#run_examples","title":"<code>run_examples()</code>","text":"<p>Runs example use cases to showcase the functionality of the <code>pattern_between_two_char</code> function.</p>"},{"location":"examples/pattern_example/#example-output","title":"Example Output","text":"<p>When running the script, you will see: 1. Patterns extracted from a predefined text block. 2. Patterns extracted from a randomly generated large text block.</p>"},{"location":"examples/pattern_example/#license","title":"License","text":"<p>This module is licensed under the MIT License.</p> <pre><code>import pprint\nfrom random import randint\n\nfrom dsg_lib.common_functions.patterns import pattern_between_two_char\n\nASCII_LIST = [\n    \" \",\n    \"!\",\n    '\"\"',\n    \"#\",\n    \"$\",\n    \"%\",\n    \"&amp;\",\n    \"'\",\n    \"(\",\n    \")\",\n    \"*\",\n    \"+\",\n    \",\",\n    \"-\",\n    \".\",\n    \"/\",\n    \"0\",\n    \"1\",\n    \"2\",\n    \"3\",\n    \"4\",\n    \"5\",\n    \"6\",\n    \"7\",\n    \"8\",\n    \"9\",\n    \":\",\n    \";\",\n    \"&lt;\",\n    \"=\",\n    \"&gt;\",\n    \"?\",\n    \"@\",\n    \"A\",\n    \"B\",\n    \"C\",\n    \"D\",\n    \"E\",\n    \"F\",\n    \"G\",\n    \"H\",\n    \"I\",\n    \"J\",\n    \"K\",\n    \"L\",\n    \"M\",\n    \"N\",\n    \"O\",\n    \"P\",\n    \"Q\",\n    \"R\",\n    \"S\",\n    \"T\",\n    \"U\",\n    \"V\",\n    \"W\",\n    \"X\",\n    \"Y\",\n    \"Z\",\n    \"[\",\n    \"\\\\\",\n    \"]\",\n    \"^\",\n    \"_\",\n    \"`\",\n    \"a\",\n    \"b\",\n    \"c\",\n    \"d\",\n    \"e\",\n    \"f\",\n    \"g\",\n    \"h\",\n    \"i\",\n    \"j\",\n    \"k\",\n    \"l\",\n    \"m\",\n    \"n\",\n    \"o\",\n    \"p\",\n    \"q\",\n    \"r\",\n    \"s\",\n    \"t\",\n    \"u\",\n    \"v\",\n    \"w\",\n    \"x\",\n    \"y\",\n    \"z\",\n    \"{\",\n    \"|\",\n    \"}\",\n    \"~\",\n    \"\u20ac\",\n    \"\u201a\",\n    \"\u0192\",\n    \"\u201e\",\n    \"\u2026\",\n    \"\u2020\",\n    \"\u2021\",\n    \"\u02c6\",\n    \"\u2030\",\n    \"\u0160\",\n    \"\u2039\",\n    \"\u0152\",\n    \"\u017d\",\n    \"\u2018\",\n    \"\u2019\",\n    \"\u201c\",\n    \"\u201d\",\n    \"\u2022\",\n    \"\u2013\",\n    \"\u2014\",\n    \"\u02dc\",\n    \"\u2122\",\n    \"\u0161\",\n    \"\u203a\",\n    \"\u0153\",\n    \"\u017e\",\n    \"\u0178\",\n    \"\u00a1\",\n    \"\u00a2\",\n    \"\u00a3\",\n    \"\u00a4\",\n    \"\u00a5\",\n    \"\u00a6\",\n    \"\u00a7\",\n    \"\u00a8\",\n    \"\u00a9\",\n    \"\u00aa\",\n    \"\u00ab\",\n    \"\u00ac\",\n    \"\u00ae\",\n    \"\u00af\",\n    \"\u00b0\",\n    \"\u00b1\",\n    \"\u00b2\",\n    \"\u00b3\",\n    \"\u00b4\",\n    \"\u00b5\",\n    \"\u00b6\",\n    \"\u00b7\",\n    \"\u00b8\",\n    \"\u00b9\",\n    \"\u00ba\",\n    \"\u00bb\",\n    \"\u00bc\",\n    \"\u00bd\",\n    \"\u00be\",\n    \"\u00bf\",\n    \"\u00c0\",\n    \"\u00c1\",\n    \"\u00c2\",\n    \"\u00c3\",\n    \"\u00c4\",\n    \"\u00c5\",\n    \"\u00c6\",\n    \"\u00c7\",\n    \"\u00c8\",\n    \"\u00c9\",\n    \"\u00ca\",\n    \"\u00cb\",\n    \"\u00cc\",\n    \"\u00cd\",\n    \"\u00ce\",\n    \"\u00cf\",\n    \"\u00d0\",\n    \"\u00d1\",\n    \"\u00d2\",\n    \"\u00d3\",\n    \"\u00d4\",\n    \"\u00d5\",\n    \"\u00d6\",\n    \"\u00d7\",\n    \"\u00d8\",\n    \"\u00d9\",\n    \"\u00da\",\n    \"\u00db\",\n    \"\u00dc\",\n    \"\u00dd\",\n    \"\u00de\",\n    \"\u00df\",\n    \"\u00e0\",\n    \"\u00e1\",\n    \"\u00e2\",\n    \"\u00e3\",\n    \"\u00e4\",\n    \"\u00e5\",\n    \"\u00e6\",\n    \"\u00e7\",\n    \"\u00e8\",\n    \"\u00e9\",\n    \"\u00ea\",\n    \"\u00eb\",\n    \"\u00ec\",\n    \"\u00ed\",\n    \"\u00ee\",\n    \"\u00ef\",\n    \"\u00f0\",\n    \"\u00f1\",\n    \"\u00f2\",\n    \"\u00f3\",\n    \"\u00f4\",\n    \"\u00f5\",\n    \"\u00f6\",\n    \"\u00f7\",\n    \"\u00f8\",\n    \"\u00f9\",\n    \"\u00fa\",\n    \"\u00fb\",\n    \"\u00fc\",\n    \"\u00fd\",\n    \"\u00fe\",\n    \"\u00ff\",\n]\n\npp = pprint.PrettyPrinter(indent=4)\n\n\ndef pattern_find(left_char: str, right_char: str, text_block: str):\n    data = pattern_between_two_char(text_block, left_char, right_char)\n    pp.pprint(data)\n\n\ndef run_examples():\n    text_block = \"Lfound oneR Lfound twoR\"\n    left_char = \"L\"\n    right_char = \"R\"\n    pattern_find(left_char=left_char, right_char=right_char, text_block=text_block)\n\n    for _ in range(100):\n        long_input = \"xyz\" * randint(100, 100000)\n        long_text = f\"{long_input}abc&lt;one&gt;123&lt;two&gt;456&lt;three&gt;{long_input}\"\n\n        result = pattern_between_two_char(\n            text_string=long_text, left_characters=\"&lt;\", right_characters=\"&gt;\"\n        )\n        print(result[\"found\"])\n\n\nif __name__ == \"__main__\":\n    run_examples()\n</code></pre>"},{"location":"examples/text_example/","title":"text_example Example","text":""},{"location":"examples/text_example/#text-example-module","title":"Text Example Module","text":"<p>This module demonstrates basic file operations using the <code>dsg_lib.common_functions.file_functions</code> library. It provides examples of saving text data to a file and reading text data from a file.</p>"},{"location":"examples/text_example/#functions","title":"Functions","text":""},{"location":"examples/text_example/#save_some_dataexample_text-str","title":"<code>save_some_data(example_text: str)</code>","text":"<p>Saves the provided text data to a file. - Parameters:   - <code>example_text</code> (str): The text data to be saved. - Behavior:   Calls the <code>save_text</code> function from <code>dsg_lib.common_functions.file_functions</code> to save the data to a file named <code>your-file-name.txt</code>.</p>"},{"location":"examples/text_example/#open_some_datathe_file_name-str-str","title":"<code>open_some_data(the_file_name: str) -&gt; str</code>","text":"<p>Reads text data from a specified file. - Parameters:   - <code>the_file_name</code> (str): The name of the file to be read. - Returns:   - <code>result</code> (str): The content of the file as a string. - Behavior:   Calls the <code>open_text</code> function from <code>dsg_lib.common_functions.file_functions</code> to read the content of the file.</p>"},{"location":"examples/text_example/#save_csv_examplecsv_data-listliststr-file_name-str-examplecsv","title":"<code>save_csv_example(csv_data: list[list[str]], file_name: str = \"example.csv\")</code>","text":"<p>Saves example rows to a CSV file. - Parameters:   - <code>csv_data</code> (list[list[str]]): Rows for CSV (first row is header).   - <code>file_name</code> (str): Target CSV file name.</p>"},{"location":"examples/text_example/#open_csv_examplefile_name-str-examplecsv-listdict","title":"<code>open_csv_example(file_name: str = \"example.csv\") -&gt; list[dict]</code>","text":"<p>Opens a CSV file and returns its content as a list of dictionaries. - Parameters:   - <code>file_name</code> (str): Name of the CSV file to read. - Returns:   - <code>list[dict]</code>: Parsed CSV rows.</p>"},{"location":"examples/text_example/#save_json_exampledata-dict-list-file_name-str-examplejson","title":"<code>save_json_example(data: dict | list, file_name: str = \"example.json\")</code>","text":"<p>Saves a dictionary or list as JSON. - Parameters:   - <code>data</code> (dict|list): Data to serialize.   - <code>file_name</code> (str): Target JSON file name.</p>"},{"location":"examples/text_example/#open_json_examplefile_name-str-examplejson-dict-list","title":"<code>open_json_example(file_name: str = \"example.json\") -&gt; dict | list</code>","text":"<p>Opens a JSON file and returns its content. - Parameters:   - <code>file_name</code> (str): Name of the JSON file to read. - Returns:   - <code>dict|list</code>: Parsed JSON content.</p>"},{"location":"examples/text_example/#example-usage","title":"Example Usage","text":"<pre><code>if __name__ == \"__main__\":\n    save_some_data(example_text)\n    opened_file: str = open_some_data(\"your-file-name.txt\")\n    print(opened_file)\n\n    # CSV example\n    csv_rows = [\n        [\"header1\", \"header2\"],\n        [\"row1col1\", \"row1col2\"]\n    ]\n    save_csv_example(csv_rows)\n    print(open_csv_example())\n\n    # JSON example\n    json_obj = {\"foo\": \"bar\", \"count\": 1}\n    save_json_example(json_obj)\n    print(open_json_example())\n</code></pre>"},{"location":"examples/text_example/#notes","title":"Notes","text":"<ul> <li>Ensure that the <code>dsg_lib</code> library is installed and accessible in your environment.</li> <li>The file operations assume that the file paths and permissions are correctly configured.</li> </ul>"},{"location":"examples/text_example/#license","title":"License","text":"<p>This module is licensed under the MIT License.</p> <pre><code>from dsg_lib.common_functions.file_functions import (\n    open_text, save_text,\n    save_csv, open_csv,\n    save_json, open_json\n)\n\nexample_text = \"\"\"\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Page Title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n&lt;h1&gt;This is a Heading&lt;/h1&gt;\n&lt;p&gt;This is a paragraph.&lt;/p&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n \"\"\"\n\n\ndef save_some_data(example_text: str):\n    # function requires file_name and data as a string to be sent.\n    # see documentation for additonal information\n    save_text(file_name=\"your-file-name.txt\", data=example_text)\n\n\ndef open_some_data(the_file_name: str) -&gt; str:\n    # function requires file_name and a string will be returned\n    # see documentation for additonal information\n    result: str = open_text(file_name=the_file_name)\n    return result\n\n\ndef save_csv_example(\n    csv_data: list[list[str]],\n    file_name: str = \"example.csv\"\n) -&gt; None:\n    \"\"\"\n    Save example rows to a CSV file.\n\n    Args:\n        csv_data (list[list[str]]): Rows for CSV (first row is header).\n        file_name (str): Target CSV file name.\n    \"\"\"\n    # write rows out\n    save_csv(file_name=file_name, data=csv_data)\n\n\ndef open_csv_example(\n    file_name: str = \"example.csv\"\n) -&gt; list[dict]:\n    \"\"\"\n    Open a CSV file and return its content as list of dicts.\n\n    Args:\n        file_name (str): Name of CSV to read.\n\n    Returns:\n        list[dict]: Parsed CSV rows.\n    \"\"\"\n    return open_csv(file_name=file_name)\n\n\ndef save_json_example(\n    data: dict | list,\n    file_name: str = \"example.json\"\n) -&gt; None:\n    \"\"\"\n    Save a dict or list as JSON.\n\n    Args:\n        data (dict|list): Data to serialize.\n        file_name (str): Target JSON file name.\n    \"\"\"\n    save_json(file_name=file_name, data=data)\n\n\ndef open_json_example(\n    file_name: str = \"example.json\"\n) -&gt; dict | list:\n    \"\"\"\n    Open a JSON file and return its content.\n\n    Args:\n        file_name (str): Name of JSON to read.\n\n    Returns:\n        dict|list: Parsed JSON content.\n    \"\"\"\n    return open_json(file_name=file_name)\n\n\nif __name__ == \"__main__\":\n    save_some_data(example_text)\n    opened_file: str = open_some_data(\"your-file-name.txt\")\n    print(opened_file)\n\n    # CSV example\n    csv_rows = [\n        [\"header1\", \"header2\"],\n        [\"row1col1\", \"row1col2\"]\n    ]\n    save_csv_example(csv_rows)\n    print(open_csv_example())\n\n    # JSON example\n    json_obj = {\"foo\": \"bar\", \"count\": 1}\n    save_json_example(json_obj)\n    print(open_json_example())\n</code></pre>"},{"location":"examples/validate_emails/","title":"validate_emails Example","text":""},{"location":"examples/validate_emails/#email-validation-example-script","title":"Email Validation Example Script","text":"<p>This module demonstrates how to validate a list of email addresses using various configurations. It leverages the <code>validate_email_address</code> function from the <code>dsg_lib.common_functions.email_validation</code> module to perform the validation.</p> <p>The script is designed to: - Validate a predefined list of email addresses. - Use multiple configurations to test different validation scenarios. - Measure and display the time taken to validate all email addresses. - Print the validation results in a sorted order for better readability.</p>"},{"location":"examples/validate_emails/#features","title":"Features","text":"<ul> <li>Email Validation: Checks the validity of email addresses based on various configurations.</li> <li>Custom Configurations: Supports multiple validation options such as deliverability checks, allowing quoted local parts, and more.</li> <li>Performance Measurement: Tracks the time taken to validate all email addresses.</li> <li>Result Sorting: Outputs the validation results in a sorted format for easier analysis.</li> </ul>"},{"location":"examples/validate_emails/#usage","title":"Usage","text":"<p>Run the script as a standalone module:</p> <pre><code>$ python validate_emails.py\n</code></pre>"},{"location":"examples/validate_emails/#attributes","title":"Attributes","text":""},{"location":"examples/validate_emails/#email-addresses","title":"Email Addresses","text":"<p>A predefined list of email addresses to validate. The list includes: - Valid email addresses. - Invalid email addresses. - Edge cases such as emails with non-ASCII characters, quoted local parts, and domain literals.</p>"},{"location":"examples/validate_emails/#configurations","title":"Configurations","text":"<p>A list of dictionaries, where each dictionary represents a validation configuration. Configuration options include: - <code>check_deliverability</code> (bool): Whether to check if the email address is deliverable. - <code>test_environment</code> (bool): Whether the function is being run in a test environment. - <code>allow_smtputf8</code> (bool): Whether to allow non-ASCII characters in the email address. - <code>allow_empty_local</code> (bool): Whether to allow email addresses with an empty local part. - <code>allow_quoted_local</code> (bool): Whether to allow email addresses with a quoted local part. - <code>allow_display_name</code> (bool): Whether to allow email addresses with a display name. - <code>allow_domain_literal</code> (bool): Whether to allow email addresses with a domain literal. - <code>globally_deliverable</code> (bool): Whether the email address should be globally deliverable. - <code>timeout</code> (int): The timeout for the validation in seconds. - <code>dns_type</code> (str): The type of DNS to use for the validation. Can be <code>'dns'</code> or <code>'timeout'</code>.</p>"},{"location":"examples/validate_emails/#functions","title":"Functions","text":""},{"location":"examples/validate_emails/#validate_email_addressemail-str-kwargs-dict-dict","title":"<code>validate_email_address(email: str, **kwargs: dict) -&gt; dict</code>","text":"<p>Validates an email address using the provided configuration and returns a dictionary with the results.</p>"},{"location":"examples/validate_emails/#example-output","title":"Example Output","text":"<p>The script outputs the validation results in a sorted order, along with the time taken for the validation process. Each result includes: - The email address. - The validation status. - Additional metadata based on the configuration used.</p>"},{"location":"examples/validate_emails/#license","title":"License","text":"<p>This module is licensed under the MIT License.</p> <pre><code>import pprint\nimport time\n\nfrom typing import List, Dict, Any\n\nfrom dsg_lib.common_functions.email_validation import validate_email_address\n\ndef run_validation(\n    email_addresses: List[str],\n    configurations: List[Dict[str, Any]],\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Validate each email against multiple configurations.\n\n    Args:\n        email_addresses: List of email strings to validate.\n        configurations: List of parameter dicts for validation.\n\n    Returns:\n        A sorted list of result dicts (sorted by \"email\" key).\n    \"\"\"\n    results: List[Dict[str, Any]] = []\n    # iterate over every email and config combination\n    for email in email_addresses:\n        for config in configurations:\n            # call the core email validator and collect its output\n            res = validate_email_address(email, **config)\n            results.append(res)\n    # sort by email for consistent output\n    return sorted(results, key=lambda x: x[\"email\"])\n\ndef main() -&gt; None:\n    \"\"\"\n    Entry point for the email validation example.\n\n    Defines a list of emails and configurations, measures execution time,\n    runs validation, and pretty\u2011prints the results.\n    \"\"\"\n    # list of example email addresses\n    email_addresses: List[str] = [\n        \"bob@devsetgo.com\",\n        \"bob@devset.go\",\n        \"foo@yahoo.com\",\n        \"bob@gmail.com\",\n        \"very fake@devsetgo.com\",\n        \"jane.doe@example.com\",\n        \"john_doe@example.co.uk\",\n        \"user.name+tag+sorting@example.com\",\n        \"x@example.com\",  # shortest possible email address\n        \"example-indeed@strange-example.com\",\n        \"admin@mailserver1\",  # local domain name with no TLD\n        \"example@s.example\",  # see the list of Internet top-level domains\n        '\" \"@example.org',  # space between the quotes\n        '\"john..doe\"@example.org',  # quoted double dot\n        \"mailhost!username@example.org\",  # bangified host route used for uucp mailers\n        \"user%example.com@example.org\",  # percent sign in local part\n        \"user-@example.org\",  # valid due to the last character being an allowed character\n        # Invalid email addresses\n        \"Abc.example.com\",  # no @ character\n        \"A@b@c@example.com\",  # only one @ is allowed outside quotation marks\n        'a\"b(c)d,e:f;g&lt;h&gt;i[j\\\\k]l@example.com',  # none of the special characters in this local part are allowed outside quotation marks\n        'just\"not\"right@example.com',  # quoted strings must be dot separated or the only element making up the local-part\n        'this is\"not\\\\allowed@example.com',  # spaces, quotes, and backslashes may only exist when within quoted strings and preceded by a backslash\n        'this\\\\ still\\\\\"not\\\\\\\\allowed@example.com',  # even if escaped (preceded by a backslash), spaces, quotes, and backslashes must still be contained by quotes\n        \"1234567890123456789012345678901234567890123456789012345678901234+x@example.com\",  # local part is longer than 64 characters\n        # Emails with empty local part\n        \"@example.com\",  # only valid if allow_empty_local is True\n        # Emails with non-ASCII characters\n        \"\u00fc\u00f1\u00ee\u00e7\u00f8\u00f0\u00e9@example.com\",  # only valid if allow_smtputf8 is True\n        \"user@\u00fc\u00f1\u00ee\u00e7\u00f8\u00f0\u00e9.com\",  # only valid if allow_smtputf8 is True\n        # Emails with quoted local part\n        '\"john.doe\"@example.com',  # only valid if allow_quoted_local is True\n        '\"john..doe\"@example.com',  # only valid if allow_quoted_local is True\n        # Emails with display name\n        \"John Doe &lt;john@example.com&gt;\",  # only valid if allow_display_name is True\n        # Emails with domain literal\n        \"user@[192.0.2.1]\",  # only valid if allow_domain_literal is True\n        # Emails with long local part\n        \"a\" * 65 + \"@example.com\",  # local part is longer than 64 characters\n        # Emails with invalid characters\n        \"john doe@example.com\",  # space is not allowed\n        \"john@doe@example.com\",  # only one @ is allowed\n        \"john.doe@.com\",  # domain can't start with a dot\n        \"john.doe@example..com\",  # domain can't have two consecutive dots\n        \"test@google.com\",\n    ]\n\n    # various validation parameter sets to exercise different rules\n    configurations: List[Dict[str, Any]] = [\n        {\n            \"check_deliverability\": True,\n            \"test_environment\": False,\n            \"allow_smtputf8\": False,\n            \"allow_empty_local\": False,\n            \"allow_quoted_local\": False,\n            \"allow_display_name\": False,\n            \"allow_domain_literal\": False,\n            \"globally_deliverable\": None,\n            \"timeout\": 10,\n            \"dns_type\": \"timeout\",\n        },\n        {\n            \"check_deliverability\": False,\n            \"test_environment\": True,\n            \"allow_smtputf8\": True,\n            \"allow_empty_local\": True,\n            \"allow_quoted_local\": True,\n            \"allow_display_name\": True,\n            \"allow_domain_literal\": True,\n            \"globally_deliverable\": None,\n            \"timeout\": 5,\n            \"dns_type\": \"dns\",\n        },\n        {\"check_deliverability\": True},\n        {\n            \"check_deliverability\": False,\n            \"test_environment\": False,\n            \"allow_smtputf8\": True,\n            \"allow_empty_local\": False,\n            \"allow_quoted_local\": True,\n            \"allow_display_name\": False,\n            \"allow_domain_literal\": True,\n            \"globally_deliverable\": None,\n            \"timeout\": 15,\n            \"dns_type\": \"timeout\",\n        },\n        {\n            \"check_deliverability\": True,\n            \"test_environment\": True,\n            \"allow_smtputf8\": False,\n            \"allow_empty_local\": True,\n            \"allow_quoted_local\": False,\n            \"allow_display_name\": True,\n            \"allow_domain_literal\": False,\n            \"globally_deliverable\": None,\n            \"timeout\": 20,\n            \"dns_type\": \"dns\",\n        },\n        {\n            \"check_deliverability\": False,\n            \"test_environment\": False,\n            \"allow_smtputf8\": True,\n            \"allow_empty_local\": True,\n            \"allow_quoted_local\": True,\n            \"allow_display_name\": True,\n            \"allow_domain_literal\": True,\n            \"globally_deliverable\": None,\n            \"timeout\": 25,\n            \"dns_type\": \"timeout\",\n        },\n        {\n            \"check_deliverability\": True,\n            \"test_environment\": True,\n            \"allow_smtputf8\": False,\n            \"allow_empty_local\": False,\n            \"allow_quoted_local\": False,\n            \"allow_display_name\": False,\n            \"allow_domain_literal\": False,\n            \"globally_deliverable\": None,\n            \"timeout\": 30,\n            \"dns_type\": \"dns\",\n        },\n        {\n            \"check_deliverability\": False,\n            \"test_environment\": True,\n            \"allow_smtputf8\": True,\n            \"allow_empty_local\": False,\n            \"allow_quoted_local\": True,\n            \"allow_display_name\": True,\n            \"allow_domain_literal\": False,\n            \"globally_deliverable\": None,\n            \"timeout\": 35,\n            \"dns_type\": \"timeout\",\n        },\n        {\n            \"check_deliverability\": True,\n            \"test_environment\": False,\n            \"allow_smtputf8\": False,\n            \"allow_empty_local\": True,\n            \"allow_quoted_local\": True,\n            \"allow_display_name\": False,\n            \"allow_domain_literal\": True,\n            \"globally_deliverable\": None,\n            \"timeout\": 40,\n            \"dns_type\": \"dns\",\n        },\n        {\n            \"check_deliverability\": False,\n            \"test_environment\": True,\n            \"allow_smtputf8\": True,\n            \"allow_empty_local\": False,\n            \"allow_quoted_local\": False,\n            \"allow_display_name\": True,\n            \"allow_domain_literal\": True,\n            \"globally_deliverable\": None,\n            \"timeout\": 45,\n            \"dns_type\": \"timeout\",\n        },\n    ]\n\n    # measure and run\n    start_time: float = time.time()\n    results = run_validation(email_addresses, configurations)\n    elapsed: float = time.time() - start_time\n\n    # output each result\n    for record in results:\n        pprint.pprint(record, indent=4)\n\n    print(f\"Time taken: {elapsed:.2f}s\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"fastapi/default_endpoints/","title":"Reference","text":""},{"location":"fastapi/default_endpoints/#dsg_lib.fastapi_functions.default_endpoints","title":"<code>dsg_lib.fastapi_functions.default_endpoints</code>","text":""},{"location":"fastapi/default_endpoints/#dsg_lib.fastapi_functions.default_endpoints.create_default_router","title":"<code>create_default_router(config)</code>","text":"<p>Creates a router with default endpoints, including a configurable robots.txt.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>List[Dict[str, str]]</code> <p>A list of dictionaries specifying which bots are allowed or disallowed.</p> required <p>Returns:</p> Name Type Description <code>APIRouter</code> <code>APIRouter</code> <p>A FastAPI router with the default endpoints.</p> Source code in <code>dsg_lib/fastapi_functions/default_endpoints.py</code> <pre><code>def create_default_router(config: List[Dict[str, str]]) -&gt; APIRouter:\n    \"\"\"\n    Creates a router with default endpoints, including a configurable robots.txt.\n\n    Args:\n        config (List[Dict[str, str]]): A list of dictionaries specifying which bots are allowed or disallowed.\n\n    Returns:\n        APIRouter: A FastAPI router with the default endpoints.\n    \"\"\"\n    router = APIRouter()\n\n    @router.get(\"/robots.txt\", response_class=Response)\n    async def robots_txt():\n        \"\"\"\n        Generates a robots.txt file based on the provided configuration.\n\n        Returns:\n            Response: The robots.txt content.\n        \"\"\"\n        logger.info(\"Generating robots.txt\")\n        lines = [\"User-agent: *\"]\n        for entry in config:\n            bot = entry.get(\"bot\")\n            allow = entry.get(\"allow\", True)\n            if bot:\n                logger.debug(f\"Configuring bot: {bot}, Allow: {allow}\")\n                lines.append(f\"User-agent: {bot}\")\n                lines.append(\"Disallow: /\" if not allow else \"Allow: /\")\n        robots_txt_content = \"\\n\".join(lines)\n        logger.info(\"robots.txt generated successfully\")\n        return Response(robots_txt_content, media_type=\"text/plain\")\n\n    return router\n</code></pre>"},{"location":"fastapi/http_codes/","title":"Reference","text":""},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes","title":"<code>dsg_lib.fastapi_functions.http_codes</code>","text":"<p>http_codes.py</p> <p>This module provides a dictionary of HTTP status codes and their descriptions.</p> <p>The dictionary <code>ALL_HTTP_CODES</code> contains the HTTP status codes as keys. Each key maps to another dictionary that contains a description of the status code, an extended description, and a link to its documentation on the Mozilla Developer Network (MDN).</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Get the description, extended description, and link for HTTP status code 200\nstatus_200 = http_codes.ALL_HTTP_CODES[200]\nprint(status_200)\n# {'description': 'OK', 'extended_description': 'The request has succeeded', 'link': 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200'}\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>ALL_HTTP_CODES</code> <code>dict</code> <p>A dictionary of HTTP status codes. Each key is an</p> <p>Author: Mike Ryan Date: 2024/05/16 License: MIT</p>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes.DELETE_CODES","title":"<code>DELETE_CODES = generate_code_dict(common_codes + [202, 204, 205, 409])</code>  <code>module-attribute</code>","text":"<p>DELETE_CODES is a dictionary of HTTP status codes for DELETE requests. It includes all the common codes, plus some additional codes that are specific to DELETE requests.</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Print the dictionary of HTTP status codes for DELETE requests\nprint(http_codes.DELETE_CODES)\n</code></pre></p>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes.GET_CODES","title":"<code>GET_CODES = generate_code_dict(common_codes + [206, 304, 307, 410, 502])</code>  <code>module-attribute</code>","text":"<p>GET_CODES is a dictionary of HTTP status codes for GET requests. It includes all the common codes, plus some additional codes that are specific to GET requests.</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Print the dictionary of HTTP status codes for GET requests\nprint(http_codes.GET_CODES)\n</code></pre></p>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes.PATCH_CODES","title":"<code>PATCH_CODES = generate_code_dict(common_codes + [202, 204, 206, 409, 412, 413])</code>  <code>module-attribute</code>","text":"<p>PATCH_CODES is a dictionary of HTTP status codes for PATCH requests. It includes all the common codes, plus some additional codes that are specific to PATCH requests.</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Print the dictionary of HTTP status codes for PATCH requests\nprint(http_codes.PATCH_CODES)\n</code></pre></p>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes.POST_CODES","title":"<code>POST_CODES = generate_code_dict(common_codes + [201, 202, 205, 307, 409, 413, 415])</code>  <code>module-attribute</code>","text":"<p>POST_CODES is a dictionary of HTTP status codes for POST requests. It includes all the common codes, plus some additional codes that are specific to POST requests.</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Print the dictionary of HTTP status codes for POST requests\nprint(http_codes.POST_CODES)\n</code></pre></p>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes.PUT_CODES","title":"<code>PUT_CODES = generate_code_dict(common_codes + [202, 204, 206, 409, 412, 413])</code>  <code>module-attribute</code>","text":"<p>PUT_CODES is a dictionary of HTTP status codes for PUT requests. It includes all the common codes, plus some additional codes that are specific to PUT requests.</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Print the dictionary of HTTP status codes for PUT requests\nprint(http_codes.PUT_CODES)\n</code></pre></p>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions.http_codes.generate_code_dict","title":"<code>generate_code_dict(codes, description_only=False)</code>","text":"<p>Generate a dictionary of specific HTTP error codes from the http_codes dictionary.</p> <p>This function takes a list of HTTP status codes and an optional boolean flag. If the flag is True, the function returns a dictionary where each key is an HTTP status code from the input list and each value is the corresponding description from the ALL_HTTP_CODES dictionary. If the flag is False, the function returns a dictionary where each key is an HTTP status code from the input list and each value is the corresponding dictionary from the ALL_HTTP_CODES dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>codes</code> <code>list</code> <p>A list of HTTP status codes.</p> required <code>description_only</code> <code>bool</code> <p>If True, only the description of the codes will be returned.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[int, Union[str, Dict[str, str]]]</code> <p>A dictionary where each key is an HTTP error code from the input</p> <code>Dict[int, Union[str, Dict[str, str]]]</code> <p>list and each value depends on the description_only parameter. If</p> <code>Dict[int, Union[str, Dict[str, str]]]</code> <p>description_only is True, the value is the description string. If</p> <code>Dict[int, Union[str, Dict[str, str]]]</code> <p>description_only is False, the value is a dictionary with keys</p> <code>Dict[int, Union[str, Dict[str, str]]]</code> <p>'description', 'extended_description', and 'link'.</p> <p>Example: <pre><code>from dsg_lib.fastapi_functions import http_codes\n\n# Generate a dictionary for HTTP status codes 200 and 404\nstatus_dict = http_codes.generate_code_dict([200, 404])\nprint(status_dict)\n# {200: {'description': 'OK', 'extended_description': 'The request has succeeded', 'link': 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200'},\n#  404: {'description': 'Not Found', 'extended_description': 'The requested resource could not be found', 'link': 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404'}}\n\n# Generate a dictionary for HTTP status codes 200 and 404 with only descriptions\nstatus_dict = http_codes.generate_code_dict([200, 404], description_only=True)\nprint(status_dict)  # {200: 'OK', 404: 'Not Found'}\n</code></pre></p> Source code in <code>dsg_lib/fastapi_functions/http_codes.py</code> <pre><code>def generate_code_dict(\n    codes: List[int], description_only: bool = False\n) -&gt; Dict[int, Union[str, Dict[str, str]]]:\n    \"\"\"\n    Generate a dictionary of specific HTTP error codes from the http_codes\n    dictionary.\n\n    This function takes a list of HTTP status codes and an optional boolean\n    flag. If the flag is True, the function returns a dictionary where each key\n    is an HTTP status code from the input list and each value is the\n    corresponding description from the ALL_HTTP_CODES dictionary. If the flag is\n    False, the function returns a dictionary where each key is an HTTP status\n    code from the input list and each value is the corresponding dictionary from\n    the ALL_HTTP_CODES dictionary.\n\n    Args:\n        codes (list): A list of HTTP status codes.\n        description_only (bool, optional): If True, only the description of the codes will be returned.\n        Defaults to False.\n\n    Returns:\n        dict: A dictionary where each key is an HTTP error code from the input\n        list and each value depends on the description_only parameter. If\n        description_only is True, the value is the description string. If\n        description_only is False, the value is a dictionary with keys\n        'description', 'extended_description', and 'link'.\n\n    Example:\n    ```python\n\n    from dsg_lib.fastapi_functions import http_codes\n\n    # Generate a dictionary for HTTP status codes 200 and 404\n    status_dict = http_codes.generate_code_dict([200, 404])\n    print(status_dict)\n    # {200: {'description': 'OK', 'extended_description': 'The request has succeeded', 'link': 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200'},\n    #  404: {'description': 'Not Found', 'extended_description': 'The requested resource could not be found', 'link': 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404'}}\n\n    # Generate a dictionary for HTTP status codes 200 and 404 with only descriptions\n    status_dict = http_codes.generate_code_dict([200, 404], description_only=True)\n    print(status_dict)  # {200: 'OK', 404: 'Not Found'}\n    ```\n    \"\"\"\n\n    if description_only:\n        # Log the operation\n        logger.debug(f\"description_only is True and returning HTTP codes: {codes}\")\n\n        # If description_only is True, return a dictionary where each key is an\n        # HTTP error code from the input list and each value is the\n        # corresponding description from the ALL_HTTP_CODES dictionary.\n        return {\n            code: ALL_HTTP_CODES[code][\"description\"]\n            for code in codes\n            if code in ALL_HTTP_CODES\n        }\n    else:\n        # Log the operation\n        logger.debug(f\"returning HTTP codes: {codes}\")\n\n        # If description_only is False, return a dictionary where each key is an\n        # HTTP error code from the input list and each value is the\n        # corresponding dictionary from the ALL_HTTP_CODES dictionary.\n        return {code: ALL_HTTP_CODES[code] for code in codes if code in ALL_HTTP_CODES}\n</code></pre>"},{"location":"fastapi/http_codes/#dsg_lib.fastapi_functions._all_codes","title":"<code>dsg_lib.fastapi_functions._all_codes</code>","text":"<p>This module contains a dictionary mapping HTTP status codes to their descriptions, extended descriptions, and links to their documentation.</p> <p>Each key in this dictionary is an HTTP status code, and each value is another dictionary with keys 'description', 'extended_description', and 'link'.</p> <p>The 'description' key maps to a brief string that describes the HTTP status code. The 'extended_description' key maps to a more detailed explanation of the status code. The 'link' key maps to a string that is a link to the documentation for the HTTP status code.</p> Example <pre><code>from dsg_lib.fastapi_functions.http_codes import ALL_HTTP_CODES\n\n# Get the dictionary for HTTP status code 200\nstatus_200 = ALL_HTTP_CODES[200]\nprint(status_200)\n# Output: {'description': 'OK', 'extended_description': 'The request has succeeded.', 'link': 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200'}\n\n# Get the description for HTTP status code 404\ndescription_404 = ALL_HTTP_CODES[404]['description']\nprint(description_404)  # Output: 'Not Found'\n\n# Get the extended description for HTTP status code 200\nextended_description_200 = ALL_HTTP_CODES[200]['extended_description']\nprint(extended_description_200)  # Output: 'The request has succeeded.'\n\n# Get the link to the documentation for HTTP status code 500\nlink_500 = ALL_HTTP_CODES[500]['link']\nprint(link_500)  # Output: 'https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500'\n</code></pre> <p>Author: Mike Ryan Date: 2024/05/16 License: MIT</p>"},{"location":"fastapi/system_health_endpoints/","title":"Reference","text":""},{"location":"fastapi/system_health_endpoints/#dsg_lib.fastapi_functions.system_health_endpoints","title":"<code>dsg_lib.fastapi_functions.system_health_endpoints</code>","text":"<p>This module provides a configurable health endpoint for a FastAPI application. It includes the following routes:</p> <ul> <li> <p><code>/api/health/status</code>: Returns the status of the application. If the   application is running, it will return <code>{\"status\": \"UP\"}</code>. This endpoint can   be enabled or disabled using the configuration.</p> </li> <li> <p><code>/api/health/uptime</code>: Returns the uptime of the application in a dictionary   with the keys \"Days\", \"Hours\", \"Minutes\", and \"Seconds\". The uptime is   calculated from the time the application was started. This endpoint can be   enabled or disabled using the configuration.</p> </li> <li> <p><code>/api/health/heapdump</code>: Returns a heap dump of the application. The heap dump   is a list of dictionaries, each representing a line of code. Each dictionary   includes the filename, line number, size of memory consumed, and the number of   times the line is referenced. This endpoint can be enabled or disabled using   the configuration.</p> </li> </ul> <p>The module uses the <code>FastAPI</code>, <code>time</code>, <code>tracemalloc</code>, <code>loguru</code>, <code>packaging</code>, and <code>dsg_lib.fastapi.http_codes</code> modules.</p> <p>Functions:</p> Name Description <code>create_health_router</code> <p>dict) -&gt; FastAPI.APIRouter: Creates a FastAPI router with health endpoints based on the provided configuration.</p> Example <pre><code>from FastAPI import FastAPI\nfrom dsg_lib.fastapi_functions import\nsystem_health_endpoints\n\napp = FastAPI()\n\n# User configuration\nconfig = {\n    \"enable_status_endpoint\": True,\n    \"enable_uptime_endpoint\": False,\n    \"enable_heapdump_endpoint\": True,\n}\n\n# Health router\nhealth_router =\nsystem_health_endpoints.create_health_router(config)\napp.include_router(health_router, prefix=\"/api/health\",\ntags=[\"system-health\"])\n\n# Get the status of the application\nresponse = client.get(\"/api/health/status\")\nprint(response.json())  # {\"status\": \"UP\"}\n\n# Get the uptime of the application response =\nclient.get(\"/api/health/uptime\")\nprint(response.json())\n# {\"uptime\": {\"Days\": 0, \"Hours\": 0, \"Minutes\": 1, \"Seconds\": 42.17}}\n\n# Get the heap dump of the application response =\nclient.get(\"/api/health/heapdump\")\nprint(response.json())\n# {\"memory_use\":{\"current\": \"123456\", \"peak\": \"789012\"}, \"heap_dump\": [{\"filename\": \"main.py\", \"lineno\": 10, \"size\": 1234, \"count\": 1}, ...]}\n</code></pre> <p>Author: Mike Ryan Date: 2024/05/16 License: MIT</p>"},{"location":"fastapi/system_health_endpoints/#dsg_lib.fastapi_functions.system_health_endpoints.create_health_router","title":"<code>create_health_router(config)</code>","text":"<p>Create a health router with the following endpoints:</p> <ul> <li> <p><code>/status</code>: Returns the status of the application. This endpoint can be   enabled or disabled using the <code>enable_status_endpoint</code> key in the   configuration.</p> </li> <li> <p><code>/uptime</code>: Returns the uptime of the application. This endpoint can be   enabled or disabled using the <code>enable_uptime_endpoint</code> key in the   configuration.</p> </li> <li> <p><code>/heapdump</code>: Returns a heap dump of the application. This endpoint can be   enabled or disabled using the <code>enable_heapdump_endpoint</code> key in the   configuration.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>A dictionary with the configuration for the endpoints.</p> required <p>Returns:</p> Name Type Description <code>APIRouter</code> <p>A FastAPI router with the configured endpoints.</p> Example <pre><code>from FastAPI import FastAPI\nfrom dsg_lib.fastapi_functions import\nsystem_health_endpoints\n\napp = FastAPI()\n\n# User configuration\nconfig = {\n    \"enable_status_endpoint\": True,\n    \"enable_uptime_endpoint\": False,\n    \"enable_heapdump_endpoint\": True,\n}\n\n# Health router\nhealth_router =\nsystem_health_endpoints.create_health_router(config)\napp.include_router(health_router, prefix=\"/api/health\",\ntags=[\"system-health\"])\n\n# Get the status of the application\nresponse = client.get(\"/api/health/status\")\nprint(response.json())  # {\"status\": \"UP\"}\n\n# Get the uptime of the application response =\nclient.get(\"/api/health/uptime\")\nprint(response.json())\n# {\"uptime\": {\"Days\": 0, \"Hours\": 0, \"Minutes\": 1, \"Seconds\": 42.17}}\n\n# Get the heap dump of the application response =\nclient.get(\"/api/health/heapdump\")\nprint(response.json())\n# {\"memory_use\":{\"current\": \"123456\", \"peak\": \"789012\"}, \"heap_dump\": [{\"filename\": \"main.py\", \"lineno\": 10, \"size\": 1234, \"count\": 1}, ...]}\n</code></pre> Source code in <code>dsg_lib/fastapi_functions/system_health_endpoints.py</code> <pre><code>def create_health_router(config: dict):\n    \"\"\"\n    Create a health router with the following endpoints:\n\n    - `/status`: Returns the status of the application. This endpoint can be\n      enabled or disabled using the `enable_status_endpoint` key in the\n      configuration.\n\n    - `/uptime`: Returns the uptime of the application. This endpoint can be\n      enabled or disabled using the `enable_uptime_endpoint` key in the\n      configuration.\n\n    - `/heapdump`: Returns a heap dump of the application. This endpoint can be\n      enabled or disabled using the `enable_heapdump_endpoint` key in the\n      configuration.\n\n    Args:\n        config (dict): A dictionary with the configuration for the endpoints.\n        Each key should be the name of an endpoint (e.g.,\n        `enable_status_endpoint`) and the value should be a boolean indicating\n        whether the endpoint is enabled or not.\n\n    Returns:\n        APIRouter: A FastAPI router with the configured endpoints.\n\n    Example:\n        ```python\n        from FastAPI import FastAPI\n        from dsg_lib.fastapi_functions import\n        system_health_endpoints\n\n        app = FastAPI()\n\n        # User configuration\n        config = {\n            \"enable_status_endpoint\": True,\n            \"enable_uptime_endpoint\": False,\n            \"enable_heapdump_endpoint\": True,\n        }\n\n        # Health router\n        health_router =\n        system_health_endpoints.create_health_router(config)\n        app.include_router(health_router, prefix=\"/api/health\",\n        tags=[\"system-health\"])\n\n        # Get the status of the application\n        response = client.get(\"/api/health/status\")\n        print(response.json())  # {\"status\": \"UP\"}\n\n        # Get the uptime of the application response =\n        client.get(\"/api/health/uptime\")\n        print(response.json())\n        # {\"uptime\": {\"Days\": 0, \"Hours\": 0, \"Minutes\": 1, \"Seconds\": 42.17}}\n\n        # Get the heap dump of the application response =\n        client.get(\"/api/health/heapdump\")\n        print(response.json())\n        # {\"memory_use\":{\"current\": \"123456\", \"peak\": \"789012\"}, \"heap_dump\": [{\"filename\": \"main.py\", \"lineno\": 10, \"size\": 1234, \"count\": 1}, ...]}\n\n        ```\n    \"\"\"\n    # Try to import FastAPI, handle ImportError if FastAPI is not installed\n    try:\n        import fastapi\n        from fastapi import APIRouter, HTTPException, status\n        from fastapi.responses import ORJSONResponse\n    except ImportError:  # pragma: no cover\n        APIRouter = HTTPException = status = ORJSONResponse = fastapi = (\n            None\n        )\n\n    # Check FastAPI version\n    min_version = \"0.100.0\"  # replace with your minimum required version\n    if fastapi is not None and packaging_version.parse(\n        fastapi.__version__\n    ) &lt; packaging_version.parse(min_version):\n        raise ImportError(\n            f\"FastAPI version &gt;= {min_version} required, run `pip install --upgrade fastapi`\"\n        )  # pragma: no cover\n\n    # Store the start time of the application\n    app_start_time = time.time()\n\n    # TODO: determine method to shutdown/restart python application\n\n    status_response = generate_code_dict([400, 405, 500], description_only=False)\n\n    tracemalloc.start()\n    # Create a new router\n    router = APIRouter()\n\n    # Check if the status endpoint is enabled in the configuration\n    if config.get(\"enable_status_endpoint\", True):\n        # Define the status endpoint\n        @router.get(\n            \"/status\",\n            tags=[\"system-health\"],\n            status_code=status.HTTP_200_OK,\n            response_class=ORJSONResponse,\n            responses=status_response,\n        )\n        async def health_status():\n            \"\"\"\n            Returns the status of the application.\n\n            This endpoint returns a dictionary with the status of the\n            application. If the application is running, it will return\n            `{\"status\": \"UP\"}`.\n\n            Returns:\n                dict: A dictionary with the status of the application. The\n                dictionary has a single key, \"status\", and the value is a string\n                that indicates the status of the application.\n\n            Raises:\n                HTTPException: If there is an error while getting the status of\n                the application.\n\n            Example:\n                ```python\n                from FastAPI import FastAPI\n                from dsg_lib.fastapi_functions import\n                system_health_endpoints\n\n                app = FastAPI()\n\n                # User configuration\n                config = {\n                    \"enable_status_endpoint\": True,\n                    \"enable_uptime_endpoint\": False,\n                    \"enable_heapdump_endpoint\": True,\n                }\n\n                # Health router\n                health_router =\n                system_health_endpoints.create_health_router(config)\n                app.include_router(health_router, prefix=\"/api/health\",\n                tags=[\"system-health\"])\n\n                # Get the status of the application\n                response = client.get(\"/api/health/status\")\n                print(response.json())  # {\"status\": \"UP\"}\n            ```\n            \"\"\"\n            # Log the status request\n            logger.info(\"Health status of up returned\")\n            # Return a dictionary with the status of the application\n            return {\"status\": \"UP\"}\n\n    # Check if the uptime endpoint is enabled in the configuration\n    if config.get(\"enable_uptime_endpoint\", True):\n        # Define the uptime endpoint\n        @router.get(\"/uptime\", response_class=ORJSONResponse, responses=status_response)\n        async def get_uptime():\n            \"\"\"\n            Calculate and return the uptime of the application.\n\n            This endpoint returns a dictionary with the uptime of the\n            application. The uptime is calculated from the time the application\n            was started and is returned in a dictionary with the keys \"Days\",\n            \"Hours\", \"Minutes\", and \"Seconds\".\n\n            Returns:\n                dict: A dictionary with the uptime of the application. The\n                dictionary has keys for \"Days\", \"Hours\", \"Minutes\", and\n                \"Seconds\".\n\n            Raises:\n                HTTPException: If there is an error while calculating the uptime\n                of the application.\n\n            Example:\n                ```python\n                from FastAPI import FastAPI\n                from dsg_lib.fastapi_functions import\n                system_health_endpoints\n\n                app = FastAPI()\n\n                # User configuration\n                config = {\n                    \"enable_status_endpoint\": True,\n                    \"enable_uptime_endpoint\": False,\n                    \"enable_heapdump_endpoint\": True,\n                }\n\n                # Health router\n                health_router =\n                system_health_endpoints.create_health_router(config)\n                app.include_router(health_router, prefix=\"/api/health\",\n                tags=[\"system-health\"])\n\n                # Get the uptime of the application response =\n                client.get(\"/api/health/uptime\")\n                print(response.json())\n                # {\"uptime\": {\"Days\": 0, \"Hours\": 0, \"Minutes\": 1, \"Seconds\": 42.17}}\n\n                ```\n            \"\"\"\n            # Calculate the total uptime in seconds This is done by subtracting\n            # the time when the application started from the current time\n            uptime_seconds = time.time() - app_start_time\n\n            # Convert the uptime from seconds to days, hours, minutes, and\n            # seconds\n            days, rem = divmod(uptime_seconds, 86400)\n            hours, rem = divmod(rem, 3600)\n            minutes, seconds = divmod(rem, 60)\n\n            # Log the uptime\n            logger.info(\n                f\"Uptime: {int(days)} days, {int(hours)} hours, {int(minutes)} minutes, {round(seconds, 2)} seconds\"\n            )\n\n            # Return a dictionary with the uptime The dictionary has keys for\n            # days, hours, minutes, and seconds\n            return {\n                \"uptime\": {\n                    \"Days\": int(days),\n                    \"Hours\": int(hours),\n                    \"Minutes\": int(minutes),\n                    \"Seconds\": round(seconds, 2),\n                }\n            }\n\n    if config.get(\"enable_heapdump_endpoint\", True):\n\n        @router.get(\n            \"/heapdump\", response_class=ORJSONResponse, responses=status_response\n        )\n        async def get_heapdump():\n            \"\"\"\n            Returns a heap dump of the application.\n\n            This endpoint returns a snapshot of the current memory usage of the\n            application. The heap dump is a list of dictionaries, each\n            representing a line of code. Each dictionary includes the filename,\n            line number, size of memory consumed, and the number of times the\n            line is referenced.\n\n            Returns:\n                dict: A dictionary with the current and peak memory usage, and\n                the heap dump of the application. The dictionary has two keys,\n                \"memory_use\" and \"heap_dump\". The \"memory_use\" key contains a\n                dictionary with the current and peak memory usage. The\n                \"heap_dump\" key contains a list of dictionaries, each\n                representing a line of code.\n\n            Raises:\n                HTTPException: If there is an error while getting the heap dump\n                of the application.\n\n            Example:\n                ```python\n                from FastAPI import FastAPI\n                from dsg_lib.fastapi_functions import\n                system_health_endpoints\n\n                app = FastAPI()\n\n                # User configuration\n                config = {\n                    \"enable_status_endpoint\": True,\n                    \"enable_uptime_endpoint\": False,\n                    \"enable_heapdump_endpoint\": True,\n                }\n\n                # Health router\n                health_router =\n                system_health_endpoints.create_health_router(config)\n                app.include_router(health_router, prefix=\"/api/health\",\n                tags=[\"system-health\"])\n\n                # Get the heap dump of the application response =\n                client.get(\"/api/health/heapdump\")\n                print(response.json())\n                # {\"memory_use\":{\"current\": \"123456\", \"peak\": \"789012\"}, \"heap_dump\": [{\"filename\": \"main.py\", \"lineno\": 10, \"size\": 1234, \"count\": 1}, ...]}\n\n                ```\n            \"\"\"\n\n            try:\n                # Take a snapshot of the current memory usage\n                snapshot = tracemalloc.take_snapshot()\n                # Get the top 10 lines consuming memory\n                top_stats = snapshot.statistics(\"traceback\")\n\n                heap_dump = []\n                for stat in top_stats[:10]:\n                    # Get the first frame from the traceback\n                    frame = stat.traceback[0]\n                    # Add the frame to the heap dump\n                    heap_dump.append(\n                        {\n                            \"filename\": frame.filename,\n                            \"lineno\": frame.lineno,\n                            \"size\": stat.size,\n                            \"count\": stat.count,\n                        }\n                    )\n\n                logger.info(f\"Heap dump returned {heap_dump}\")\n                memory_use = tracemalloc.get_traced_memory()\n                return {\n                    \"memory_use\": {\n                        \"current\": f\"{memory_use[0]:,}\",\n                        \"peak\": f\"{memory_use[1]:,}\",\n                    },\n                    \"heap_dump\": heap_dump,\n                }\n            except Exception as ex:\n                logger.error(f\"Error in get_heapdump: {ex}\")\n                raise HTTPException(\n                    status_code=500, detail=f\"Error in get_heapdump: {ex}\"\n                )\n\n    return router\n</code></pre>"}]}